"""åŸºäº LangGraph çš„å¤šæ™ºèƒ½ä½“ä»£ç å®¡æŸ¥å·¥ä½œæµã€‚

å·¥ä½œæµç»“æ„ï¼š
1. Intent Analysisï¼ˆMap-Reduceï¼‰ï¼šå¹¶è¡Œåˆ†ææ–‡ä»¶æ„å›¾
2. Managerï¼šç”Ÿæˆä»»åŠ¡åˆ—è¡¨å¹¶æŒ‰é£é™©ç±»å‹åˆ†ç»„
3. Expert Executionï¼šå¹¶è¡Œæ‰§è¡Œä¸“å®¶ç»„ä»»åŠ¡ï¼ˆå¹¶å‘æ§åˆ¶ï¼‰
4. Reporterï¼šç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
"""

import logging
from typing import Dict, Any, List, Optional
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import ToolNode
from langchain_core.messages import HumanMessage
from core.state import ReviewState
from core.llm import LLMProvider
from core.langchain_llm import LangChainLLMAdapter
from core.config import Config
from tools.langchain_tools import create_tools_with_context
from agents.nodes.intent_analysis import intent_analysis_node
from agents.nodes.manager import manager_node
from agents.nodes.expert_execution import expert_execution_node
from agents.nodes.reporter import reporter_node

logger = logging.getLogger(__name__)


def create_multi_agent_workflow(
    config: Config,
    enable_checkpointing: bool = False
) -> Any:
    """åˆ›å»ºå¤šæ™ºèƒ½ä½“å·¥ä½œæµå›¾ã€‚
    
    Args:
        config: é…ç½®å¯¹è±¡ã€‚
        enable_checkpointing: æ˜¯å¦å¯ç”¨ checkpointerï¼ˆé»˜è®¤ç¦ç”¨ï¼‰ã€‚
    
    Returns:
        ç¼–è¯‘åçš„ LangGraph å·¥ä½œæµã€‚
    """
    # Initialize LLM provider
    llm_provider = LLMProvider(config.llm)
    llm_adapter = LangChainLLMAdapter(llm_provider=llm_provider)
    
    workspace_root = config.system.workspace_root
    asset_key = config.system.asset_key
    
    langchain_tools = create_tools_with_context(
        workspace_root=workspace_root,
        asset_key=asset_key
    )
    
    checkpointer = MemorySaver() if enable_checkpointing else None
    
    # Create workflow graph
    workflow = StateGraph(ReviewState)
    
    # Add nodes
    workflow.add_node("intent_analysis", intent_analysis_node)
    workflow.add_node("manager", manager_node)
    workflow.add_node("expert_execution", expert_execution_node)
    workflow.add_node("reporter", reporter_node)
    
    # Set entry point
    workflow.set_entry_point("intent_analysis")
    
    # Add edges
    # Intent Analysis -> Manager
    workflow.add_edge("intent_analysis", "manager")
    
    # Manager -> Expert Execution or Reporter (conditional)
    workflow.add_conditional_edges(
        "manager",
        route_to_experts,
        {
            "expert_execution": "expert_execution",
            "reporter": "reporter"
        }
    )
    
    # Expert Execution -> Reporter
    workflow.add_edge("expert_execution", "reporter")
    
    # Reporter -> END
    workflow.add_edge("reporter", END)
    
    # Compile workflow with checkpointer
    compile_kwargs = {}
    if checkpointer:
        compile_kwargs["checkpointer"] = checkpointer
    
    compiled = workflow.compile(**compile_kwargs)
    
    # Wrap nodes to inject dependencies
    return _wrap_workflow_with_dependencies(
        compiled, 
        llm_provider, 
        llm_adapter,
        config, 
        langchain_tools
    )


def route_to_experts(state: ReviewState) -> str:
    """ä» Manager è·¯ç”±åˆ° expert_execution æˆ– reporterã€‚
    
    å¦‚æœ work_list ä¸ºç©ºï¼Œè·³è½¬åˆ° reporterï¼›å¦åˆ™æ‰§è¡Œ expert_executionã€‚
    """
    print("\n" + "="*80)
    print("ğŸ”€ [è·¯ç”±] route_to_experts - å†³ç­–ä¸‹ä¸€æ­¥")
    print("="*80)
    
    work_list = state.get("work_list", [])
    expert_tasks = state.get("expert_tasks", {})
    
    if not work_list or not expert_tasks:
        print("  â­ï¸  æ— ä»»åŠ¡ï¼Œè·³è¿‡ä¸“å®¶æ‰§è¡Œï¼Œç›´æ¥ç”ŸæˆæŠ¥å‘Š")
        logger.info("No work list or expert tasks, skipping to reporter")
        return "reporter"
    
    print(f"  â¡ï¸  è·¯ç”±åˆ° expert_execution")
    print(f"     - ä¸“å®¶ç»„æ•°: {len(expert_tasks)}")
    print("="*80)
    logger.info(f"Routing to expert_execution with {len(expert_tasks)} expert groups")
    return "expert_execution"


def _wrap_workflow_with_dependencies(
    compiled_graph: Any,
    llm_provider: LLMProvider,
    llm_adapter: LangChainLLMAdapter,
    config: Config,
    langchain_tools: List[Any]
) -> Any:
    """åŒ…è£…å·¥ä½œæµèŠ‚ç‚¹ä»¥æ³¨å…¥ä¾èµ–ï¼ˆLLMã€é…ç½®ã€å·¥å…·ï¼‰ã€‚
    
    é€šè¿‡ä¿®æ”¹ state çš„ metadata å­—æ®µæ³¨å…¥ä¾èµ–ã€‚
    """
    # Store original invoke methods
    original_ainvoke = compiled_graph.ainvoke
    original_invoke = compiled_graph.invoke
    
    async def ainvoke_with_deps(state: ReviewState, **kwargs) -> ReviewState:
        """æ‰§è¡Œå·¥ä½œæµï¼ˆæ³¨å…¥ä¾èµ–åˆ° stateï¼‰ã€‚"""
        if "messages" not in state:
            state["messages"] = []
        
        if "metadata" not in state:
            state["metadata"] = {}
        
        state["metadata"]["llm_provider"] = llm_provider
        state["metadata"]["llm_adapter"] = llm_adapter
        state["metadata"]["config"] = config
        state["metadata"]["langchain_tools"] = langchain_tools
        
        # Call original invoke
        return await original_ainvoke(state, **kwargs)
    
    def invoke_with_deps(state: ReviewState, **kwargs) -> ReviewState:
        """æ‰§è¡Œå·¥ä½œæµï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰ã€‚"""
        if "messages" not in state:
            state["messages"] = []
        
        if "metadata" not in state:
            state["metadata"] = {}
        
        state["metadata"]["llm_provider"] = llm_provider
        state["metadata"]["llm_adapter"] = llm_adapter
        state["metadata"]["config"] = config
        state["metadata"]["langchain_tools"] = langchain_tools
        
        # Call original invoke
        return original_invoke(state, **kwargs)
    
    # Replace methods
    compiled_graph.ainvoke = ainvoke_with_deps
    compiled_graph.invoke = invoke_with_deps
    
    return compiled_graph


async def map_intent_analysis(state: ReviewState) -> ReviewState:
    """æ„å›¾åˆ†æçš„ Map å‡½æ•°ï¼ˆåœ¨ intent_analysis_node ä¸­å®ç°å¹¶è¡Œæ‰§è¡Œï¼‰ã€‚"""
    # For now, we'll process all files in the intent_analysis_node
    # In a full implementation, we could use LangGraph's map capabilities
    # or implement custom parallel execution
    return await intent_analysis_node(state)


async def run_multi_agent_workflow(
    diff_context: str,
    changed_files: List[str],
    config: Config = None,
    lint_errors: List[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """è¿è¡Œå¤šæ™ºèƒ½ä½“ä»£ç å®¡æŸ¥å·¥ä½œæµã€‚
    
    Args:
        diff_context: Git diff å­—ç¬¦ä¸²ã€‚
        changed_files: å˜æ›´æ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚
        config: é…ç½®å¯¹è±¡ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰ã€‚
        lint_errors: é¢„æ£€æŸ¥çš„ lint é”™è¯¯åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰ã€‚
    
    Returns:
        åŒ…å«æœ€ç»ˆå®¡æŸ¥ç»“æœçš„çŠ¶æ€å­—å…¸ã€‚
    """
    if config is None:
        from core.config import Config
        config = Config.load_default()
    
    # Create workflow
    app = create_multi_agent_workflow(config)
    
    # Initialize state
    initial_state: ReviewState = {
        "messages": [],
        "diff_context": diff_context,
        "changed_files": changed_files,
        "file_analyses": [],
        "work_list": [],
        "expert_tasks": {},
        "expert_results": {},
        "confirmed_issues": [],
        "final_report": "",
        "lint_errors": lint_errors or [],
        "metadata": {
            "workflow_version": "multi_agent_parallel",
            "config_provider": config.llm.provider,
            "confidence_threshold": 0.5
        }
    }
    
    # Run the workflow
    print("\n" + "="*80)
    print("ğŸš€ å¤šæ™ºèƒ½ä½“å·¥ä½œæµå¯åŠ¨")
    print("="*80)
    print(f"ğŸ“ è¾“å…¥:")
    print(f"   - Diff ä¸Šä¸‹æ–‡: {len(diff_context)} å­—ç¬¦")
    print(f"   - å˜æ›´æ–‡ä»¶æ•°: {len(changed_files)}")
    print(f"   - Lint é”™è¯¯æ•°: {len(lint_errors) if lint_errors else 0}")
    print("="*80)
    
    try:
        invoke_kwargs = {}
        
        # å¯é€‰ï¼šå¦‚æœå¯ç”¨äº† checkpointerï¼Œç”Ÿæˆ thread_id å¹¶ä¼ å…¥ config
        # import hashlib
        # import time
        # thread_id = hashlib.md5(
        #     f"{time.time()}_{','.join(changed_files)}".encode()
        # ).hexdigest()[:16]
        # invoke_kwargs['config'] = {
        #     "configurable": {
        #         "thread_id": thread_id
        #     }
        # }
        
        final_state = await app.ainvoke(initial_state, **invoke_kwargs)
        
        print("\n" + "="*80)
        print("âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
        print("="*80)
        print(f"ğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"   - ç¡®è®¤é—®é¢˜æ•°: {len(final_state.get('confirmed_issues', []))}")
        print(f"   - æŠ¥å‘Šé•¿åº¦: {len(final_state.get('final_report', ''))} å­—ç¬¦")
        print("="*80)
        
        return final_state
    except Exception as e:
        logger.error(f"Workflow execution error: {e}", exc_info=True)
        # Error handling: return error state
        return {
            **initial_state,
            "confirmed_issues": [],
            "final_report": f"Workflow execution error: {str(e)}",
            "metadata": {
                **initial_state.get("metadata", {}),
                "workflow_error": str(e)
            }
        }
