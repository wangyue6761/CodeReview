"""Multi-agent workflow for code review using LangGraph.

é‡æ„è¯´æ˜ï¼š
1. æ·»åŠ  LangGraph checkpointer æ”¯æŒï¼ˆMemorySaverï¼‰ç”¨äºè®°å¿†æŒä¹…åŒ–
2. ä½¿ç”¨ LangChain æ ‡å‡†å·¥å…·å®šä¹‰ï¼ˆ@tool è£…é¥°å™¨ï¼‰
3. ä½¿ç”¨ llm.bind_tools() ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
4. ä½¿ç”¨ LangGraph ToolNode æ‰§è¡Œå·¥å…·è°ƒç”¨

This module implements a dynamic parallel execution workflow with:
- Map-Reduce pattern for intent analysis
- Manager node for task routing
- Parallel expert execution with concurrency control
- Final report generation
"""

import logging
from typing import Dict, Any, List, Optional
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import ToolNode
from langchain_core.messages import HumanMessage
from core.state import ReviewState
from core.llm import LLMProvider
from core.langchain_llm import LangChainLLMAdapter
from core.config import Config
from tools.repo_tools import FetchRepoMapTool
from tools.file_tools import ReadFileTool
from tools.langchain_tools import create_tools_with_context
from agents.nodes.intent_analysis import intent_analysis_node
from agents.nodes.manager import manager_node
from agents.nodes.expert_execution import expert_execution_node
from agents.nodes.reporter import reporter_node

logger = logging.getLogger(__name__)


def create_multi_agent_workflow(
    config: Config,
    enable_checkpointing: bool = False  # é‡æ„è¯´æ˜ï¼šé»˜è®¤ç¦ç”¨ï¼Œå› ä¸ºä»£ç å®¡æŸ¥å·¥ä½œæµé€šå¸¸ä¸éœ€è¦è·¨ä¼šè¯æŒä¹…åŒ–
) -> Any:
    """Create the multi-agent workflow graph.
    
    é‡æ„è¯´æ˜ï¼š
    1. æ·»åŠ  checkpointer æ”¯æŒï¼ˆMemorySaverï¼‰ç”¨äºè®°å¿†æŒä¹…åŒ–
    2. åˆ›å»º LangChain LLM é€‚é…å™¨ï¼Œæ”¯æŒ LCEL è¯­æ³•
    3. ä½¿ç”¨æ ‡å‡†å·¥å…·å®šä¹‰ï¼ˆ@tool è£…é¥°å™¨ï¼‰
    4. ä½¿ç”¨ llm.bind_tools() ç»‘å®šå·¥å…·ï¼ˆåœ¨éœ€è¦å·¥å…·è°ƒç”¨çš„èŠ‚ç‚¹ä¸­ï¼‰
    
    Workflow structure:
    1. Intent Analysis (Map-Reduce): Analyze each changed file in parallel
    2. Manager: Generate work_list and group by risk_type into expert_tasks
    3. Expert Execution: Parallel execution of expert groups with concurrency control
    4. Reporter: Generate final report from expert results
    
    Args:
        config: Configuration object.
        enable_checkpointing: æ˜¯å¦å¯ç”¨ checkpointerï¼ˆè®°å¿†æŒä¹…åŒ–ï¼‰ã€‚
    
    Returns:
        Compiled LangGraph workflow with checkpointer support.
    """
    # Initialize LLM provider
    llm_provider = LLMProvider(config.llm)
    
    # é‡æ„è¯´æ˜ï¼šåˆ›å»º LangChain LLM é€‚é…å™¨ï¼Œæ”¯æŒ LCEL è¯­æ³•
    llm_adapter = LangChainLLMAdapter(llm_provider=llm_provider)
    
    # Initialize tools (ä¿æŒå‘åå…¼å®¹ï¼ŒåŒæ—¶æ”¯æŒæ–°å·¥å…·)
    workspace_root = config.system.workspace_root
    asset_key = config.system.asset_key
    tools = [
        # FetchRepoMapTool(asset_key=asset_key),
        ReadFileTool(workspace_root=workspace_root)
    ]
    
    # é‡æ„è¯´æ˜ï¼šåˆ›å»º LangChain æ ‡å‡†å·¥å…·ï¼ˆä½¿ç”¨ @tool è£…é¥°å™¨ï¼‰
    langchain_tools = create_tools_with_context(
        workspace_root=workspace_root,
        asset_key=asset_key
    )
    
    # é‡æ„è¯´æ˜ï¼šåˆ›å»º checkpointer ç”¨äºè®°å¿†æŒä¹…åŒ–
    # è¿™æ˜¯ LangGraph æ ‡å‡†åšæ³•ï¼Œæ›¿ä»£ ConversationBufferMemory ç­‰ä¼ ç»Ÿ Memory ç±»
    checkpointer = MemorySaver() if enable_checkpointing else None
    
    # Create workflow graph
    workflow = StateGraph(ReviewState)
    
    # Add nodes
    workflow.add_node("intent_analysis", intent_analysis_node)
    workflow.add_node("manager", manager_node)
    workflow.add_node("expert_execution", expert_execution_node)
    workflow.add_node("reporter", reporter_node)
    
    # é‡æ„è¯´æ˜ï¼šæ·»åŠ  ToolNode ç”¨äºæ‰§è¡Œå·¥å…·è°ƒç”¨
    # è¿™æ˜¯ LangGraph æ ‡å‡†åšæ³•ï¼Œæ›¿ä»£æ‰‹åŠ¨è§£æå·¥å…·è°ƒç”¨
    # æ³¨æ„ï¼šå½“å‰å·¥ä½œæµä¸­å·¥å…·è°ƒç”¨ä¸»è¦åœ¨ expert_execution_node ä¸­æ‰‹åŠ¨å¤„ç†
    # å¦‚æœéœ€è¦ï¼Œå¯ä»¥æ·»åŠ ä¸€ä¸ªä¸“é—¨çš„å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹
    # tool_node = ToolNode(langchain_tools)
    # workflow.add_node("tools", tool_node)
    
    # Set entry point
    workflow.set_entry_point("intent_analysis")
    
    # Add edges
    # Intent Analysis -> Manager
    workflow.add_edge("intent_analysis", "manager")
    
    # Manager -> Expert Execution or Reporter (conditional)
    workflow.add_conditional_edges(
        "manager",
        route_to_experts,
        {
            "expert_execution": "expert_execution",
            "reporter": "reporter"
        }
    )
    
    # Expert Execution -> Reporter
    workflow.add_edge("expert_execution", "reporter")
    
    # Reporter -> END
    workflow.add_edge("reporter", END)
    
    # Compile workflow with checkpointer
    # é‡æ„è¯´æ˜ï¼šä½¿ç”¨ checkpointer ç¼–è¯‘å·¥ä½œæµï¼Œæ”¯æŒè®°å¿†æŒä¹…åŒ–
    compile_kwargs = {}
    if checkpointer:
        compile_kwargs["checkpointer"] = checkpointer
    
    compiled = workflow.compile(**compile_kwargs)
    
    # Wrap nodes to inject dependencies
    return _wrap_workflow_with_dependencies(
        compiled, 
        llm_provider, 
        llm_adapter,
        config, 
        tools,
        langchain_tools
    )


def route_to_experts(state: ReviewState) -> str:
    """Route from manager to expert_execution or reporter.
    
    If work_list is empty, skip to reporter. Otherwise, go to expert_execution.
    
    Args:
        state: Current workflow state.
    
    Returns:
        Next node name: "expert_execution" or "reporter".
    """
    print("\n" + "="*80)
    print("ğŸ”€ [è·¯ç”±] route_to_experts - å†³ç­–ä¸‹ä¸€æ­¥")
    print("="*80)
    
    work_list = state.get("work_list", [])
    expert_tasks = state.get("expert_tasks", {})
    
    if not work_list or not expert_tasks:
        print("  â­ï¸  æ— ä»»åŠ¡ï¼Œè·³è¿‡ä¸“å®¶æ‰§è¡Œï¼Œç›´æ¥ç”ŸæˆæŠ¥å‘Š")
        logger.info("No work list or expert tasks, skipping to reporter")
        return "reporter"
    
    print(f"  â¡ï¸  è·¯ç”±åˆ° expert_execution")
    print(f"     - ä¸“å®¶ç»„æ•°: {len(expert_tasks)}")
    print("="*80)
    logger.info(f"Routing to expert_execution with {len(expert_tasks)} expert groups")
    return "expert_execution"


def _wrap_workflow_with_dependencies(
    compiled_graph: Any,
    llm_provider: LLMProvider,
    llm_adapter: LangChainLLMAdapter,
    config: Config,
    tools: List[Any],
    langchain_tools: List[Any]
) -> Any:
    """Wrap workflow nodes to inject dependencies (LLM provider, config, tools).
    
    é‡æ„è¯´æ˜ï¼š
    - æ·»åŠ  llm_adapter åˆ° metadataï¼Œæ”¯æŒ LCEL è¯­æ³•
    - æ·»åŠ  langchain_tools åˆ° metadataï¼Œæ”¯æŒå·¥å…·ç»‘å®š
    
    This is a workaround since LangGraph doesn't directly support dependency injection.
    We'll modify the state to include these dependencies in metadata before execution.
    
    Args:
        compiled_graph: Compiled LangGraph workflow.
        llm_provider: LLM provider instance.
        llm_adapter: LangChain LLM adapter instance (for LCEL syntax).
        config: Configuration object.
        tools: List of tool instances (legacy BaseTool).
        langchain_tools: List of LangChain tool instances (using @tool decorator).
    
    Returns:
        Wrapped compiled graph.
    """
    # Store original invoke methods
    original_ainvoke = compiled_graph.ainvoke
    original_invoke = compiled_graph.invoke
    
    async def ainvoke_with_deps(state: ReviewState, **kwargs) -> ReviewState:
        """Invoke workflow with dependencies injected into state.
        
        é‡æ„è¯´æ˜ï¼š
        - åˆå§‹åŒ– messages å­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        - æ³¨å…¥ llm_adapter æ”¯æŒ LCEL è¯­æ³•
        - æ³¨å…¥ langchain_tools æ”¯æŒå·¥å…·ç»‘å®š
        """
        # é‡æ„è¯´æ˜ï¼šåˆå§‹åŒ– messages å­—æ®µï¼ˆLangGraph æ ‡å‡†ï¼‰
        if "messages" not in state:
            state["messages"] = []
        
        # Inject dependencies into metadata
        if "metadata" not in state:
            state["metadata"] = {}
        
        state["metadata"]["llm_provider"] = llm_provider
        state["metadata"]["llm_adapter"] = llm_adapter  # æ–°å¢ï¼šæ”¯æŒ LCEL è¯­æ³•
        state["metadata"]["config"] = config
        state["metadata"]["tools"] = tools  # ä¿æŒå‘åå…¼å®¹
        state["metadata"]["langchain_tools"] = langchain_tools  # æ–°å¢ï¼šLangChain æ ‡å‡†å·¥å…·
        
        # Call original invoke
        return await original_ainvoke(state, **kwargs)
    
    def invoke_with_deps(state: ReviewState, **kwargs) -> ReviewState:
        """Invoke workflow with dependencies injected into state (sync version)."""
        # é‡æ„è¯´æ˜ï¼šåˆå§‹åŒ– messages å­—æ®µï¼ˆLangGraph æ ‡å‡†ï¼‰
        if "messages" not in state:
            state["messages"] = []
        
        # Inject dependencies into metadata
        if "metadata" not in state:
            state["metadata"] = {}
        
        state["metadata"]["llm_provider"] = llm_provider
        state["metadata"]["llm_adapter"] = llm_adapter  # æ–°å¢ï¼šæ”¯æŒ LCEL è¯­æ³•
        state["metadata"]["config"] = config
        state["metadata"]["tools"] = tools  # ä¿æŒå‘åå…¼å®¹
        state["metadata"]["langchain_tools"] = langchain_tools  # æ–°å¢ï¼šLangChain æ ‡å‡†å·¥å…·
        
        # Call original invoke
        return original_invoke(state, **kwargs)
    
    # Replace methods
    compiled_graph.ainvoke = ainvoke_with_deps
    compiled_graph.invoke = invoke_with_deps
    
    return compiled_graph


# Map-Reduce wrapper for intent analysis
async def map_intent_analysis(state: ReviewState) -> ReviewState:
    """Map function for intent analysis (processes one file at a time).
    
    This is used with LangGraph's map-reduce capabilities. However, since
    LangGraph's map-reduce might not be directly available, we'll implement
    a custom parallel execution in the intent_analysis_node.
    
    Args:
        state: Current workflow state.
    
    Returns:
        Updated state with file_analyses.
    """
    # For now, we'll process all files in the intent_analysis_node
    # In a full implementation, we could use LangGraph's map capabilities
    # or implement custom parallel execution
    return await intent_analysis_node(state)


async def run_multi_agent_workflow(
    diff_context: str,
    changed_files: List[str],
    config: Config = None,
    lint_errors: List[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """Run the multi-agent workflow for code review.
    
    Args:
        diff_context: The raw Git diff string from the PR.
        changed_files: List of file paths that were changed.
        config: Optional configuration object. If None, uses default config.
        lint_errors: Optional list of linting errors from pre-agent syntax checking.
    
    Returns:
        A dictionary containing the final state with review results.
    """
    if config is None:
        from core.config import Config
        config = Config.load_default()
    
    # Create workflow
    app = create_multi_agent_workflow(config)
    
    # Initialize state
    # é‡æ„è¯´æ˜ï¼šæ·»åŠ  messages å­—æ®µåˆå§‹åŒ–ï¼ˆLangGraph æ ‡å‡†ï¼‰
    initial_state: ReviewState = {
        "messages": [],  # æ–°å¢ï¼šæ¶ˆæ¯å†å²ï¼ˆLangGraph æ ‡å‡†ï¼‰
        "diff_context": diff_context,
        "changed_files": changed_files,
        "file_analyses": [],
        "work_list": [],
        "expert_tasks": {},
        "expert_results": {},
        "confirmed_issues": [],
        "final_report": "",
        "lint_errors": lint_errors or [],
        "metadata": {
            "workflow_version": "multi_agent_parallel",
            "config_provider": config.llm.provider,
            "confidence_threshold": 0.5
        }
    }
    
    # Run the workflow
    print("\n" + "="*80)
    print("ğŸš€ å¤šæ™ºèƒ½ä½“å·¥ä½œæµå¯åŠ¨")
    print("="*80)
    print(f"ğŸ“ è¾“å…¥:")
    print(f"   - Diff ä¸Šä¸‹æ–‡: {len(diff_context)} å­—ç¬¦")
    print(f"   - å˜æ›´æ–‡ä»¶æ•°: {len(changed_files)}")
    print(f"   - Lint é”™è¯¯æ•°: {len(lint_errors) if lint_errors else 0}")
    print("="*80)
    
    try:
        # é‡æ„è¯´æ˜ï¼šå¦‚æœå¯ç”¨äº† checkpointerï¼Œéœ€è¦ä¼ å…¥ config åŒ…å« thread_id
        # ä¸ºæ¯æ¬¡è¿è¡Œç”Ÿæˆå”¯ä¸€çš„ thread_idï¼ˆåŸºäºæ—¶é—´æˆ³å’Œæ–‡ä»¶åˆ—è¡¨ï¼‰
        # æ³¨æ„ï¼šå½“å‰é»˜è®¤ç¦ç”¨ checkpointerï¼Œæ‰€ä»¥é€šå¸¸ä¸éœ€è¦ä¼ å…¥ config
        # ä½†å¦‚æœéœ€è¦å¯ç”¨ï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šå¹¶ä¼ å…¥ config
        invoke_kwargs = {}
        
        # å¯é€‰ï¼šå¦‚æœå¯ç”¨äº† checkpointerï¼Œç”Ÿæˆ thread_id å¹¶ä¼ å…¥ config
        # import hashlib
        # import time
        # thread_id = hashlib.md5(
        #     f"{time.time()}_{','.join(changed_files)}".encode()
        # ).hexdigest()[:16]
        # invoke_kwargs['config'] = {
        #     "configurable": {
        #         "thread_id": thread_id
        #     }
        # }
        
        final_state = await app.ainvoke(initial_state, **invoke_kwargs)
        
        print("\n" + "="*80)
        print("âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
        print("="*80)
        print(f"ğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"   - ç¡®è®¤é—®é¢˜æ•°: {len(final_state.get('confirmed_issues', []))}")
        print(f"   - æŠ¥å‘Šé•¿åº¦: {len(final_state.get('final_report', ''))} å­—ç¬¦")
        print("="*80)
        
        return final_state
    except Exception as e:
        logger.error(f"Workflow execution error: {e}", exc_info=True)
        # Error handling: return error state
        return {
            **initial_state,
            "confirmed_issues": [],
            "final_report": f"Workflow execution error: {str(e)}",
            "metadata": {
                **initial_state.get("metadata", {}),
                "workflow_error": str(e)
            }
        }
