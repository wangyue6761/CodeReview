"""Intent Analysis Node for the code review workflow.

é‡æ„è¯´æ˜ï¼š
- ä½¿ç”¨ LCEL (LangChain Expression Language) è¯­æ³•ï¼šprompt | llm | parser
- è¿™æ˜¯ LangGraph æ ‡å‡†åšæ³•ï¼Œæ›¿ä»£ç›´æ¥è°ƒç”¨ llm_provider.generate()
- èŠ‚ç‚¹æ¥æ”¶ state ä½œä¸ºè¾“å…¥ï¼Œè¿”å› state çš„æ›´æ–°éƒ¨åˆ†ï¼ˆPartial Updateï¼‰

This node implements a Map-Reduce pattern to analyze the intent of changed files
in parallel. Each file is analyzed independently, and results are aggregated.
"""

import asyncio
import logging
import json
import re
from typing import Dict, Any
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.messages import HumanMessage, SystemMessage
from core.state import ReviewState, FileAnalysis, RiskItem, RiskType
from core.llm import LLMProvider
from core.langchain_llm import LangChainLLMAdapter
from agents.prompts import render_prompt_template
from util.diff_utils import generate_context_text_for_file

logger = logging.getLogger(__name__)


async def intent_analysis_node(state: ReviewState) -> Dict[str, Any]:
    """Analyze the intent of all changed files in parallel (Map-Reduce pattern).
    
    This function processes all changed files in parallel and aggregates results.
    
    Args:
        state: Current workflow state with changed_files.
    
    Returns:
        Dictionary with 'file_analyses' key containing a list of FileAnalysis objects.
    """
    print("\n" + "="*80)
    print("ğŸ“‹ [èŠ‚ç‚¹1] Intent Analysis - å¹¶è¡Œåˆ†ææ–‡ä»¶æ„å›¾")
    print("="*80)
    
    # Get LLM provider from metadata (injected by workflow)
    llm_provider: LLMProvider = state.get("metadata", {}).get("llm_provider")
    if not llm_provider:
        logger.error("LLM provider not found in metadata")
        return {"file_analyses": []}
    
    # Get config for concurrency control
    config = state.get("metadata", {}).get("config")
    max_concurrent = config.system.max_concurrent_llm_requests if config else 5
    
    changed_files = state.get("changed_files", [])
    if not changed_files:
        print("  âš ï¸  æ²¡æœ‰éœ€è¦åˆ†æçš„æ–‡ä»¶")
        logger.warning("No changed files to analyze")
        return {"file_analyses": []}
    
    print(f"  ğŸ“ å¾…åˆ†ææ–‡ä»¶æ•°: {len(changed_files)}")
    print(f"  ğŸ”’ å¹¶å‘æ§åˆ¶: Semaphore(max={max_concurrent})")
    print(f"  ğŸ“ æ–‡ä»¶åˆ—è¡¨:")
    for i, file_path in enumerate(changed_files, 1):
        print(f"     {i}. {file_path}")
    
    diff_context = state.get("diff_context", "")
    
    # Create semaphore for concurrency control
    semaphore = asyncio.Semaphore(max_concurrent)
    
    # è·å– LangChain LLM é€‚é…å™¨ï¼ˆä» metadata æˆ–åˆ›å»ºæ–°å®ä¾‹ï¼‰
    llm_adapter = state.get("metadata", {}).get("llm_adapter")
    if not llm_adapter:
        # å¦‚æœæ²¡æœ‰é€‚é…å™¨ï¼Œä» llm_provider åˆ›å»º
        llm_provider = state.get("metadata", {}).get("llm_provider")
        if llm_provider:
            llm_adapter = LangChainLLMAdapter(llm_provider=llm_provider)
        else:
            logger.error("LLM provider not found in metadata")
            return {"file_analyses": []}
    
    # Process all files in parallel
    async def analyze_file(file_path: str) -> FileAnalysis:
        """Analyze a single file using LCEL syntax.
        
        é‡æ„è¯´æ˜ï¼š
        - ä½¿ç”¨ LCEL è¯­æ³•ï¼šprompt | llm | parser
        - æ›¿ä»£ç›´æ¥è°ƒç”¨ llm_provider.generate()
        """
        async with semaphore:
            try:
                print(f"  ğŸ” åˆ†æä¸­: {file_path}")
                # Extract relevant diff section for this file with line numbers
                # TODOï¼š æ€è€ƒä¸€ä¸‹ï¼Œdiffè¦ä¸è¦ä¼ å…¥removeè¡Œ
                file_diff = _extract_file_diff(diff_context, file_path)
                
                # ä½¿ç”¨ LCEL è¯­æ³•åˆ›å»ºé“¾ï¼šprompt | llm | parser
                # é‡æ„è¯´æ˜ï¼šç”±äº render_prompt_template å·²ç»æ¸²æŸ“äº†æ¨¡æ¿ï¼ˆåŒ…å« JSON ç¤ºä¾‹ï¼‰ï¼Œ
                # æˆ‘ä»¬ä¸èƒ½ä½¿ç”¨ ChatPromptTemplateï¼ˆå®ƒä¼šå°è¯•è§£æ JSON ä¸­çš„å¤§æ‹¬å·ä½œä¸ºå˜é‡ï¼‰
                # åº”è¯¥ç›´æ¥ä½¿ç”¨ HumanMessage å’Œ SystemMessage
                
                # è·å–å±é™©æ¨¡å¼é…ç½®ï¼ˆä» metadata æˆ–ä½¿ç”¨é»˜è®¤å€¼ï¼‰
                dangerous_patterns = state.get("metadata", {}).get(
                    "dangerous_patterns",
                    "ï¼ˆå±é™©æ¨¡å¼é…ç½®å°†åœ¨åç»­å¡«å……ï¼‰"
                )
                
                # æ¸²æŸ“æç¤ºæ¨¡æ¿ï¼ˆå·²ç»å®Œæˆå˜é‡æ›¿æ¢ï¼‰
                rendered_prompt = render_prompt_template(
                    "intent_analysis",
                    file_path=file_path,
                    file_diff=file_diff,
                    diff_context=diff_context[:2000],  # Limit context size
                    dangerous_patterns=dangerous_patterns
                )
                
                # é‡æ„è¯´æ˜ï¼šä½¿ç”¨ PydanticOutputParser ç›´æ¥è§£æä¸º FileAnalysis æ¨¡å‹
                # è¿™æ˜¯ LangGraph æ ‡å‡†åšæ³•ï¼Œæ›¿ä»£æ‰‹åŠ¨ JSON è§£æ
                parser = PydanticOutputParser(pydantic_object=FileAnalysis)
                
                # åˆ›å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆç›´æ¥ä½¿ç”¨å·²æ¸²æŸ“çš„æ–‡æœ¬ï¼Œå¹¶æ·»åŠ æ ¼å¼è¯´æ˜ï¼‰
                messages = [
                    SystemMessage(content="You are an expert code reviewer analyzing file changes."),
                    HumanMessage(content=rendered_prompt + "\n\n" + parser.get_format_instructions())
                ]
                
                # ä½¿ç”¨ LCEL è¯­æ³•ï¼šmessages -> llm -> parser
                try:
                    # è°ƒç”¨ LLM
                    response = await llm_adapter.ainvoke(messages, temperature=0.3)
                    # è§£æä¸º Pydantic æ¨¡å‹
                    response_text = response.content if hasattr(response, 'content') else str(response)
                    file_analysis: FileAnalysis = parser.parse(response_text)
                except Exception as e:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œå›é€€åˆ°æ–‡æœ¬è§£æ
                    logger.warning(f"PydanticOutputParser failed for {file_path}, falling back to text parsing: {e}")
                    response_text = response.content if hasattr(response, 'content') else str(response)
                    file_analysis = _parse_intent_analysis_response(response_text, file_path)
                
                print(f"  âœ… å®Œæˆ: {file_path}")
                print(f"     æ„å›¾æ‘˜è¦: {file_analysis.intent_summary[:80]}...")
                print(f"     æ½œåœ¨é£é™©æ•°: {len(file_analysis.potential_risks)}")
                logger.info(f"Analyzed intent for {file_path}: {file_analysis.intent_summary[:100]}...")
                return file_analysis
            except Exception as e:
                logger.error(f"Error analyzing intent for {file_path}: {e}")
                # Return error analysis
                return FileAnalysis(
                    file_path=file_path,
                    intent_summary=f"Error analyzing file: {str(e)}",
                    potential_risks=[],
                    complexity_score=None
                )
    
    # Process all files concurrently
    print(f"\n  ğŸš€ å¼€å§‹å¹¶è¡Œåˆ†æ {len(changed_files)} ä¸ªæ–‡ä»¶...")
    file_analyses = await asyncio.gather(*[analyze_file(f) for f in changed_files])
    
    # Convert Pydantic models to dicts for state (LangGraph TypedDict compatibility)
    file_analyses_dicts = [fa.model_dump() for fa in file_analyses]
    
    total_risks = sum(len(fa.potential_risks) for fa in file_analyses)
    print(f"\n  âœ… Intent Analysis å®Œæˆ!")
    print(f"     - åˆ†ææ–‡ä»¶æ•°: {len(file_analyses)}")
    print(f"     - å‘ç°æ½œåœ¨é£é™©: {total_risks} ä¸ª")
    print("="*80)
    logger.info(f"Completed intent analysis for {len(file_analyses)} files")
    
    return {
        "file_analyses": file_analyses_dicts
    }


def _extract_file_diff(diff_context: str, file_path: str) -> str:
    """Extract the diff section for a specific file with absolute line numbers.
    
    This function uses the unidiff library to parse the Git diff and generate
    code context with absolute line numbers in the new file (HEAD version).
    This enables accurate line number references in review comments.
    
    Args:
        diff_context: Full diff context.
        file_path: Path to the file (relative to repo root).
    
    Returns:
        Formatted code context text with absolute line numbers for the new file.
        Falls back to raw diff section if parsing fails.
    """
    try:
        # Use diff_utils to generate context with line numbers
        context_text = generate_context_text_for_file(
            diff_content=diff_context,
            file_path=file_path,
            include_context_lines=True,
            max_context_lines=5
        )
        
        if context_text:
            return context_text
        else:
            # If no context found, fall back to raw diff extraction
            logger.debug(f"Could not generate context with line numbers for {file_path}, falling back to raw diff")
    except Exception as e:
        # If parsing fails, fall back to raw diff extraction
        logger.warning(f"Failed to parse diff with line numbers for {file_path}: {e}, falling back to raw diff")
    
    # Fallback: Extract raw diff section using regex (original behavior)
    patterns = [
        rf"diff --git.*{re.escape(file_path)}.*?\n(.*?)(?=\ndiff --git|\Z)",
        rf"--- a/{re.escape(file_path)}.*?\n(.*?)(?=\n--- a/|\Z)",
    ]
    
    for pattern in patterns:
        match = re.search(pattern, diff_context, re.DOTALL)
        if match:
            return match.group(0)
    
    # If no specific section found, return a portion of the diff
    return diff_context[:1000] if diff_context else ""


# é‡æ„è¯´æ˜ï¼š_parse_intent_analysis_response_from_dict å‡½æ•°å·²è¢«ç§»é™¤
# ç°åœ¨ä½¿ç”¨ PydanticOutputParser ç›´æ¥è§£æä¸º FileAnalysis æ¨¡å‹
# è¿™æ ·å¯ä»¥ï¼š
# 1. è‡ªåŠ¨éªŒè¯æ‰€æœ‰å­—æ®µç±»å‹ï¼ˆåŒ…æ‹¬ RiskItem ä¸­çš„ line_numberï¼‰
# 2. è‡ªåŠ¨å¤„ç†åµŒå¥—çš„ RiskItem åˆ—è¡¨éªŒè¯
# 3. æä¾›æ›´å¥½çš„é”™è¯¯ä¿¡æ¯
# 4. ç¬¦åˆ LangGraph æ ‡å‡†åšæ³•

def _parse_intent_analysis_response(response: str, file_path: str) -> FileAnalysis:
    """Parse LLM response into FileAnalysis object.
    
    Args:
        response: LLM response string.
        file_path: Path to the analyzed file.
    
    Returns:
        FileAnalysis object.
    """
    try:
        # Try to parse as JSON first
        response_clean = response.strip()
        if response_clean.startswith("```json"):
            response_clean = response_clean[7:]
        if response_clean.startswith("```"):
            response_clean = response_clean[3:]
        if response_clean.endswith("```"):
            response_clean = response_clean[:-3]
        response_clean = response_clean.strip()
        
        try:
            data = json.loads(response_clean)
            intent_summary = data.get("intent_summary", response[:500])
            potential_risks_data = data.get("potential_risks", [])
            complexity_score = data.get("complexity_score")
            
            # Convert potential_risks to RiskItem objects
            potential_risks = []
            for risk_data in potential_risks_data:
                try:
                    # ä¿®å¤è¯´æ˜ï¼šline_number æ˜¯å¿…éœ€å­—æ®µï¼Œä¸èƒ½ä¸º None æˆ–æ— æ•ˆ
                    # å¿…é¡»æä¾› [start, end] æ ¼å¼ï¼Œfield_validator ä¼šéªŒè¯æ ¼å¼
                    line_number = risk_data.get("line_number")
                    if line_number is None:
                        logger.error(f"Missing line_number in risk item: {risk_data}, file_path: {file_path}")
                        continue
                    
                    # field_validator ä¼šéªŒè¯ line_number å¿…é¡»æ˜¯ [start, end] æ ¼å¼
                    # å¦‚æœæ ¼å¼ä¸æ­£ç¡®ä¼šæŠ›å‡º ValueError
                    
                    risk_item = RiskItem(
                        risk_type=RiskType(risk_data.get("risk_type", "null_safety")),
                        file_path=risk_data.get("file_path", file_path),
                        line_number=line_number,  # å¿…é¡»æ˜¯ [start, end] æ ¼å¼
                        description=risk_data.get("description", ""),
                        confidence=risk_data.get("confidence", 0.5),
                        severity=risk_data.get("severity", "info"),
                        suggestion=risk_data.get("suggestion")
                    )
                    potential_risks.append(risk_item)
                except Exception as e:
                    logger.error(f"Failed to parse risk item: {e}, risk_data: {risk_data}, file_path: {file_path}")
                    continue
            
            return FileAnalysis(
                file_path=file_path,
                intent_summary=intent_summary,
                potential_risks=potential_risks,
                complexity_score=complexity_score
            )
        except json.JSONDecodeError:
            # If JSON parsing fails, create a simple FileAnalysis from text
            return FileAnalysis(
                file_path=file_path,
                intent_summary=response[:500],
                potential_risks=[],
                complexity_score=None
            )
    except Exception as e:
        logger.error(f"Error parsing intent analysis response: {e}")
        return FileAnalysis(
            file_path=file_path,
            intent_summary=f"Error parsing response: {str(e)}",
            potential_risks=[],
            complexity_score=None
        )
