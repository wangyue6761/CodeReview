"""ä¸“å®¶åˆ†æå­å›¾ã€‚
ä½¿ç”¨ LangGraph å­å›¾æ¨¡å¼å®ç°ä¸“å®¶æ™ºèƒ½ä½“çš„å·¥å…·è°ƒç”¨å¾ªç¯ã€‚
"""

import logging
import os
import json
from typing import List, Optional, Any, Dict
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, BaseMessage, AIMessage
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.language_models import BaseChatModel
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from core.state import RiskItem, ExpertState
from core.config import Config
from langchain_core.tools import BaseTool
from agents.prompts import render_prompt_template
from util.json_utils import extract_json_from_text

logger = logging.getLogger(__name__)

def _log_http_error_details(err: Exception, *, max_body_chars: int = 4000) -> None:
    """Best-effort logging for provider HTTP errors (e.g. httpx.HTTPStatusError)."""
    try:
        resp = getattr(err, "response", None)
        req = getattr(err, "request", None)
        if resp is None:
            return

        status = getattr(resp, "status_code", None)
        url = None
        try:
            url = str(getattr(req, "url", None) or getattr(resp, "url", None))
        except Exception:
            url = None

        body = ""
        try:
            body = getattr(resp, "text", "") or ""
        except Exception:
            body = ""

        if isinstance(body, bytes):
            body = body.decode("utf-8", errors="replace")
        if not isinstance(body, str):
            body = str(body)

        body = body.strip()
        if len(body) > max_body_chars:
            body = body[:max_body_chars] + "\n...[truncated]..."

        logger.error(f"LLM HTTP error details: status={status} url={url}")
        if body:
            logger.error(f"LLM HTTP error response body:\n{body}")
        else:
            logger.error("LLM HTTP error response body: <empty>")
    except Exception:
        # Never let diagnostics crash the workflow
        return


def create_langchain_tools(
    workspace_root: Optional[str] = None,
    asset_key: Optional[str] = None
) -> List[BaseTool]:
    """åˆ›å»º LangChain å·¥å…·åˆ—è¡¨ã€‚
    
    ç»Ÿä¸€ä½¿ç”¨ langchain_tools.create_tools_with_context åˆ›å»ºæ ‡å‡†å·¥å…·ã€‚
    
    Args:
        workspace_root: å·¥ä½œåŒºæ ¹ç›®å½•ï¼ˆç”¨äºå·¥å…·ä¸Šä¸‹æ–‡ï¼‰ã€‚
        asset_key: ä»“åº“æ˜ å°„çš„èµ„äº§é”®ï¼ˆç”¨äº fetch_repo_mapï¼‰ã€‚
    
    Returns:
        LangChain å·¥å…·åˆ—è¡¨ï¼šfetch_repo_map, read_file, run_grepã€‚
    """
    from tools.langchain_tools import create_tools_with_context
    from pathlib import Path
    
    if workspace_root:
        workspace_path = Path(workspace_root)
        return create_tools_with_context(
            workspace_root=workspace_path,
            asset_key=asset_key
        )
    else:
        # å¦‚æœæ²¡æœ‰ workspace_rootï¼Œä»ç„¶åˆ›å»ºå·¥å…·ï¼ˆä½¿ç”¨é»˜è®¤å€¼ï¼‰
        return create_tools_with_context(
            workspace_root=None,
            asset_key=asset_key
        )


def tools_condition(state: ExpertState) -> str:
    """æ¡ä»¶è·¯ç”±å‡½æ•°ï¼šæ ¹æ®æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨æ¥å†³å®šè·¯ç”±ã€‚
    
    Args:
        state: ä¸“å®¶å­å›¾çŠ¶æ€ã€‚
    
    Returns:
        "tools" å¦‚æœæœ€åä¸€æ¡æ¶ˆæ¯åŒ…å«å·¥å…·è°ƒç”¨ï¼Œå¦åˆ™ "end"ã€‚
    """
    messages = state.get("messages", [])
    if not messages:
        return "end"
    
    last_message = messages[-1]
    # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    
    return "end"


def build_expert_graph(
    llm: BaseChatModel,
    tools: List[BaseTool],
    config: Optional[Config] = None,
) -> Any:
    """æ„å»ºä¸“å®¶åˆ†æå­å›¾ã€‚
    
    å­å›¾ç»“æ„ï¼š
    1. reasoner èŠ‚ç‚¹ï¼šè°ƒç”¨ LLM è¿›è¡Œåˆ†æ
    2. tools èŠ‚ç‚¹ï¼šæ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆå¦‚æœ LLM è¿”å›å·¥å…·è°ƒç”¨ï¼‰
    3. æ¡ä»¶è·¯ç”±ï¼šæ ¹æ®æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨å†³å®šç»§ç»­æˆ–ç»“æŸ
    
    Args:
        llm: LangChain æ ‡å‡† ChatModelã€‚
        tools: LangChain å·¥å…·åˆ—è¡¨ã€‚
        config: é…ç½®å¯¹è±¡ï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºè·å–æœ€å¤§è½®æ¬¡é™åˆ¶ã€‚
    
    Returns:
        ç¼–è¯‘åçš„ LangGraph å­å›¾ã€‚
    """
    # ç»‘å®šå·¥å…·åˆ° LLM
    llm_with_tools = llm.bind_tools(tools)
    
    # åˆ›å»ºå·¥å…·èŠ‚ç‚¹
    tool_node = ToolNode(tools)
    
    # åˆ›å»º Pydantic è§£æå™¨
    parser = PydanticOutputParser(pydantic_object=RiskItem)
    format_instructions = parser.get_format_instructions()
    
    # æ ¼å¼åŒ–å¯ç”¨å·¥å…·æè¿°
    tool_descriptions = []
    for tool in tools:
        desc = getattr(tool, 'description', f'Tool: {tool.name}')
        tool_descriptions.append(f"- **{tool.name}**: {desc}")
    available_tools_text = "\n".join(tool_descriptions)

    def _truncate_text(s: str, max_chars: int) -> str:
        if max_chars <= 0:
            return ""
        if s is None:
            return ""
        if len(s) <= max_chars:
            return s
        return s[:max_chars] + "\n...[truncated]..."

    def _stringify_content(content: Any) -> str:
        if content is None:
            return ""
        if isinstance(content, str):
            return content
        if isinstance(content, bytes):
            return content.decode("utf-8", errors="replace")
        try:
            return json.dumps(content, ensure_ascii=False, default=str)
        except Exception:
            return str(content)

    def _copy_with_content(msg: BaseMessage, content: str) -> BaseMessage:
        # langchain_core messages are pydantic models (v1/v2 depending on version)
        if hasattr(msg, "model_copy"):
            return msg.model_copy(update={"content": content})
        if hasattr(msg, "copy"):
            return msg.copy(update={"content": content})  # type: ignore[attr-defined]
        # Fallback: best-effort reconstruct common types
        if isinstance(msg, ToolMessage):
            return ToolMessage(content=content, tool_call_id=getattr(msg, "tool_call_id", ""))
        if isinstance(msg, HumanMessage):
            return HumanMessage(content=content)
        if isinstance(msg, SystemMessage):
            return SystemMessage(content=content)
        return msg

    def _shrink_history(messages: List[BaseMessage]) -> List[BaseMessage]:
        """Hard budget for LLM context: cap history length + truncate tool payloads.

        Notes:
        - Tool results can be very large; truncating them reduces context blowups.
        - Some tool runners may attach non-string content; we stringify to keep request payloads valid.
        """
        try:
            max_history = int(os.environ.get("EXPERT_MAX_HISTORY_MESSAGES", "16"))
        except Exception:
            max_history = 16
        try:
            max_total_chars = int(os.environ.get("EXPERT_MAX_TOTAL_CHARS", "80000"))
        except Exception:
            max_total_chars = 80000
        try:
            max_tool_chars = int(os.environ.get("EXPERT_MAX_TOOL_CHARS", "6000"))
        except Exception:
            max_tool_chars = 6000

        max_history = max(1, max_history)
        max_total_chars = max(10_000, max_total_chars)
        max_tool_chars = max(500, max_tool_chars)

        # 1) keep only the most recent messages (history is append-only via add_messages)
        tail = messages[-max_history:]

        # 2) truncate oversized tool messages (biggest offender)
        clipped: List[BaseMessage] = []
        for m in tail:
            c = getattr(m, "content", "")
            if isinstance(m, ToolMessage):
                # Ensure tool message content is string-serializable (provider adapters vary in strictness).
                c_str = _stringify_content(c)
                if len(c_str) > max_tool_chars:
                    c_str = _truncate_text(c_str, max_tool_chars)
                clipped.append(_copy_with_content(m, c_str))
                continue
            clipped.append(m)

        # 3) enforce total budget by dropping oldest remaining messages
        def total_chars(msgs: List[BaseMessage]) -> int:
            n = 0
            for x in msgs:
                cc = getattr(x, "content", "")
                if isinstance(cc, str):
                    n += len(cc)
            return n

        while len(clipped) > 1 and total_chars(clipped) > max_total_chars:
            clipped.pop(0)
        return clipped
    
    async def handle_circuit_breaker(
        messages: List[BaseMessage],
        current_round: int,
        max_rounds: int,
        raw_llm: BaseChatModel,
        format_instructions: str,
        risk_context: RiskItem
    ) -> Optional[Dict[str, Any]]:
        """å¤„ç†è½®æ¬¡ç†”æ–­é€»è¾‘ï¼ˆç‰©ç†ç†”æ–­ç‰ˆæœ¬ï¼‰ã€‚
        
        Args:
            messages: å½“å‰æ¶ˆæ¯åˆ—è¡¨ã€‚
            max_rounds: æœ€å¤§è½®æ¬¡é™åˆ¶ã€‚
            raw_llm: æœªç»‘å®šå·¥å…·çš„åŸå§‹æ¨¡å‹å®ä¾‹ã€‚
            format_instructions: Pydantic Parser çš„æ ¼å¼è¯´æ˜ã€‚
            risk_context: é£é™©é¡¹ä¸Šä¸‹æ–‡ã€‚
        
        Returns:
            å¦‚æœè§¦å‘ç†”æ–­ï¼Œè¿”å›åŒ…å«å¼ºåˆ¶ç»“æŸå“åº”çš„çŠ¶æ€ï¼›å¦åˆ™è¿”å› Noneã€‚
        """
        if current_round >= max_rounds:
            # è§¦å‘ç†”æ–­ï¼šæ„é€ å¼ºåˆ¶ç»“æŸæç¤º
            logger.warning(f"Circuit breaker triggered: {current_round} rounds >= {max_rounds} max rounds")
            
            # æ„å»ºå®Œæ•´çš„å¼ºåˆ¶åœæ­¢æç¤ºè¯
            force_stop_content = f"""âš ï¸ **ç´§æ€¥åœæ­¢ï¼šåˆ†æè½®æ¬¡å·²è¾¾ä¸Šé™ ({current_round} >= {max_rounds})**

                **è¯·ç«‹å³åœæ­¢è°ƒç”¨ä»»ä½•å·¥å…·ï¼ç›´æ¥æœ€ç»ˆåˆ†æï¼**

                è¯·æ ¹æ®ç›®å‰å·²æ”¶é›†åˆ°çš„ä¿¡æ¯ï¼Œ**ç›´æ¥è¾“å‡ºæœ€ç»ˆçš„ JSON ç»“æœ**ã€‚
                å³ä½¿ä¿¡æ¯ä¸å®Œæ•´ï¼Œä¹Ÿè¦åŸºäºç°æœ‰è¯æ®ç»™å‡ºåˆ¤æ–­ã€‚

                ## å½“å‰ä»»åŠ¡é”šç‚¹
                é£é™©ç±»å‹: {risk_context.risk_type.value}
                æ–‡ä»¶è·¯å¾„: {risk_context.file_path}
                è¡Œå·èŒƒå›´: {risk_context.line_number[0]}:{risk_context.line_number[1]}
                æè¿°: {risk_context.description}

                ## è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
                {format_instructions}

                **é‡è¦ï¼šä½ å¿…é¡»è¾“å‡ºä¸€ä¸ªæœ‰æ•ˆçš„ JSON å¯¹è±¡ï¼Œæ ¼å¼å¿…é¡»å®Œå…¨ç¬¦åˆä¸Šè¿°è¦æ±‚ã€‚ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæ€§æ–‡å­—ï¼Œåªè¾“å‡º JSONã€‚**"""
            
            force_stop_msg = SystemMessage(content=force_stop_content)
            
            # æ‰§è¡Œå¼ºåˆ¶æ¨ç†ï¼šæ„é€ æ¶ˆæ¯åˆ—è¡¨
            # TODO: å¼ºåˆ¶å…œåº•å›å¤ï¼Œä¸ä¼ å…¥å†å²å¯¹è¯ï¼Œå› ä¸ºä¼ å…¥å†å²å¯¹è¯æ¨¡å‹ä¼šç»§ç»­é—®å·¥å…·ï¼Œå› æ­¤ç›´æ¥å…œåº•å›å¤
            # Some providers require a user turn; keep it minimal to avoid 400s.
            new_messages = [
                force_stop_msg,
                HumanMessage(content="è¯·ç›´æ¥è¾“å‡ºæœ€ç»ˆ JSONï¼ˆä¸è¦è°ƒç”¨å·¥å…·ï¼Œä¸è¦è¾“å‡ºè§£é‡Šï¼‰ã€‚"),
            ]
            
            # å…³é”®ï¼šä½¿ç”¨åŸå§‹ LLMï¼ˆæœªç»‘å®šå·¥å…·ï¼‰ï¼Œç‰©ç†ä¸Šåˆ‡æ–­å·¥å…·è°ƒç”¨è·¯å¾„
            try:
                response = await raw_llm.ainvoke(new_messages)
            except Exception as e:
                # If provider rejects the request (e.g., 400) or is unavailable, return a minimal valid JSON
                # so the pipeline can proceed with a conservative, low-confidence result.
                logger.error(f"Circuit breaker fallback LLM call failed: {type(e).__name__}: {e}")
                fallback_json = {
                    "risk_type": risk_context.risk_type.value,
                    "file_path": risk_context.file_path,
                    "line_number": [risk_context.line_number[0], risk_context.line_number[1]],
                    "description": risk_context.description,
                    "confidence": 0.0,
                    "severity": "info",
                    "suggestion": None,
                }
                return {"messages": [SystemMessage(content=json.dumps(fallback_json, ensure_ascii=False))]}
            
            if hasattr(response, "tool_calls"):
                response.tool_calls = []
            
            return {
                "messages": [response]
            }
        
        return None
    
    def build_system_message(
        risk_context: RiskItem,
        risk_type_str: str,
        file_content: str
    ) -> SystemMessage:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯æ¶ˆæ¯ã€‚
        
        Args:
            risk_context: é£é™©é¡¹ä¸Šä¸‹æ–‡ã€‚
            risk_type_str: é£é™©ç±»å‹å­—ç¬¦ä¸²ã€‚
            file_content: æ–‡ä»¶å®Œæ•´å†…å®¹ï¼ˆå¯é€‰ï¼‰ã€‚
        
        Returns:
            æ„å»ºå¥½çš„ SystemMessageã€‚
        """
        # è·å–åŸºç¡€ç³»ç»Ÿæç¤ºè¯
        try:
            base_system_prompt = render_prompt_template(
                f"expert_{risk_type_str}",
                risk_type=risk_type_str,
                available_tools=available_tools_text,
                validation_logic_examples=""
            )
        except FileNotFoundError:
            # å›é€€åˆ°é€šç”¨æç¤ºè¯
            base_system_prompt = render_prompt_template(
                "expert_generic",
                risk_type=risk_type_str,
                available_tools=available_tools_text
            )
        
        # æ„å»ºå®Œæ•´çš„ SystemMessage å†…å®¹
        system_content = f"""{base_system_prompt}
            ## å½“å‰ä»»åŠ¡é”šç‚¹
            é£é™©ç±»å‹: {risk_context.risk_type.value}
            æ–‡ä»¶è·¯å¾„: {risk_context.file_path}
            è¡Œå·èŒƒå›´: {risk_context.line_number[0]}:{risk_context.line_number[1]}
            æè¿°: {risk_context.description}"""

        if file_content:
            # IMPORTANT: Do not inject full file content into the SystemMessage.
            # It can easily exceed model context (e.g. 260k+ tokens). Provide a focused window.
            try:
                start_line, end_line = int(risk_context.line_number[0]), int(risk_context.line_number[1])
            except Exception:
                start_line, end_line = 1, 1
            window = 200
            lines = file_content.splitlines()
            lo = max(1, start_line - window)
            hi = min(len(lines), end_line + window)
            snippet = "\n".join(f"{i}: {lines[i-1]}" for i in range(lo, hi + 1))

            system_content += f"""
            ## æ–‡ä»¶å†…å®¹ï¼ˆå·²æˆªå–çª—å£ï¼‰
            ä¸‹é¢ä»…æä¾›ä¸é£é™©è¡Œå·ç›¸å…³çš„å±€éƒ¨çª—å£ï¼ˆ{lo}-{hi}ï¼‰ã€‚å¦‚éœ€æ›´å¤šä¸Šä¸‹æ–‡ï¼Œè¯·ä½¿ç”¨ read_file å·¥å…·æŒ‰éœ€è¯»å–ï¼ˆå»ºè®®é™åˆ¶ max_linesï¼‰ã€‚

            {snippet}"""

        system_content += f"""
            ## è¾“å‡ºæ ¼å¼è¦æ±‚
            {format_instructions}
            """
        
        return SystemMessage(content=system_content)
    
    # å®šä¹‰ reasoner èŠ‚ç‚¹ï¼ˆå¼‚æ­¥ï¼‰
    async def reasoner(state: ExpertState) -> ExpertState:
        """æ¨ç†èŠ‚ç‚¹ï¼šè°ƒç”¨ LLM è¿›è¡Œåˆ†æã€‚
        
        ç¬¬ä¸€è½®åŠ¨æ€æ„å»ºåŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡çš„ SystemMessageï¼Œåç»­è½®æ¬¡ç›´æ¥ä½¿ç”¨å†å²æ¶ˆæ¯ã€‚
        åŒ…å«è½®æ¬¡ç†”æ–­æœºåˆ¶ï¼Œé˜²æ­¢æ— é™å¾ªç¯ã€‚
        """
        messages = state.get("messages", [])
        risk_context = state.get("risk_context")
        file_content = state.get("file_content", "")
        risk_type_str = risk_context.risk_type.value
        
        # è®¡ç®—å½“å‰è½®æ¬¡ï¼šåªç»Ÿè®¡æ¨¡å‹è¾“å‡ºè½®æ¬¡ï¼ˆå·¥å…·æ¶ˆæ¯ä¸è®¡å…¥ï¼‰ã€‚
        current_round = 1 + sum(1 for m in messages if isinstance(m, AIMessage))
        line_start, line_end = risk_context.line_number
        print(f"  ğŸ” [ä¸“å®¶åˆ†æ] ç¬¬ {current_round} è½® | [{risk_type_str}] {risk_context.file_path}:{line_start}-{line_end}")
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_msg = build_system_message(risk_context, risk_type_str, file_content)

        # æ£€æŸ¥è½®æ¬¡ï¼šå¦‚æœè¶…è¿‡æœ€å¤§è½®æ¬¡ï¼Œè§¦å‘ç‰©ç†ç†”æ–­
        max_rounds = config.system.max_expert_rounds if config else 20
        circuit_breaker_result = await handle_circuit_breaker(
            [*messages], 
            current_round,
            max_rounds,
            llm,  # ä¼ å…¥åŸå§‹ LLMï¼ˆæœªç»‘å®šå·¥å…·ï¼‰
            format_instructions,  # ä¼ å…¥æ ¼å¼è¯´æ˜
            risk_context  # ä¼ å…¥é£é™©ä¸Šä¸‹æ–‡
        )
        if circuit_breaker_result is not None:
            return circuit_breaker_result
        
        if not messages:
            # æ„å»ºåˆå§‹ UserMessage
            user_msg_content = "è¯·åˆ†æä¸Šè¿°é£é™©é¡¹ã€‚å¦‚æœéœ€è¦æ›´å¤šä¿¡æ¯ï¼Œè¯·è°ƒç”¨å·¥å…·ã€‚åˆ†æå®Œæˆåï¼Œè¯·è¾“å‡ºæœ€ç»ˆçš„ JSON ç»“æœã€‚"
            user_msg = HumanMessage(content=user_msg_content)
            new_messages = [system_msg, user_msg]
        else:
            # åç»­è½®æ¬¡ï¼šç›´æ¥ä½¿ç”¨å†å²æ¶ˆæ¯ï¼ˆSystemMessage å·²åœ¨ç¬¬ä¸€è½®æ·»åŠ ï¼‰
            new_messages = [system_msg, *_shrink_history([*messages])]
        
        # è°ƒç”¨ LLMï¼ˆå¼‚æ­¥ï¼‰
        try:
            response = await llm_with_tools.ainvoke(new_messages)
        except Exception as e:
            # Print 400 body (or any HTTP error body) to quickly diagnose request format/size/tool support issues.
            _log_http_error_details(e)
            raise
        
        # Ensure the initial user turn becomes part of state history, so subsequent rounds
        # have a valid dialogue sequence (some providers validate this strictly).
        if not messages:
            return {"messages": [user_msg, response]}
        return {"messages": [response]}
    
    # æ„å»ºå›¾
    graph = StateGraph(ExpertState)
    
    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("reasoner", reasoner)
    graph.add_node("tools", tool_node)
    
    # è®¾ç½®å…¥å£ç‚¹
    graph.set_entry_point("reasoner")
    
    # æ·»åŠ æ¡ä»¶è¾¹
    graph.add_conditional_edges(
        "reasoner",
        tools_condition,
        {
            "tools": "tools",
            "end": END
        }
    )
    
    # å·¥å…·æ‰§è¡Œåå›åˆ° reasoner
    graph.add_edge("tools", "reasoner")
    
    # ç¼–è¯‘å›¾
    return graph.compile()


async def run_expert_analysis(
    graph: Any,
    risk_item: RiskItem,
    diff_context: Optional[str] = None,
    file_content: Optional[str] = None,
    recursion_limit: Optional[int] = None,
) -> Optional[dict]:
    """è¿è¡Œä¸“å®¶åˆ†æå­å›¾ã€‚
    
    Args:
        graph: ç¼–è¯‘åçš„ä¸“å®¶å­å›¾ã€‚
        risk_item: å¾…åˆ†æçš„é£é™©é¡¹ã€‚
        risk_type_str: é£é™©ç±»å‹å­—ç¬¦ä¸²ï¼ˆç”¨äºæ¸²æŸ“æç¤ºè¯æ¨¡æ¿ï¼‰ã€‚
        diff_context: æ–‡ä»¶çš„ diff ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰ã€‚
        file_content: æ–‡ä»¶çš„å®Œæ•´å†…å®¹ï¼ˆå¯é€‰ï¼‰ã€‚
    
    Returns:
        åŒ…å« 'result' å’Œ 'messages' çš„å­—å…¸ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› Noneã€‚
        - result: æœ€ç»ˆéªŒè¯ç»“æœï¼ˆRiskItem å¯¹è±¡ï¼‰
        - messages: å¯¹è¯å†å²ï¼ˆæ¶ˆæ¯åˆ—è¡¨ï¼‰
    """
    try:
        # åˆ›å»º Pydantic è§£æå™¨
        parser = PydanticOutputParser(pydantic_object=RiskItem)
        
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state: ExpertState = {
            "messages": [],
            "risk_context": risk_item,
            "final_result": None,
            "diff_context": diff_context,
            "file_content": file_content
        }
        
        # è¿è¡Œå­å›¾
        invoke_kwargs: Dict[str, Any] = {}
        if recursion_limit is not None:
            invoke_kwargs["config"] = {"recursion_limit": int(recursion_limit)}
        final_state = await graph.ainvoke(initial_state, **invoke_kwargs)
        
        # ä»æ¶ˆæ¯ä¸­æå–æœ€åä¸€æ¡æ¶ˆæ¯çš„æ–‡æœ¬å†…å®¹
        messages = final_state.get("messages", [])
        if not messages:
            logger.warning("No messages in final state")
            return None
        
        # è·å–æœ€åä¸€æ¡æ¶ˆæ¯çš„æ–‡æœ¬å†…å®¹
        last_message = messages[-1]
        response_text = last_message.content if hasattr(last_message, "content") else str(last_message)
        
        # ä»å“åº”æ–‡æœ¬ä¸­æå– JSON
        json_text = extract_json_from_text(response_text)
        if not json_text:
            logger.warning("Could not extract JSON from response")
            logger.warning(f"Response text (first 500 chars): {response_text[:500]}")
            return None
        
        # ä½¿ç”¨ PydanticOutputParser è§£ææå–çš„ JSON
        try:
            result: RiskItem = parser.parse(json_text)
        except Exception as e:
            logger.warning(f"PydanticOutputParser failed to parse extracted JSON: {e}")
            logger.warning(f"Extracted JSON (first 500 chars): {json_text[:500]}")
            logger.warning(f"Original response (first 500 chars): {response_text[:500]}")
            return None
        
        return {
            "result": result,
            "messages": messages
        }
        
    except Exception as e:
        import traceback
        error_msg = str(e) if str(e) else type(e).__name__
        error_traceback = traceback.format_exception(type(e), e, e.__traceback__)
        logger.error(f"Error running expert analysis: {error_msg}")
        logger.error(f"Traceback:\n{''.join(error_traceback)}")
        return None
