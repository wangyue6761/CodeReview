"""è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬ï¼Œä» test_prs.json è¯»å–æµ‹è¯•ç”¨ä¾‹å¹¶æ‰¹é‡è¿è¡Œä»£ç å®¡æŸ¥ã€‚

å·¥ä½œæµç¨‹ï¼š
1. è¯»å– test_prs.json (1-1638è¡Œ)
2. è§£ææµ‹è¯•ç”¨ä¾‹ï¼ˆä»“åº“åã€PRå·ã€base/headåˆ†æ”¯ï¼‰
3. æ ¹æ®å‘½ä»¤è¡Œå‚æ•°è¿‡æ»¤ç”¨ä¾‹
4. å¯¹æ¯ä¸ªç”¨ä¾‹è°ƒç”¨ä»£ç å®¡æŸ¥ç³»ç»Ÿ
5. å°†ç»“æœä¿å­˜ä¸ºå¸¦ä»“åº“å’ŒPRæ ‡è¯†çš„æ–‡ä»¶
"""

import asyncio
import argparse
import json
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥ä¸»å®¡æŸ¥å‡½æ•°
from main import run_review


def extract_pr_number(prlink: str) -> Optional[int]:
    """ä» prlink æå–PRå·ã€‚
    
    Args:
        prlink: PRé“¾æ¥ï¼Œæ ¼å¼å¦‚ "https://github.com/ai-code-review-evaluation/sentry-greptile/pull/1"
    
    Returns:
        PRå·ï¼Œå¦‚æœè§£æå¤±è´¥è¿”å› None
    """
    # åŒ¹é…æ ¼å¼: .../pull/{number}
    pattern = r"/pull/(\d+)"
    match = re.search(pattern, prlink)
    
    if match:
        return int(match.group(1))
    
    return None


def parse_cases_range(cases_str: str) -> Tuple[int, int]:
    """è§£æcaseèŒƒå›´å­—ç¬¦ä¸²ï¼Œå¦‚ "1-10"ã€‚
    
    Args:
        cases_str: èŒƒå›´å­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚ "1-10" æˆ– "1"
    
    Returns:
        (start, end) å…ƒç»„ï¼Œendä¸º-1è¡¨ç¤ºå…¨éƒ¨
    """
    if not cases_str:
        return (1, -1)  # é»˜è®¤å…¨éƒ¨
    
    # æ”¯æŒæ ¼å¼: "1-10" æˆ– "10"
    if "-" in cases_str:
        parts = cases_str.split("-", 1)
        start = int(parts[0])
        end = int(parts[1])
        return (start, end)
    else:
        # å•ä¸ªæ•°å­—ï¼Œè¡¨ç¤ºå‰Nä¸ª
        num = int(cases_str)
        return (1, num)


def load_test_cases(test_file: Path) -> Dict:
    """åŠ è½½æµ‹è¯•ç”¨ä¾‹JSONæ–‡ä»¶ã€‚
    
    Args:
        test_file: æµ‹è¯•æ–‡ä»¶è·¯å¾„ï¼ˆåº”è¯¥æ˜¯æå–åçš„ test_cases.jsonï¼‰
    
    Returns:
        è§£æåçš„JSONå­—å…¸
    """
    with open(test_file, "r", encoding="utf-8") as f:
        return json.load(f)


def collect_all_cases(test_data: Dict) -> List[Dict]:
    """æ”¶é›†æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹ï¼ŒæŒ‰é¡ºåºæ’åˆ—ã€‚
    
    Args:
        test_data: ä»test_prs.jsonåŠ è½½çš„æ•°æ®
    
    Returns:
        ç”¨ä¾‹åˆ—è¡¨ï¼Œæ¯ä¸ªç”¨ä¾‹åŒ…å«ï¼šrepo_group, case_name, case_data
    """
    all_cases = []
    
    for repo_group, cases in test_data.items():
        for case_name, case_data in cases.items():
            all_cases.append({
                "repo_group": repo_group,
                "case_name": case_name,
                "case_data": case_data
            })
    
    return all_cases


def filter_cases(
    all_cases: List[Dict],
    repos: Optional[List[str]] = None,
    cases_range: Optional[Tuple[int, int]] = None
) -> List[Dict]:
    """æ ¹æ®å‚æ•°è¿‡æ»¤ç”¨ä¾‹ã€‚
    
    Args:
        all_cases: æ‰€æœ‰ç”¨ä¾‹åˆ—è¡¨
        repos: è¦æµ‹è¯•çš„ä»“åº“åˆ†ç»„åˆ—è¡¨ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
        cases_range: caseèŒƒå›´ (start, end)ï¼Œendä¸º-1è¡¨ç¤ºå…¨éƒ¨
    
    Returns:
        è¿‡æ»¤åçš„ç”¨ä¾‹åˆ—è¡¨
    """
    filtered = []
    
    for i, case in enumerate(all_cases, 1):
        # è¿‡æ»¤ä»“åº“
        if repos and case["repo_group"] not in repos:
            continue
        
        # è¿‡æ»¤caseèŒƒå›´
        if cases_range:
            start, end = cases_range
            if end == -1:
                # å…¨éƒ¨
                pass
            elif i < start or i > end:
                continue
        
        filtered.append(case)
    
    return filtered


def enhance_results_with_metadata(
    results_file: Path,
    repo_name: str,
    pr_number: int,
    case_name: str
) -> None:
    """åœ¨ç»“æœJSONæ–‡ä»¶ä¸­æ·»åŠ å…ƒæ•°æ®ã€‚
    
    Args:
        results_file: ç»“æœæ–‡ä»¶è·¯å¾„
        repo_name: ä»“åº“å
        pr_number: PRå·
        case_name: caseåç§°
    """
    with open(results_file, "r", encoding="utf-8") as f:
        results = json.load(f)
    
    # æ·»åŠ å…ƒæ•°æ®
    if "metadata" not in results:
        results["metadata"] = {}
    
    results["metadata"]["repo_name"] = repo_name
    results["metadata"]["pr_number"] = pr_number
    results["metadata"]["case_name"] = case_name
    results["metadata"]["test_timestamp"] = datetime.now().isoformat()
    
    # ä¿å­˜å›æ–‡ä»¶
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)


async def run_single_test(
    case: Dict,
    datasets_dir: Path,
    output_dir: Path,
    quiet: bool = False
) -> Tuple[bool, str]:
    """è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹ã€‚
    
    Args:
        case: æµ‹è¯•ç”¨ä¾‹å­—å…¸
        datasets_dir: æ•°æ®é›†ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        quiet: æ˜¯å¦é™é»˜æ¨¡å¼
    
    Returns:
        (success, message) å…ƒç»„
    """
    case_data = case["case_data"]
    prlink = case_data.get("prlink", "")
    case_name = case["case_name"]
    
    # ä»case_dataä¸­è·å–ä»“åº“åï¼ˆæ ¼å¼ï¼š{repo_group}-greptileï¼‰
    repo_name = case_data.get("repo_name")
    if not repo_name:
        return (False, f"case_dataä¸­ç¼ºå°‘repo_nameå­—æ®µ")
    
    # ä»prlinkæå–PRå·
    pr_number = extract_pr_number(prlink)
    if not pr_number:
        return (False, f"æ— æ³•ä»prlinkæå–PRå·: {prlink}")
    
    # æ„å»ºä»“åº“è·¯å¾„
    repo_path = datasets_dir / repo_name
    if not repo_path.exists():
        return (False, f"ä»“åº“ç›®å½•ä¸å­˜åœ¨: {repo_path}")
    
    # è·å–baseå’Œheadåˆ†æ”¯
    base_branch = case_data.get("base_branch")
    head_branch = case_data.get("head_branch")
    
    if not base_branch or not head_branch:
        return (False, f"base_branchæˆ–head_branchä¸ºnull: base={base_branch}, head={head_branch}")
    
    # æ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
    output_file = output_dir / f"review_results_{repo_name}_pr{pr_number}.json"
    
    # è¿è¡Œå®¡æŸ¥
    try:
        if not quiet:
            print(f"\n{'='*80}")
            print(f"æµ‹è¯•ç”¨ä¾‹: {case_name}")
            print(f"ä»“åº“: {repo_name}, PR: {pr_number}")
            print(f"åˆ†æ”¯: {base_branch} -> {head_branch}")
            print(f"{'='*80}")
        
        exit_code = await run_review(
            repo_path=repo_path,
            base_branch=base_branch,
            head_branch=head_branch,
            output_file=output_file,
            quiet=quiet
        )
        
        if exit_code != 0:
            return (False, f"å®¡æŸ¥å¤±è´¥ï¼Œé€€å‡ºç : {exit_code}")
        
        # å¢å¼ºç»“æœæ–‡ä»¶ï¼Œæ·»åŠ å…ƒæ•°æ®
        enhance_results_with_metadata(
            results_file=output_file,
            repo_name=repo_name,
            pr_number=pr_number,
            case_name=case_name
        )
        
        return (True, f"æˆåŠŸ: {output_file}")
        
    except Exception as e:
        return (False, f"æ‰§è¡Œå¼‚å¸¸: {str(e)}")


def parse_arguments() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚
    
    Returns:
        è§£æåçš„å‚æ•°å‘½åç©ºé—´
    """
    parser = argparse.ArgumentParser(
        description="è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬ - æ‰¹é‡è¿è¡Œä»£ç å®¡æŸ¥",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æµ‹è¯•æ‰€æœ‰ç”¨ä¾‹
  python test/run_automated_tests.py --datasets-dir ./datasets
  
  # æµ‹è¯•å‰10ä¸ªç”¨ä¾‹
  python test/run_automated_tests.py --datasets-dir ./datasets --cases 1-10
  
  # æµ‹è¯•æŒ‡å®šä»“åº“çš„å‰5ä¸ªç”¨ä¾‹
  python test/run_automated_tests.py --datasets-dir ./datasets --repos sentry,grafana --cases 1-5
        """
    )
    
    parser.add_argument(
        "--datasets-dir",
        type=str,
        required=True,
        help="è¢«æµ‹ä»“åº“æ•°æ®é›†ç›®å½•ï¼ˆå¿…éœ€ï¼‰"
    )
    
    parser.add_argument(
        "--repos",
        type=str,
        default=None,
        help="è¦æµ‹è¯•çš„ä»“åº“åˆ—è¡¨ï¼Œé€—å·åˆ†éš”ï¼ˆé»˜è®¤ï¼šå…¨éƒ¨ï¼‰"
    )
    
    parser.add_argument(
        "--cases",
        type=str,
        default=None,
        help="è¦æµ‹è¯•å‰å‡ ä¸ªcaseï¼Œæ ¼å¼ä¸ºèŒƒå›´å¦‚ '1-10' è¡¨ç¤ºæµ‹è¯•å‰10ä¸ªcaseï¼ˆé»˜è®¤ï¼šå…¨éƒ¨ï¼‰"
    )
    
    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼štest/resultsï¼‰"
    )
    
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="é™é»˜æ¨¡å¼ï¼ˆå‡å°‘è¾“å‡ºï¼‰"
    )
    
    return parser.parse_args()


async def main():
    """ä¸»å‡½æ•°ã€‚"""
    args = parse_arguments()
    
    print("ğŸš€ è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬")
    print("=" * 80)
    
    # è§£æå‚æ•°
    datasets_dir = Path(args.datasets_dir).resolve()
    
    # è¾“å‡ºç›®å½•ï¼šé»˜è®¤ä½¿ç”¨ test/resultsï¼Œå¦‚æœæŒ‡å®šäº†åˆ™ä½¿ç”¨æŒ‡å®šçš„
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        # é»˜è®¤è¾“å‡ºåˆ° test/results ç›®å½•
        test_dir = Path(__file__).parent
        output_dir = test_dir / "results"
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    repos = None
    if args.repos:
        repos = [r.strip() for r in args.repos.split(",")]
    
    cases_range = None
    if args.cases:
        cases_range = parse_cases_range(args.cases)
    
    # åŠ è½½æµ‹è¯•ç”¨ä¾‹ï¼ˆä¼˜å…ˆä½¿ç”¨æå–åçš„ test_cases.jsonï¼‰
    test_dir = Path(__file__).parent
    test_file = test_dir / "test_cases.json"
    
    # å¦‚æœ test_cases.json ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨åŸå§‹çš„ test_prs.json
    if not test_file.exists():
        test_file = test_dir / "test_prs.json"
        print(f"âš ï¸  test_cases.json ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶ test_prs.json")
        print(f"   æç¤º: è¿è¡Œ python test/extract_test_cases.py ç”Ÿæˆ test_cases.json")
    
    if not test_file.exists():
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return 1
    
    print(f"ğŸ“ æ•°æ®é›†ç›®å½•: {datasets_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“„ æµ‹è¯•æ–‡ä»¶: {test_file}")
    
    print("\nğŸ“– åŠ è½½æµ‹è¯•ç”¨ä¾‹...")
    try:
        test_data = load_test_cases(test_file)
        all_cases = collect_all_cases(test_data)
        filtered_cases = filter_cases(all_cases, repos=repos, cases_range=cases_range)
        
        print(f"âœ… åŠ è½½å®Œæˆ: æ€»å…± {len(all_cases)} ä¸ªç”¨ä¾‹ï¼Œè¿‡æ»¤å {len(filtered_cases)} ä¸ªç”¨ä¾‹")
    except Exception as e:
        print(f"âŒ åŠ è½½æµ‹è¯•ç”¨ä¾‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    if not filtered_cases:
        print("âš ï¸  æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æµ‹è¯•ç”¨ä¾‹")
        return 0
    
    # è¿è¡Œæµ‹è¯•ï¼ˆé¡ºåºæ‰§è¡Œï¼Œä¸å¹¶è¡Œï¼‰
    print(f"\nğŸ§ª å¼€å§‹è¿è¡Œ {len(filtered_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼ˆé¡ºåºæ‰§è¡Œï¼‰...")
    
    results = {
        "success": [],
        "failed": [],
        "skipped": []
    }
    
    # é¡ºåºæ‰§è¡Œæ¯ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼ˆä¸å¹¶è¡Œï¼‰
    for i, case in enumerate(filtered_cases, 1):
        if not args.quiet:
            print(f"\n[{i}/{len(filtered_cases)}] ", end="", flush=True)
        
        success, message = await run_single_test(
            case=case,
            datasets_dir=datasets_dir,
            output_dir=output_dir,
            quiet=args.quiet
        )
        
        if success:
            results["success"].append((case["case_name"], message))
            if not args.quiet:
                print(f"âœ… {message}")
        else:
            results["failed"].append((case["case_name"], message))
            if not args.quiet:
                print(f"âŒ {message}")
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    print("\n" + "=" * 80)
    print("ğŸ“Š æµ‹è¯•æŠ¥å‘Š")
    print("=" * 80)
    print(f"âœ… æˆåŠŸ: {len(results['success'])}")
    print(f"âŒ å¤±è´¥: {len(results['failed'])}")
    print(f"â­ï¸  è·³è¿‡: {len(results['skipped'])}")
    
    if results["failed"]:
        print("\nå¤±è´¥çš„ç”¨ä¾‹:")
        for case_name, message in results["failed"]:
            print(f"  - {case_name}: {message}")
    
    print(f"\nğŸ’¾ ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: {output_dir}")
    
    return 0 if len(results["failed"]) == 0 else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

