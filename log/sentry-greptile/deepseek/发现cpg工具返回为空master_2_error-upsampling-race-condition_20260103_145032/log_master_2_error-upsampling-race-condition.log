Expert Analysis Log
================================================================================
Repository: sentry-greptile
Model: deepseek
Timestamp: 2026-01-03T14:57:42.709041
================================================================================

Worklist Summary
================================================================================
Total Risks: 3
Risk Distribution:
  - RiskType.BUSINESS_INTENT: 1
  - RiskType.LIFECYCLE: 1
  - RiskType.NULL_SAFETY: 1
================================================================================

Expert Analysis 1:
================================================================================
Risk Type: unknown
File: src/sentry/testutils/factories.py
Line: (344, 358)
Description: 函数 `_set_sample_rate_from_error_sampling` 使用了链式调用 `normalized_data.get("contexts", {}).get("error_sampling", {}).get("client_sample_rate")`，虽然使用了空字典作为默认值，但未处理 `normalized_data` 本身为 None 或非字典类型的情况。此外，在设置 `sample_rate` 时直接赋值 `normalized_data["sample_rate"]`，假设 `normalized_data` 是可变字典，但函数签名只要求 `MutableMapping[str, Any]`，某些实现可能不支持键赋值。
--------------------------------------------------------------------------------

Analysis Result:
{
  "risk_type": "null_safety",
  "file_path": "src/sentry/testutils/factories.py",
  "line_number": [
    344,
    358
  ],
  "description": "函数 `_set_sample_rate_from_error_sampling` 存在两个潜在的空安全风险：1) 链式调用 `normalized_data.get(\"contexts\", {}).get(\"error_sampling\", {}).get(\"client_sample_rate\")` 假设 `normalized_data` 是字典类型且不为 None，但函数签名只要求 `MutableMapping[str, Any]`，未处理 `normalized_data` 为 None 或非字典类型的情况；2) 在设置 `sample_rate` 时直接赋值 `normalized_data[\"sample_rate\"]`，假设 `normalized_data` 是可变字典且支持键赋值，但某些 `MutableMapping` 实现可能不支持。",
  "confidence": 0.7,
  "severity": "warning",
  "suggestion": "1) 在函数开头添加对 `normalized_data` 是否为 None 或非字典类型的检查；2) 使用 `normalized_data.setdefault(\"sample_rate\", sample_rate)` 或先检查 `normalized_data` 是否支持 `__setitem__` 操作。"
}

Risk Item:
{
  "risk_type": "null_safety",
  "file_path": "src/sentry/testutils/factories.py",
  "line_number": [
    344,
    358
  ],
  "description": "函数 `_set_sample_rate_from_error_sampling` 使用了链式调用 `normalized_data.get(\"contexts\", {}).get(\"error_sampling\", {}).get(\"client_sample_rate\")`，虽然使用了空字典作为默认值，但未处理 `normalized_data` 本身为 None 或非字典类型的情况。此外，在设置 `sample_rate` 时直接赋值 `normalized_data[\"sample_rate\"]`，假设 `normalized_data` 是可变字典，但函数签名只要求 `MutableMapping[str, Any]`，某些实现可能不支持键赋值。",
  "confidence": 0.8,
  "severity": "warning",
  "suggestion": null
}

Conversation History (23 messages):
================================================================================

Message 1 [Assistant]:
--------------------------------------------------------------------------------
Content:
我将分析这个风险项。首先，让我读取完整的文件内容以获取更多上下文。
Tool Calls:
[
  {
    "name": "read_file",
    "args": {
      "file_path": "src/sentry/testutils/factories.py"
    },
    "id": "call_00_xFpa3QSVh2pRQWPeuy0dXbaB",
    "type": "tool_call"
  }
]

Message 2 [Tool]:
--------------------------------------------------------------------------------
Tool: read_file
Tool Call ID: call_00_xFpa3QSVh2pRQWPeuy0dXbaB
Result:
{
    "content": "from __future__ import annotations\n\nimport contextlib\nimport copy\nimport io\nimport os\nimport random\nimport zipfile\nfrom base64 import b64encode\nfrom binascii import hexlify\nfrom collections.abc import Mapping, MutableMapping, Sequence\nfrom datetime import UTC, datetime\nfrom enum import Enum\nfrom hashlib import sha1\nfrom importlib import import_module\nfrom typing import Any\nfrom unittest import mock\nfrom uuid import uuid4\n\nimport orjson\nimport petname\nfrom django.conf import settings\nfrom django.contrib.auth.models import AnonymousUser\nfrom django.core.files.base import ContentFile\nfrom django.db import router, transaction\nfrom django.test.utils import override_settings\nfrom django.utils import timezone\nfrom django.utils.text import slugify\n\nfrom sentry.auth.access import RpcBackedAccess\nfrom sentry.auth.services.auth.model import RpcAuthState, RpcMemberSsoState\nfrom sentry.constants import SentryAppInstallationStatus, SentryAppStatus\nfrom sentry.event_manager import EventManager\nfrom sentry.eventstore.models import Event\nfrom sentry.hybridcloud.models.outbox import RegionOutbox, outbox_context\nfrom sentry.hybridcloud.models.webhookpayload import WebhookPayload\nfrom sentry.hybridcloud.outbox.category import OutboxCategory, OutboxScope\nfrom sentry.incidents.grouptype import MetricIssue\nfrom sentry.incidents.logic import (\n    create_alert_rule,\n    create_alert_rule_trigger,\n    create_alert_rule_trigger_action,\n    query_datasets_to_type,\n)\nfrom sentry.incidents.models.alert_rule import (\n    AlertRuleDetectionType,\n    AlertRuleThresholdType,\n    AlertRuleTriggerAction,\n)\nfrom sentry.incidents.models.incident import (\n    Incident,\n    IncidentActivity,\n    IncidentProject,\n    IncidentTrigger,\n    IncidentType,\n    TriggerStatus,\n)\nfrom sentry.integrations.models.doc_integration import DocIntegration\nfrom sentry.integrations.models.doc_integration_avatar import DocIntegrationAvatar\nfrom sentry.integrations.models.external_actor import ExternalActor\nfrom sentry.integrations.models.external_issue import ExternalIssue\nfrom sentry.integrations.models.integration import Integration\nfrom sentry.integrations.models.integration_external_project import IntegrationExternalProject\nfrom sentry.integrations.models.integration_feature import (\n    Feature,\n    IntegrationFeature,\n    IntegrationTypes,\n)\nfrom sentry.integrations.models.organization_integration import OrganizationIntegration\nfrom sentry.integrations.models.repository_project_path_config import RepositoryProjectPathConfig\nfrom sentry.integrations.types import ExternalProviders\nfrom sentry.issues.grouptype import get_group_type_by_type_id\nfrom sentry.models.activity import Activity\nfrom sentry.models.apikey import ApiKey\nfrom sentry.models.apitoken import ApiToken\nfrom sentry.models.artifactbundle import ArtifactBundle\nfrom sentry.models.authidentity import AuthIdentity\nfrom sentry.models.authprovider import AuthProvider\nfrom sentry.models.commit import Commit\nfrom sentry.models.commitauthor import CommitAuthor\nfrom sentry.models.commitfilechange import CommitFileChange\nfrom sentry.models.dashboard import Dashboard\nfrom sentry.models.dashboard_widget import (\n    DashboardWidget,\n    DashboardWidgetDisplayTypes,\n    DashboardWidgetQuery,\n)\nfrom sentry.models.debugfile import ProjectDebugFile\nfrom sentry.models.environment import Environment\nfrom sentry.models.files.control_file import ControlFile\nfrom sentry.models.files.file import File\nfrom sentry.models.group import Group\nfrom sentry.models.grouphistory import GroupHistory\nfrom sentry.models.grouplink import GroupLink\nfrom sentry.models.groupopenperiod import GroupOpenPeriod\nfrom sentry.models.grouprelease import GroupRelease\nfrom sentry.models.organization import Organization\nfrom sentry.models.organizationmapping import OrganizationMapping\nfrom sentry.models.organizationmember import OrganizationMember\nfrom sentry.models.organizationmemberinvite import OrganizationMemberInvite\nfrom sentry.models.organizationmemberteam import OrganizationMemberTeam\nfrom sentry.models.organizationslugreservation import OrganizationSlugReservation\nfrom sentry.models.orgauthtoken import OrgAuthToken\nfrom sentry.models.project import Project\nfrom sentry.models.projectbookmark import ProjectBookmark\nfrom sentry.models.projectcodeowners import ProjectCodeOwners\nfrom sentry.models.projecttemplate import ProjectTemplate\nfrom sentry.models.release import Release, ReleaseStatus\nfrom sentry.models.releasecommit import ReleaseCommit\nfrom sentry.models.releaseenvironment import ReleaseEnvironment\nfrom sentry.models.releasefile import ReleaseFile, update_artifact_index\nfrom sentry.models.releaseprojectenvironment import ReleaseProjectEnvironment\nfrom sentry.models.repository import Repository\nfrom sentry.models.rule import Rule\nfrom sentry.models.rulesnooze import RuleSnooze\nfrom sentry.models.savedsearch import SavedSearch\nfrom sentry.models.team import Team\nfrom sentry.models.userreport import UserReport\nfrom sentry.notifications.models.notificationaction import (\n    ActionService,\n    ActionTarget,\n    ActionTrigger,\n    NotificationAction,\n)\nfrom sentry.notifications.models.notificationsettingprovider import NotificationSettingProvider\nfrom sentry.organizations.services.organization import RpcOrganization, RpcUserOrganizationContext\nfrom sentry.performance_issues.performance_problem import PerformanceProblem\nfrom sentry.sentry_apps.installations import (\n    SentryAppInstallationCreator,\n    SentryAppInstallationTokenCreator,\n)\nfrom sentry.sentry_apps.logic import SentryAppCreator\nfrom sentry.sentry_apps.models.platformexternalissue import PlatformExternalIssue\nfrom sentry.sentry_apps.models.sentry_app import SentryApp\nfrom sentry.sentry_apps.models.sentry_app_avatar import SentryAppAvatar\nfrom sentry.sentry_apps.models.sentry_app_installation import SentryAppInstallation\nfrom sentry.sentry_apps.models.sentry_app_installation_for_provider import (\n    SentryAppInstallationForProvider,\n)\nfrom sentry.sentry_apps.models.servicehook import ServiceHook\nfrom sentry.sentry_apps.services.hook import hook_service\nfrom sentry.sentry_apps.token_exchange.grant_exchanger import GrantExchanger\nfrom sentry.signals import project_created\nfrom sentry.silo.base import SiloMode\nfrom sentry.snuba.dataset import Dataset\nfrom sentry.snuba.models import QuerySubscriptionDataSourceHandler\nfrom sentry.tempest.models import MessageType as TempestMessageType\nfrom sentry.tempest.models import TempestCredentials\nfrom sentry.testutils.outbox import outbox_runner\nfrom sentry.testutils.silo import assume_test_silo_mode\nfrom sentry.types.activity import ActivityType\nfrom sentry.types.actor import Actor\nfrom sentry.types.region import Region, get_local_region, get_region_by_name\nfrom sentry.types.token import AuthTokenType\nfrom sentry.uptime.models import (\n    IntervalSecondsLiteral,\n    ProjectUptimeSubscription,\n    UptimeStatus,\n    UptimeSubscription,\n    UptimeSubscriptionRegion,\n)\nfrom sentry.uptime.types import UptimeMonitorMode\nfrom sentry.users.models.identity import Identity, IdentityProvider, IdentityStatus\nfrom sentry.users.models.user import User\nfrom sentry.users.models.user_avatar import UserAvatar\nfrom sentry.users.models.user_option import UserOption\nfrom sentry.users.models.useremail import UserEmail\nfrom sentry.users.models.userpermission import UserPermission\nfrom sentry.users.models.userrole import UserRole\nfrom sentry.users.services.user import RpcUser\nfrom sentry.utils import loremipsum\nfrom sentry.workflow_engine.models import (\n    Action,\n    ActionAlertRuleTriggerAction,\n    AlertRuleDetector,\n    AlertRuleWorkflow,\n    DataCondition,\n    DataConditionGroup,\n    DataConditionGroupAction,\n    DataSource,\n    DataSourceDetector,\n    Detector,\n    DetectorState,\n    DetectorWorkflow,\n    IncidentGroupOpenPeriod,\n    Workflow,\n    WorkflowDataConditionGroup,\n)\nfrom sentry.workflow_engine.registry import data_source_type_registry\nfrom social_auth.models import UserSocialAuth\n\n\nclass EventType(Enum):\n    ERROR = \"error\"\n    DEFAULT = \"default\"\n\n\ndef get_fixture_path(*parts: str) -> str:\n    path = os.path.realpath(__file__)\n    for _ in range(4):  # src/sentry/testutils/{__file__}\n        path = os.path.dirname(path)\n    return os.path.join(path, \"fixtures\", *parts)\n\n\ndef make_sentence(words=None):\n    if words is None:\n        words = int(random.weibullvariate(8, 3))\n    return \" \".join(random.choice(loremipsum.words) for _ in range(words))\n\n\ndef make_word(words=None):\n    if words is None:\n        words = int(random.weibullvariate(8, 3))\n    return random.choice(loremipsum.words)\n\n\nDEFAULT_EVENT_DATA = {\n    \"extra\": {\n        \"loadavg\": [0.97607421875, 0.88330078125, 0.833984375],\n        \"sys.argv\": [\n            \"/Users/dcramer/.virtualenvs/sentry/bin/raven\",\n            \"test\",\n            \"https://ebc35f33e151401f9deac549978bda11:f3403f81e12e4c24942d505f086b2cad@sentry.io/1\",\n        ],\n        \"user\": \"dcramer\",\n    },\n    \"modules\": {\"raven\": \"3.1.13\"},\n    \"request\": {\n        \"cookies\": {},\n        \"data\": {},\n        \"env\": {},\n        \"headers\": {},\n        \"method\": \"GET\",\n        \"query_string\": \"\",\n        \"url\": \"http://example.com\",\n    },\n    \"stacktrace\": {\n        \"frames\": [\n            {\n                \"abs_path\": \"www/src/sentry/models/foo.py\",\n                \"context_line\": \"                        string_max_length=self.string_max_length)\",\n                \"filename\": \"sentry/models/foo.py\",\n                \"function\": \"build_msg\",\n                \"in_app\": True,\n                \"lineno\": 29,\n                \"module\": \"raven.base\",\n                \"post_context\": [\n                    \"                },\",\n                    \"            })\",\n                    \"\",\n                    \"        if 'stacktrace' in data:\",\n                    \"            if self.include_paths:\",\n                ],\n                \"pre_context\": [\n                    \"\",\n                    \"            data.update({\",\n                    \"                'stacktrace': {\",\n                    \"                    'frames': get_stack_info(frames,\",\n                    \"                        list_max_length=self.list_max_length,\",\n                ],\n                \"vars\": {\n                    \"culprit\": \"raven.scripts.runner\",\n                    \"date\": \"datetime.datetime(2013, 2, 14, 20, 6, 33, 479471)\",\n                    \"event_id\": \"598fb19363e745ec8be665e6ba88b1b2\",\n                    \"event_type\": \"raven.events.Message\",\n                    \"frames\": \"<generator object iter_stack_frames at 0x103fef050>\",\n                    \"handler\": \"<raven.events.Message object at 0x103feb710>\",\n                    \"k\": \"logentry\",\n                    \"public_key\": None,\n                    \"result\": {\n                        \"logentry\": \"{'message': 'This is a test message generated using ``raven test``', 'params': []}\"\n                    },\n                    \"self\": \"<raven.base.Client object at 0x104397f10>\",\n                    \"stack\": True,\n                    \"tags\": None,\n                    \"time_spent\": None,\n                },\n            },\n            {\n                \"abs_path\": \"/Users/dcramer/.virtualenvs/sentry/lib/python2.7/site-packages/raven/base.py\",\n                \"context_line\": \"                        string_max_length=self.string_max_length)\",\n                \"filename\": \"raven/base.py\",\n                \"function\": \"build_msg\",\n                \"in_app\": False,\n                \"lineno\": 290,\n                \"module\": \"raven.base\",\n                \"post_context\": [\n                    \"                },\",\n                    \"            })\",\n                    \"\",\n                    \"        if 'stacktrace' in data:\",\n                    \"            if self.include_paths:\",\n                ],\n                \"pre_context\": [\n                    \"\",\n                    \"            data.update({\",\n                    \"                'stacktrace': {\",\n                    \"                    'frames': get_stack_info(frames,\",\n                    \"                        list_max_length=self.list_max_length,\",\n                ],\n                \"vars\": {\n                    \"culprit\": \"raven.scripts.runner\",\n                    \"date\": \"datetime.datetime(2013, 2, 14, 20, 6, 33, 479471)\",\n                    \"event_id\": \"598fb19363e745ec8be665e6ba88b1b2\",\n                    \"event_type\": \"raven.events.Message\",\n                    \"frames\": \"<generator object iter_stack_frames at 0x103fef050>\",\n                    \"handler\": \"<raven.events.Message object at 0x103feb710>\",\n                    \"k\": \"logentry\",\n                    \"public_key\": None,\n                    \"result\": {\n                        \"logentry\": \"{'message': 'This is a test message generated using ``raven test``', 'params': []}\"\n                    },\n                    \"self\": \"<raven.base.Client object at 0x104397f10>\",\n                    \"stack\": True,\n                    \"tags\": None,\n                    \"time_spent\": None,\n                },\n            },\n        ]\n    },\n    \"tags\": [],\n    \"platform\": \"python\",\n}\n\ndefault_detector_config_data = {\n    MetricIssue.slug: {\"threshold_period\": 1, \"detection_type\": \"static\"}\n}\n\n\ndef _patch_artifact_manifest(path, org=None, release=None, project=None, extra_files=None):\n    with open(path, \"rb\") as fp:\n        manifest = orjson.loads(fp.read())\n    if org:\n        manifest[\"org\"] = org\n    if release:\n        manifest[\"release\"] = release\n    if project:\n        manifest[\"project\"] = project\n    for path in extra_files or {}:\n        manifest[\"files\"][path] = {\"url\": path}\n    return orjson.dumps(manifest).decode()\n\n\ndef _set_sample_rate_from_error_sampling(normalized_data: MutableMapping[str, Any]) -> None:\n    \"\"\"Set 'sample_rate' on normalized_data if contexts.error_sampling.client_sample_rate is present and valid.\"\"\"\n    client_sample_rate = None\n    try:\n        client_sample_rate = (\n            normalized_data.get(\"contexts\", {}).get(\"error_sampling\", {}).get(\"client_sample_rate\")\n        )\n    except Exception:\n        pass\n    if client_sample_rate:\n        try:\n            normalized_data[\"sample_rate\"] = float(client_sample_rate)\n        except Exception:\n            pass\n\n\n# TODO(dcramer): consider moving to something more scalable like factoryboy\nclass Factories:\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_organization(name=None, owner=None, region: Region | str | None = None, **kwargs):\n        if not name:\n            name = petname.generate(2, \" \", letters=10).title()\n\n        with contextlib.ExitStack() as ctx:\n            if region is None or SiloMode.get_current_mode() == SiloMode.MONOLITH:\n                region_name = get_local_region().name\n            else:\n                if isinstance(region, Region):\n                    region_name = region.name\n                else:\n                    region_obj = get_region_by_name(region)  # Verify it exists\n                    region_name = region_obj.name\n\n                ctx.enter_context(\n                    override_settings(SILO_MODE=SiloMode.REGION, SENTRY_REGION=region_name)\n                )\n\n            with outbox_context(flush=False):\n                org = Organization.objects.create(name=name, **kwargs)\n\n            with assume_test_silo_mode(SiloMode.CONTROL):\n                # Organization mapping creation relies on having a matching org slug reservation\n                OrganizationSlugReservation(\n                    organization_id=org.id,\n                    region_name=region_name,\n                    user_id=owner.id if owner else -1,\n                    slug=org.slug,\n                ).save(unsafe_write=True)\n\n            # Manually replicate org data after adding an org slug reservation\n            org.handle_async_replication(org.id)\n\n            # Flush remaining organization update outboxes accumulated by org create\n            RegionOutbox(\n                shard_identifier=org.id,\n                shard_scope=OutboxScope.ORGANIZATION_SCOPE,\n                category=OutboxCategory.ORGANIZATION_UPDATE,\n            ).drain_shard()\n\n        if owner:\n            Factories.create_member(organization=org, user_id=owner.id, role=\"owner\")\n        return org\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_org_mapping(org=None, **kwds):\n        if org:\n            kwds.setdefault(\"organization_id\", org.id)\n            kwds.setdefault(\"slug\", org.slug)\n            kwds.setdefault(\"name\", org.name)\n            kwds.setdefault(\"idempotency_key\", uuid4().hex)\n            kwds.setdefault(\"region_name\", \"na\")\n        return OrganizationMapping.objects.create(**kwds)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_member(teams=None, team_roles=None, **kwargs):\n        kwargs.setdefault(\"role\", \"member\")\n        teamRole = kwargs.pop(\"teamRole\", None)\n\n        # user_id will have precedence over user\n        user = kwargs.pop(\"user\", None)\n        user_id = kwargs.pop(\"user_id\", None)\n        if not user_id and user:\n            user_id = user.id\n        kwargs[\"user_id\"] = user_id\n\n        # inviter_id will have precedence over inviter\n        inviter = kwargs.pop(\"inviter\", None)\n        inviter_id = kwargs.pop(\"inviter_id\", None)\n        if not inviter_id and inviter:\n            inviter_id = inviter.id\n        kwargs[\"inviter_id\"] = inviter_id\n\n        om = OrganizationMember.objects.create(**kwargs)\n\n        if team_roles:\n            for team, role in team_roles:\n                Factories.create_team_membership(team=team, member=om, role=role)\n        elif teams:\n            for team in teams:\n                Factories.create_team_membership(team=team, member=om, role=teamRole)\n        return om\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_member_invite(\n        organization: Organization | None = None,\n        email: str | None = None,\n        **kwargs,\n    ) -> OrganizationMemberInvite:\n        if organization is None:\n            organization = Factories.create_organization()\n        if email is None:\n            email = f\"{petname.generate().title()}@email.com\"\n        om = OrganizationMember.objects.create(organization=organization)\n        return OrganizationMemberInvite.objects.create(\n            organization=organization, organization_member_id=om.id, email=email, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_team_membership(team, member=None, user=None, role=None):\n        if member is None:\n            member, created = OrganizationMember.objects.get_or_create(\n                user_id=user.id if user else None,\n                organization=team.organization,\n                defaults={\"role\": \"member\"},\n            )\n\n        return OrganizationMemberTeam.objects.create(\n            team=team, organizationmember=member, is_active=True, role=role\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_api_key(organization, **kwargs) -> ApiKey:\n        return ApiKey.objects.create(organization_id=organization.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_auth_provider(**kwargs):\n        return AuthProvider.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_auth_identity(**kwargs):\n        return AuthIdentity.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_auth_token(user, scope_list: list[str] | None = None, **kwargs) -> ApiToken:\n        if scope_list is None:\n            scope_list = []\n        return ApiToken.objects.create(\n            user=user,\n            scope_list=scope_list,\n            token_type=AuthTokenType.USER,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_org_auth_token(*args, **kwargs) -> OrgAuthToken:\n        return OrgAuthToken.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_team(organization, **kwargs):\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n        if not kwargs.get(\"slug\"):\n            kwargs[\"slug\"] = slugify(str(kwargs[\"name\"]))\n        members = kwargs.pop(\"members\", None)\n\n        team = Team.objects.create(organization=organization, **kwargs)\n        if members:\n            for user in members:\n                Factories.create_team_membership(team=team, user=user)\n        return team\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_environment(project, **kwargs):\n        name = kwargs.get(\"name\", petname.generate(3, \" \", letters=10)[:64])\n\n        organization = kwargs.get(\"organization\")\n        organization_id = organization.id if organization else project.organization_id\n\n        env = Environment.objects.create(organization_id=organization_id, name=name)\n        env.add_project(project, is_hidden=kwargs.get(\"is_hidden\"))\n        return env\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project(\n        organization=None, teams=None, fire_project_created=False, **kwargs\n    ) -> Project:\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n        if not kwargs.get(\"slug\"):\n            kwargs[\"slug\"] = slugify(str(kwargs[\"name\"]))\n        if not organization and teams:\n            organization = teams[0].organization\n\n        with transaction.atomic(router.db_for_write(Project)):\n            project = Project.objects.create(organization=organization, **kwargs)\n            if teams:\n                for team in teams:\n                    project.add_team(team)\n            if fire_project_created:\n                project_created.send(\n                    project=project, user=AnonymousUser(), default_rules=True, sender=Factories\n                )\n        return project\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_template(project=None, organization=None, **kwargs) -> ProjectTemplate:\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n\n        with transaction.atomic(router.db_for_write(Project)):\n            project_template = ProjectTemplate.objects.create(organization=organization, **kwargs)\n\n        return project_template\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_bookmark(project, user):\n        return ProjectBookmark.objects.create(project_id=project.id, user_id=user.id)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_rule(\n        project,\n        action_data=None,\n        allow_no_action_data=False,\n        condition_data=None,\n        name=\"Test Alert\",\n        action_match=\"all\",\n        filter_match=\"all\",\n        frequency=30,\n        **kwargs,\n    ):\n        actions = None\n        if not allow_no_action_data:\n            action_data = action_data or [\n                {\n                    \"id\": \"sentry.rules.actions.notify_event.NotifyEventAction\",\n                    \"name\": \"Send a notification (for all legacy integrations)\",\n                },\n                {\n                    \"id\": \"sentry.rules.actions.notify_event_service.NotifyEventServiceAction\",\n                    \"service\": \"mail\",\n                    \"name\": \"Send a notification via mail\",\n                },\n            ]\n            actions = action_data\n        condition_data = condition_data or [\n            {\n                \"id\": \"sentry.rules.conditions.first_seen_event.FirstSeenEventCondition\",\n                \"name\": \"A new issue is created\",\n            },\n            {\n                \"id\": \"sentry.rules.conditions.every_event.EveryEventCondition\",\n                \"name\": \"The event occurs\",\n            },\n        ]\n        data = {\n            \"conditions\": condition_data,\n            \"action_match\": action_match,\n            \"filter_match\": filter_match,\n            \"frequency\": frequency,\n        }\n        if actions:\n            data[\"actions\"] = actions\n\n        return Rule.objects.create(\n            label=name,\n            project=project,\n            data=data,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_slack_project_rule(project, integration_id, channel_id=None, channel_name=None):\n        action_data = [\n            {\n                \"id\": \"sentry.rules.actions.notify_event.SlackNotifyServiceAction\",\n                \"name\": \"Send a Slack notification\",\n                \"workspace\": integration_id,\n                \"channel_id\": channel_id or \"123453\",\n                \"channel\": channel_name or \"#general\",\n            }\n        ]\n        return Factories.create_project_rule(project, action_data)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_key(project):\n        return project.key_set.get_or_create()[0]\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_tempest_credentials(\n        project: Project,\n        created_by: User | None = None,\n        client_id: str | None = None,\n        client_secret: str | None = None,\n        message: str = \"\",\n        message_type: str | None = None,\n        latest_fetched_item_id: str | None = None,\n    ):\n        if client_id is None:\n            client_id = str(uuid4())\n        if client_secret is None:\n            client_secret = str(uuid4())\n        if message_type is None:\n            message_type = TempestMessageType.ERROR\n\n        return TempestCredentials.objects.create(\n            project=project,\n            created_by_id=created_by.id if created_by else None,\n            client_id=client_id,\n            client_secret=client_secret,\n            message=message,\n            message_type=message_type,\n            latest_fetched_item_id=latest_fetched_item_id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release(\n        project: Project,\n        user: User | None = None,\n        version: str | None = None,\n        date_added: datetime | None = None,\n        additional_projects: Sequence[Project] | None = None,\n        environments: Sequence[Environment] | None = None,\n        date_released: datetime | None = None,\n        adopted: datetime | None = None,\n        unadopted: datetime | None = None,\n        status: int | None = ReleaseStatus.OPEN,\n    ):\n        if version is None:\n            version = hexlify(os.urandom(20)).decode()\n\n        if date_added is None:\n            date_added = timezone.now()\n\n        if additional_projects is None:\n            additional_projects = []\n\n        release = Release.objects.create(\n            version=version,\n            organization_id=project.organization_id,\n            date_added=date_added,\n            date_released=date_released,\n            status=status,\n        )\n\n        release.add_project(project)\n        for additional_project in additional_projects:\n            release.add_project(additional_project)\n\n        for environment in environments or []:\n            ReleaseEnvironment.objects.create(\n                organization=project.organization, release=release, environment=environment\n            )\n            for project in [project, *additional_projects]:\n                ReleaseProjectEnvironment.objects.create(\n                    project=project,\n                    release=release,\n                    environment=environment,\n                    adopted=adopted,\n                    unadopted=unadopted,\n                )\n\n        Activity.objects.create(\n            type=ActivityType.RELEASE.value,\n            project=project,\n            ident=Activity.get_version_ident(version),\n            user_id=user.id if user else None,\n            data={\"version\": version},\n        )\n\n        # add commits\n        if user:\n            author = Factories.create_commit_author(project=project, user=user)\n            repo = Factories.create_repo(project, name=f\"organization-{project.slug}\")\n            commit = Factories.create_commit(\n                project=project,\n                repo=repo,\n                author=author,\n                release=release,\n                key=\"deadbeef\",\n                message=\"placeholder commit message\",\n            )\n\n            release.update(authors=[str(author.id)], commit_count=1, last_commit_id=commit.id)\n\n        return release\n\n    @staticmethod\n    def create_group_release(project: Project, group: Group, release: Release) -> GroupRelease:\n        return GroupRelease.objects.create(\n            project_id=project.id,\n            group_id=group.id,\n            release_id=release.id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release_file(release_id, file=None, name=None, dist_id=None):\n        if file is None:\n            file = Factories.create_file(\n                name=\"log.txt\",\n                size=32,\n                headers={\"Content-Type\": \"text/plain\"},\n                checksum=\"dc1e3f3e411979d336c3057cce64294f3420f93a\",\n            )\n\n        if name is None:\n            name = file.name\n\n        organization_id = Release.objects.get(pk=release_id).organization.id\n\n        return ReleaseFile.objects.create(\n            organization_id=organization_id,\n            release_id=release_id,\n            name=name,\n            file=file,\n            dist_id=dist_id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_artifact_bundle_zip(\n        org=None, release=None, project=None, extra_files=None, fixture_path=\"artifact_bundle\"\n    ):\n        bundle = io.BytesIO()\n        bundle_dir = get_fixture_path(fixture_path)\n        with zipfile.ZipFile(bundle, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n            for path, content in (extra_files or {}).items():\n                zipf.writestr(path, content)\n            for path, _, files in os.walk(bundle_dir):\n                for filename in files:\n                    fullpath = os.path.join(path, filename)\n                    relpath = os.path.relpath(fullpath, bundle_dir)\n                    if filename == \"manifest.json\":\n                        manifest = _patch_artifact_manifest(\n                            fullpath, org, release, project, extra_files\n                        )\n                        zipf.writestr(relpath, manifest)\n                    else:\n                        zipf.write(fullpath, relpath)\n\n        return bundle.getvalue()\n\n    @classmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release_archive(cls, org, release: str, project=None, dist=None):\n        bundle = cls.create_artifact_bundle_zip(org, release, project)\n        file = File.objects.create(name=\"release-artifacts.zip\")\n        file.putfile(ContentFile(bundle))\n        release_obj = Release.objects.get(organization__slug=org, version=release)\n        return update_artifact_index(release_obj, dist, file)\n\n    @classmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_artifact_bundle(\n        cls,\n        org,\n        bundle_id=None,\n        artifact_count=0,\n        fixture_path=\"artifact_bundle_debug_ids\",\n        date_uploaded=None,\n        date_last_modified=None,\n    ):\n        if date_uploaded is None:\n            date_uploaded = timezone.now()\n\n        bundle = cls.create_artifact_bundle_zip(org.slug, fixture_path=fixture_path)\n        file_ = File.objects.create(name=\"artifact-bundle.zip\")\n        file_.putfile(ContentFile(bundle))\n        # The 'artifact_count' should correspond to the 'bundle' contents but for the purpose of tests we can also\n        # mock it with an arbitrary value.\n        artifact_bundle = ArtifactBundle.objects.create(\n            organization_id=org.id,\n            bundle_id=bundle_id or uuid4(),\n            file=file_,\n            artifact_count=artifact_count,\n            date_uploaded=date_uploaded,\n            date_last_modified=date_last_modified,\n        )\n        return artifact_bundle\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_code_mapping(project, repo=None, organization_integration=None, **kwargs):\n        kwargs.setdefault(\"stack_root\", \"\")\n        kwargs.setdefault(\"source_root\", \"\")\n        kwargs.setdefault(\"default_branch\", \"master\")\n\n        if not repo:\n            repo = Factories.create_repo(project=project)\n        return RepositoryProjectPathConfig.objects.create(\n            project=project,\n            repository=repo,\n            organization_integration_id=organization_integration.id,\n            integration_id=organization_integration.integration_id,\n            organization_id=organization_integration.organization_id,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_repo(\n        project, name=None, provider=None, integration_id=None, url=None, external_id=None\n    ):\n        repo, _ = Repository.objects.get_or_create(\n            organization_id=project.organization_id,\n            name=name\n            or \"{}-{}\".format(petname.generate(2, \"\", letters=10), random.randint(1000, 9999)),\n            provider=provider,\n            integration_id=integration_id,\n            url=url,\n            external_id=external_id,\n        )\n        return repo\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit(\n        repo, project=None, author=None, release=None, message=None, key=None, date_added=None\n    ):\n        commit = Commit.objects.get_or_create(\n            organization_id=repo.organization_id,\n            repository_id=repo.id,\n            key=key or sha1(uuid4().hex.encode(\"utf-8\")).hexdigest(),\n            defaults={\n                \"message\": message or make_sentence(),\n                \"author\": author\n                or Factories.create_commit_author(organization_id=repo.organization_id),\n                \"date_added\": date_added or timezone.now(),\n            },\n        )[0]\n\n        if release:\n            assert project\n            ReleaseCommit.objects.create(\n                organization_id=repo.organization_id,\n                project_id=project.id,\n                release=release,\n                commit=commit,\n                order=1,\n            )\n\n        Factories.create_commit_file_change(commit=commit, filename=\"/models/foo.py\")\n        Factories.create_commit_file_change(commit=commit, filename=\"/worsematch/foo.py\")\n        Factories.create_commit_file_change(commit=commit, filename=\"/models/other.py\")\n\n        return commit\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit_author(organization_id=None, project=None, user=None, email=None):\n        if email:\n            user_email = email\n        else:\n            user_email = user.email if user else f\"{make_word()}@example.com\"\n        return CommitAuthor.objects.get_or_create(\n            organization_id=organization_id or project.organization_id,\n            email=user_email,\n            defaults={\"name\": user.name if user else make_word()},\n        )[0]\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit_file_change(commit, filename):\n        return CommitFileChange.objects.get_or_create(\n            organization_id=commit.organization_id, commit=commit, filename=filename, type=\"M\"\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user(\n        email=None, is_superuser=False, is_staff=False, is_active=True, **kwargs\n    ) -> User:\n        if email is None:\n            email = uuid4().hex + \"@example.com\"\n\n        kwargs.setdefault(\"username\", email)\n\n        user = User(\n            email=email, is_superuser=is_superuser, is_staff=is_staff, is_active=is_active, **kwargs\n        )\n        if kwargs.get(\"password\") is None:\n            user.set_password(\"admin\")\n        user.save()\n\n        # UserEmail is created by a signal\n        assert UserEmail.objects.filter(user=user, email=email).update(is_verified=True)\n\n        return user\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_useremail(user, email=None, **kwargs):\n        if not email:\n            email = uuid4().hex + \"@example.com\"\n\n        kwargs.setdefault(\"is_verified\", True)\n\n        useremail = UserEmail(user=user, email=email, **kwargs)\n        useremail.save()\n\n        return useremail\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_avatar(*args, **kwargs):\n        return UserAvatar.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_role(*args, **kwargs):\n        return UserRole.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_usersocialauth(\n        user: User,\n        provider: str | None = None,\n        uid: str | None = None,\n        extra_data: dict[str, Any] | None = None,\n    ):\n        if not provider:\n            provider = \"asana\"\n        if not uid:\n            uid = \"abc-123\"\n        usa = UserSocialAuth(user=user, provider=provider, uid=uid, extra_data=extra_data)\n        usa.save()\n        return usa\n\n    @staticmethod\n    def inject_performance_problems(jobs, _):\n        for job in jobs:\n            job[\"performance_problems\"] = []\n            for f in job[\"data\"][\"fingerprint\"]:\n                f_data = f.split(\"-\", 1)\n                if len(f_data) < 2:\n                    raise ValueError(\n                        \"Invalid performance fingerprint data. Format must be 'group_type-fingerprint'.\"\n                    )\n                group_type = get_group_type_by_type_id(int(f_data[0]))\n                perf_fingerprint = f_data[1]\n\n                job[\"performance_problems\"].append(\n                    PerformanceProblem(\n                        fingerprint=perf_fingerprint,\n                        op=\"db\",\n                        desc=\"\",\n                        type=group_type,\n                        parent_span_ids=None,\n                        cause_span_ids=None,\n                        offender_span_ids=[],\n                        evidence_data={},\n                        evidence_display=[],\n                    )\n                )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def store_event(\n        data,\n        project_id: int,\n        assert_no_errors: bool = True,\n        default_event_type: EventType | None = None,\n        sent_at: datetime | None = None,\n    ) -> Event:\n        \"\"\"\n        Like `create_event`, but closer to how events are actually\n        ingested. Prefer to use this method over `create_event`\n        \"\"\"\n\n        # this creates a basic message event\n        if default_event_type == EventType.DEFAULT:\n            data.update({\"stacktrace\": copy.deepcopy(DEFAULT_EVENT_DATA[\"stacktrace\"])})\n\n        # this creates an error event\n        elif default_event_type == EventType.ERROR:\n            data.update({\"exception\": [{\"value\": \"BadError\"}]})\n\n        manager = EventManager(data, sent_at=sent_at)\n        manager.normalize()\n        if assert_no_errors:\n            errors = manager.get_data().get(\"errors\")\n            assert not errors, errors\n\n        normalized_data = manager.get_data()\n\n        _set_sample_rate_from_error_sampling(normalized_data)\n\n        event = None\n\n        # When fingerprint is present on transaction, inject performance problems\n        if (\n            normalized_data.get(\"type\") == \"transaction\"\n            and normalized_data.get(\"fingerprint\") is not None\n        ):\n            with mock.patch(\n                \"sentry.event_manager._detect_performance_problems\",\n                Factories.inject_performance_problems,\n            ):\n                event = manager.save(project_id)\n\n        else:\n            event = manager.save(project_id)\n\n        if event.groups:\n            for group in event.groups:\n                group.save()\n\n        if event.group:\n            event.group.save()\n\n        return event\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_group(project, create_open_period=True, **kwargs):\n        from sentry.models.group import GroupStatus\n        from sentry.models.groupopenperiod import GroupOpenPeriod\n        from sentry.testutils.helpers.datetime import before_now\n        from sentry.types.group import GroupSubStatus\n\n        kwargs.setdefault(\"message\", \"Hello world\")\n        kwargs.setdefault(\"data\", {})\n        if \"type\" not in kwargs[\"data\"]:\n            kwargs[\"data\"].update({\"type\": \"default\", \"metadata\": {\"title\": kwargs[\"message\"]}})\n        if \"short_id\" not in kwargs:\n            kwargs[\"short_id\"] = project.next_short_id()\n        if \"metadata\" in kwargs:\n            metadata = kwargs.pop(\"metadata\")\n            kwargs[\"data\"].setdefault(\"metadata\", {}).update(metadata)\n        if \"status\" not in kwargs:\n            kwargs[\"status\"] = GroupStatus.UNRESOLVED\n            kwargs[\"substatus\"] = GroupSubStatus.NEW\n\n        group = Group.objects.create(project=project, **kwargs)\n        if create_open_period:\n            open_period = GroupOpenPeriod.objects.create(\n                group=group,\n                project=project,\n                date_started=group.first_seen or before_now(minutes=5),\n            )\n            if group.status == GroupStatus.RESOLVED:\n                open_period.update(\n                    date_ended=group.resolved_at if group.resolved_at else timezone.now()\n                )\n\n        return group\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_file(**kwargs):\n        return File.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_file_from_path(path, name=None, **kwargs):\n        if name is None:\n            name = os.path.basename(path)\n\n        file = Factories.create_file(name=name, **kwargs)\n        with open(path) as f:\n            file.putfile(f)\n        return file\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dif_file(\n        project,\n        debug_id=None,\n        object_name=None,\n        features=None,\n        data=None,\n        file=None,\n        cpu_name=None,\n        code_id=None,\n        **kwargs,\n    ):\n        if debug_id is None:\n            debug_id = str(uuid4())\n\n        if object_name is None:\n            object_name = \"%s.dSYM\" % debug_id\n\n        if features is not None:\n            if data is None:\n                data = {}\n            data[\"features\"] = features\n\n        if file is None:\n            file = Factories.create_file(\n                name=object_name,\n                size=42,\n                headers={\"Content-Type\": \"application/x-mach-binary\"},\n                checksum=\"dc1e3f3e411979d336c3057cce64294f3420f93a\",\n            )\n\n        return ProjectDebugFile.objects.create(\n            debug_id=debug_id,\n            code_id=code_id,\n            project_id=project.id,\n            object_name=object_name,\n            cpu_name=cpu_name or \"x86_64\",\n            file=file,\n            checksum=file.checksum,\n            data=data,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dif_from_path(path, object_name=None, **kwargs):\n        if object_name is None:\n            object_name = os.path.basename(path)\n\n        headers = {\"Content-Type\": \"application/x-mach-binary\"}\n        file = Factories.create_file_from_path(path, name=object_name, headers=headers)\n        return Factories.create_dif_file(file=file, object_name=object_name, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def add_user_permission(user, permission):\n        UserPermission.objects.create(user=user, permission=permission)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app(**kwargs):\n        published = kwargs.pop(\"published\", False)\n        args = Factories._sentry_app_kwargs(**kwargs)\n        user = args.pop(\"user\", None)\n        app = SentryAppCreator(is_internal=False, **args).run(user=user, request=None)\n\n        if published:\n            app.update(status=SentryAppStatus.PUBLISHED)\n\n        return app\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_avatar(*args, **kwargs):\n        return SentryAppAvatar.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_internal_integration(**kwargs) -> SentryApp:\n        args = Factories._sentry_app_kwargs(**kwargs)\n        args[\"verify_install\"] = False\n        user = args.pop(\"user\", None)\n        app = SentryAppCreator(is_internal=True, **args).run(\n            user=user, request=None, skip_default_auth_token=True\n        )\n        return app\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_internal_integration_token(\n        user,\n        internal_integration: SentryApp | None = None,\n        install: SentryAppInstallation | None = None,\n        request=None,\n    ) -> ApiToken:\n        if internal_integration and install:\n            raise ValueError(\"Only one of internal_integration or install arg can be provided\")\n        elif internal_integration is None and install is None:\n            raise ValueError(\"Must pass in either internal_integration or install arg\")\n\n        if internal_integration is not None and install is None:\n            # Fetch install from provided or created internal integration\n            with assume_test_silo_mode(SiloMode.CONTROL):\n                install = SentryAppInstallation.objects.get(\n                    sentry_app=internal_integration.id,\n                    organization_id=internal_integration.owner_id,\n                )\n        elif install is None:\n            raise AssertionError(\"unreachable\")\n\n        return SentryAppInstallationTokenCreator(sentry_app_installation=install).run(\n            user=user, request=request\n        )\n\n    @staticmethod\n    def _sentry_app_kwargs(**kwargs):\n        _kwargs = {\n            \"user\": kwargs.get(\"user\", Factories.create_user()),\n            \"name\": kwargs.get(\"name\", petname.generate(2, \" \", letters=10).title()),\n            \"organization_id\": kwargs.get(\n                \"organization_id\", kwargs.pop(\"organization\", Factories.create_organization()).id\n            ),\n            \"author\": kwargs.get(\"author\", \"A Company\"),\n            \"scopes\": kwargs.get(\"scopes\", ()),\n            \"verify_install\": kwargs.get(\"verify_install\", True),\n            \"webhook_url\": kwargs.get(\"webhook_url\", \"https://example.com/webhook\"),\n            \"events\": [],\n            \"schema\": {},\n        }\n\n        _kwargs.update(**kwargs)\n        return _kwargs\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_sentry_app_installation(\n        organization=None,\n        slug=None,\n        user=None,\n        status=None,\n        prevent_token_exchange=False,\n    ):\n        if not organization:\n            organization = Factories.create_organization()\n\n        Factories.create_project(organization=organization)\n\n        with assume_test_silo_mode(SiloMode.CONTROL):\n            install = SentryAppInstallationCreator(\n                slug=(slug or Factories.create_sentry_app(organization=organization).slug),\n                organization_id=organization.id,\n            ).run(\n                user=(user or Factories.create_user()),\n                request=None,\n            )\n\n            install.status = SentryAppInstallationStatus.INSTALLED if status is None else status\n            install.save()\n            if not prevent_token_exchange and (\n                install.sentry_app.status != SentryAppStatus.INTERNAL\n            ):\n                assert install.api_grant is not None\n                assert install.sentry_app.application is not None\n                assert install.sentry_app.proxy_user is not None\n                GrantExchanger(\n                    install=install,\n                    code=install.api_grant.code,\n                    client_id=install.sentry_app.application.client_id,\n                    user=install.sentry_app.proxy_user,\n                ).run()\n                install = SentryAppInstallation.objects.get(id=install.id)\n        return install\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_installation_for_provider(\n        sentry_app_id: int,\n        organization_id: int,\n        provider: str,\n    ) -> SentryAppInstallationForProvider:\n        installation = SentryAppInstallation.objects.get(\n            sentry_app_id=sentry_app_id, organization_id=organization_id\n        )\n        return SentryAppInstallationForProvider.objects.create(\n            organization_id=organization_id,\n            provider=provider,\n            sentry_app_installation=installation,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_stacktrace_link_schema():\n        return {\"type\": \"stacktrace-link\", \"uri\": \"/redirect/\"}\n\n    @staticmethod\n    def create_issue_link_schema():\n        return {\n            \"type\": \"issue-link\",\n            \"link\": {\n                \"uri\": \"/sentry/issues/link\",\n                \"required_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    }\n                ],\n            },\n            \"create\": {\n                \"uri\": \"/sentry/issues/create\",\n                \"required_fields\": [\n                    {\"type\": \"text\", \"name\": \"title\", \"label\": \"Title\"},\n                    {\"type\": \"text\", \"name\": \"summary\", \"label\": \"Summary\"},\n                ],\n                \"optional_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"points\",\n                        \"label\": \"Points\",\n                        \"options\": [[\"1\", \"1\"], [\"2\", \"2\"], [\"3\", \"3\"], [\"5\", \"5\"], [\"8\", \"8\"]],\n                    },\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    },\n                ],\n            },\n        }\n\n    @staticmethod\n    def create_alert_rule_action_schema():\n        return {\n            \"type\": \"alert-rule-action\",\n            \"title\": \"Create Task with App\",\n            \"settings\": {\n                \"type\": \"alert-rule-settings\",\n                \"uri\": \"/sentry/alert-rule\",\n                \"required_fields\": [\n                    {\"type\": \"text\", \"name\": \"title\", \"label\": \"Title\"},\n                    {\"type\": \"text\", \"name\": \"summary\", \"label\": \"Summary\"},\n                ],\n                \"optional_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"points\",\n                        \"label\": \"Points\",\n                        \"options\": [[\"1\", \"1\"], [\"2\", \"2\"], [\"3\", \"3\"], [\"5\", \"5\"], [\"8\", \"8\"]],\n                    },\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    },\n                ],\n            },\n        }\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_service_hook(\n        actor=None, org=None, project=None, events=None, url=None, project_ids=None, **kwargs\n    ):\n        if project:\n            if project_ids is not None:\n                raise ValueError(\"Cannot provide both project and project_ids\")\n            project_ids = [project.id]\n        if not actor:\n            actor = Factories.create_user()\n        if not org:\n            if project:\n                org = project.organization\n            else:\n                org = Factories.create_organization(owner=actor)\n        if project_ids is None:  # empty list for project_ids is valid and means no project filter\n            project_ids = [Factories.create_project(organization=org).id]\n        if events is None:\n            events = [\"event.created\"]\n        if not url:\n            url = \"https://example.com/sentry/webhook\"\n\n        app_id = kwargs.pop(\"application_id\", None)\n        if app_id is None and \"application\" in kwargs:\n            app_id = kwargs[\"application\"].id\n        installation_id = kwargs.pop(\"installation_id\", None)\n        if installation_id is None and \"installation\" in kwargs:\n            installation_id = kwargs[\"installation\"].id\n        hook_id = hook_service.create_service_hook(\n            application_id=app_id,\n            actor_id=actor.id,\n            installation_id=installation_id,\n            organization_id=org.id,\n            project_ids=project_ids,\n            events=events,\n            url=url,\n        ).id\n        return ServiceHook.objects.get(id=hook_id)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_feature(feature=None, sentry_app=None, description=None):\n        if not sentry_app:\n            sentry_app = Factories.create_sentry_app()\n\n        integration_feature = IntegrationFeature.objects.create(\n            target_id=sentry_app.id,\n            target_type=IntegrationTypes.SENTRY_APP.value,\n            feature=feature or Feature.API,\n        )\n\n        if description:\n            integration_feature.update(user_description=description)\n\n        return integration_feature\n\n    @staticmethod\n    def _doc_integration_kwargs(**kwargs):\n        _kwargs = {\n            \"name\": kwargs.get(\"name\", petname.generate(2, \" \", letters=10).title()),\n            \"author\": kwargs.get(\"author\", \"me\"),\n            \"description\": kwargs.get(\"description\", \"hi im a description\"),\n            \"url\": kwargs.get(\"url\", \"https://sentry.io\"),\n            \"popularity\": kwargs.get(\"popularity\", 1),\n            \"is_draft\": kwargs.get(\"is_draft\", True),\n            \"metadata\": kwargs.get(\"metadata\", {}),\n        }\n        _kwargs[\"slug\"] = slugify(_kwargs[\"name\"])\n        _kwargs.update(**kwargs)\n        return _kwargs\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration(features=None, has_avatar: bool = False, **kwargs) -> DocIntegration:\n        doc = DocIntegration.objects.create(**Factories._doc_integration_kwargs(**kwargs))\n        if features:\n            Factories.create_doc_integration_features(features=features, doc_integration=doc)\n        if has_avatar:\n            Factories.create_doc_integration_avatar(doc_integration=doc)\n        return doc\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration_features(\n        features=None, doc_integration=None\n    ) -> list[IntegrationFeature]:\n        if not features:\n            features = [Feature.API]\n        if not doc_integration:\n            doc_integration = Factories.create_doc_integration()\n        return IntegrationFeature.objects.bulk_create(\n            [\n                IntegrationFeature(\n                    target_id=doc_integration.id,\n                    target_type=IntegrationTypes.DOC_INTEGRATION.value,\n                    feature=feature,\n                )\n                for feature in features\n            ]\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration_avatar(doc_integration=None, **kwargs) -> DocIntegrationAvatar:\n        if not doc_integration:\n            doc_integration = Factories.create_doc_integration()\n        photo = ControlFile.objects.create(name=\"test.png\", type=\"avatar.file\")\n        photo.putfile(io.BytesIO(b\"imaginethiswasphotobytes\"))\n\n        return DocIntegrationAvatar.objects.create(\n            doc_integration=doc_integration, avatar_type=0, control_file_id=photo.id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_userreport(\n        project: Project, event_id: str | None = None, **kwargs: Any\n    ) -> UserReport:\n        event = Factories.store_event(\n            data={\n                \"timestamp\": datetime.now(UTC).isoformat(),\n                \"event_id\": event_id or \"a\" * 32,\n                \"message\": \"testing\",\n            },\n            project_id=project.id,\n        )\n        assert event.group is not None\n\n        return UserReport.objects.create(\n            group_id=event.group.id,\n            event_id=event.event_id,\n            project_id=project.id,\n            name=\"Jane Bloggs\",\n            email=\"jane@example.com\",\n            comments=\"the application crashed\",\n            **kwargs,\n        )\n\n    @staticmethod\n    def create_session():\n        engine = import_module(settings.SESSION_ENGINE)\n\n        session = engine.SessionStore()\n        session.save()\n        return session\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_platform_external_issue(\n        group=None, service_type=None, display_name=None, web_url=None\n    ):\n        return PlatformExternalIssue.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            service_type=service_type,\n            display_name=display_name,\n            web_url=web_url,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_integration_external_issue(group=None, integration=None, key=None, **kwargs):\n        external_issue = ExternalIssue.objects.create(\n            organization_id=group.organization.id, integration_id=integration.id, key=key, **kwargs\n        )\n\n        GroupLink.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            linked_type=GroupLink.LinkedType.issue,\n            linked_id=external_issue.id,\n            relationship=GroupLink.Relationship.references,\n        )\n\n        return external_issue\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_integration_external_project(\n        organization_id: int, integration_id: int, *args: Any, **kwargs: Any\n    ) -> IntegrationExternalProject:\n        oi = OrganizationIntegration.objects.get(\n            organization_id=organization_id, integration_id=integration_id\n        )\n        return IntegrationExternalProject.objects.create(\n            organization_integration_id=oi.id, *args, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident(\n        organization,\n        projects,\n        detection_uuid=None,\n        status=1,\n        title=None,\n        query=\"test query\",\n        date_started=None,\n        date_detected=None,\n        date_closed=None,\n        alert_rule=None,\n        subscription=None,\n    ):\n        if not title:\n            title = petname.generate(2, \" \", letters=10).title()\n        if alert_rule is None:\n            alert_rule = Factories.create_alert_rule(\n                organization, projects, query=query, time_window=1\n            )\n\n        incident = Incident.objects.create(\n            organization=organization,\n            detection_uuid=detection_uuid,\n            status=status,\n            title=title,\n            alert_rule=alert_rule,\n            date_started=date_started or timezone.now(),\n            date_detected=date_detected or timezone.now(),\n            date_closed=timezone.now() if date_closed is not None else date_closed,\n            type=IncidentType.ALERT_TRIGGERED.value,\n            subscription=subscription,\n        )\n        for project in projects:\n            IncidentProject.objects.create(incident=incident, project=project)\n\n        return incident\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_activity(incident, type, comment=None, user_id=None):\n        return IncidentActivity.objects.create(\n            incident=incident, type=type, comment=comment, user_id=user_id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule(\n        organization,\n        projects,\n        name=None,\n        owner=None,\n        query=\"level:error\",\n        aggregate=\"count()\",\n        time_window=10,\n        threshold_period=1,\n        environment=None,\n        date_added=None,\n        query_type=None,\n        dataset=Dataset.Events,\n        threshold_type=AlertRuleThresholdType.ABOVE,\n        resolve_threshold=None,\n        user=None,\n        event_types=None,\n        comparison_delta=None,\n        description=None,\n        sensitivity=None,\n        seasonality=None,\n        detection_type=AlertRuleDetectionType.STATIC,\n    ):\n        if not name:\n            name = petname.generate(2, \" \", letters=10).title()\n\n        if query_type is None:\n            query_type = query_datasets_to_type[dataset]\n\n        alert_rule = create_alert_rule(\n            organization,\n            projects,\n            name,\n            query,\n            aggregate,\n            time_window,\n            threshold_type,\n            threshold_period,\n            owner=owner,\n            resolve_threshold=resolve_threshold,\n            query_type=query_type,\n            dataset=dataset,\n            environment=environment,\n            user=user,\n            event_types=event_types,\n            comparison_delta=comparison_delta,\n            description=description,\n            sensitivity=sensitivity,\n            seasonality=seasonality,\n            detection_type=detection_type,\n        )\n\n        if date_added is not None:\n            alert_rule.update(date_added=date_added)\n\n        return alert_rule\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_trigger(alert_rule, label=None, alert_threshold=100):\n        if not label:\n            label = petname.generate(2, \" \", letters=10).title()\n\n        return create_alert_rule_trigger(alert_rule, label, alert_threshold)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_trigger(incident, alert_rule_trigger, status=None):\n        if status is None:\n            status = TriggerStatus.ACTIVE.value\n\n        return IncidentTrigger.objects.create(\n            alert_rule_trigger=alert_rule_trigger, incident=incident, status=status\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_trigger_action(\n        trigger,\n        type=AlertRuleTriggerAction.Type.EMAIL,\n        target_type=AlertRuleTriggerAction.TargetType.USER,\n        target_identifier=None,\n        integration=None,\n        sentry_app=None,\n        sentry_app_config=None,\n    ):\n        return create_alert_rule_trigger_action(\n            trigger,\n            type,\n            target_type,\n            target_identifier,\n            integration.id if integration else None,\n            sentry_app.id if sentry_app else None,\n            sentry_app_config=sentry_app_config,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_external_user(user: User, **kwargs: Any) -> ExternalActor:\n        kwargs.setdefault(\"provider\", ExternalProviders.GITHUB.value)\n        kwargs.setdefault(\"external_name\", \"\")\n\n        return ExternalActor.objects.create(user_id=user.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_external_team(team: Team, **kwargs: Any) -> ExternalActor:\n        kwargs.setdefault(\"provider\", ExternalProviders.GITHUB.value)\n        kwargs.setdefault(\"external_name\", \"@getsentry/ecosystem\")\n\n        return ExternalActor.objects.create(team_id=team.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_codeowners(project, code_mapping, **kwargs):\n        kwargs.setdefault(\"raw\", \"\")\n\n        return ProjectCodeOwners.objects.create(\n            project=project, repository_project_path_config=code_mapping, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_slack_integration(\n        organization: Organization, external_id: str, **kwargs: Any\n    ) -> Integration:\n        integration = Integration.objects.create(\n            provider=\"slack\",\n            name=\"Team A\",\n            external_id=external_id,\n            metadata={\n                \"access_token\": \"xoxp-xxxxxxxxx-xxxxxxxxxx-xxxxxxxxxxxx\",\n                \"installation_type\": \"born_as_bot\",\n            },\n        )\n        integration.add_organization(organization)\n        return integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_integration(\n        organization: Organization,\n        external_id: str,\n        oi_params: Mapping[str, Any] | None = None,\n        **integration_params: Any,\n    ) -> Integration:\n        integration = Integration.objects.create(external_id=external_id, **integration_params)\n        with outbox_runner():\n            organization_integration = integration.add_organization(organization)\n            assert organization_integration is not None\n        organization_integration.update(**(oi_params or {}))\n\n        return integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_provider_integration(**integration_params: Any) -> Integration:\n        return Integration.objects.create(**integration_params)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_provider_integration_for(\n        organization: Organization | RpcOrganization,\n        user: User | RpcUser | None,\n        **integration_params: Any,\n    ) -> tuple[Integration, OrganizationIntegration]:\n        integration = Integration.objects.create(**integration_params)\n        org_integration = integration.add_organization(organization, user)\n        assert org_integration is not None\n        return integration, org_integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity_integration(\n        user: User | RpcUser,\n        organization: Organization | RpcOrganization,\n        integration_params: Mapping[Any, Any],\n        identity_params: Mapping[Any, Any],\n    ) -> tuple[Integration, OrganizationIntegration, Identity, IdentityProvider]:\n        # Avoid common pitfalls in tests\n        assert \"provider\" in integration_params\n        assert \"external_id\" in integration_params\n        assert \"external_id\" in identity_params\n\n        integration = Factories.create_provider_integration(**integration_params)\n        identity_provider = Factories.create_identity_provider(integration=integration)\n        identity = Factories.create_identity(\n            user=user, identity_provider=identity_provider, **identity_params\n        )\n        organization_integration = integration.add_organization(\n            organization_id=organization.id, user=user, default_auth_id=identity.id\n        )\n        assert organization_integration is not None\n        return integration, organization_integration, identity, identity_provider\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_organization_integration(**integration_params: Any) -> OrganizationIntegration:\n        return OrganizationIntegration.objects.create(**integration_params)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity_provider(\n        integration: Integration | None = None,\n        config: dict[str, Any] | None = None,\n        **kwargs: Any,\n    ) -> IdentityProvider:\n        if integration is not None:\n            integration_values = dict(\n                type=integration.provider,\n                external_id=integration.external_id,\n            )\n            if any((key in kwargs) for key in integration_values):\n                raise ValueError(\n                    \"Values from integration should not be in kwargs: \"\n                    + repr(list(integration_values.keys()))\n                )\n            kwargs.update(integration_values)\n\n        return IdentityProvider.objects.create(config=config or {}, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity(\n        user: User | RpcUser, identity_provider: IdentityProvider, external_id: str, **kwargs: Any\n    ) -> Identity:\n        return Identity.objects.create(\n            external_id=external_id,\n            idp=identity_provider,\n            user_id=user.id,\n            status=IdentityStatus.VALID,\n            scopes=[],\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_group_history(\n        group: Group,\n        status: int,\n        release: Release | None = None,\n        user_id: int | None = None,\n        team_id: int | None = None,\n        prev_history: GroupHistory | None = None,\n        date_added: datetime | None = None,\n    ) -> GroupHistory:\n        prev_history_date = None\n        if prev_history:\n            prev_history_date = prev_history.date_added\n\n        kwargs = {}\n        if date_added:\n            kwargs[\"date_added\"] = date_added\n        return GroupHistory.objects.create(\n            organization=group.organization,\n            group=group,\n            project=group.project,\n            release=release,\n            user_id=user_id,\n            team_id=team_id,\n            status=status,\n            prev_history=prev_history,\n            prev_history_date=prev_history_date,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_comment(issue, project, user, text=\"hello world\"):\n        data = {\"text\": text}\n        return Activity.objects.create(\n            project=project,\n            group=issue,\n            type=ActivityType.NOTE.value,\n            user_id=user.id,\n            data=data,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_saved_search(name: str, **kwargs):\n        if \"owner\" in kwargs:\n            owner = kwargs.pop(\"owner\")\n            kwargs[\"owner_id\"] = owner.id if not isinstance(owner, int) else owner\n        return SavedSearch.objects.create(name=name, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_notification_action(\n        organization: Organization | None = None,\n        projects: list[Project] | None = None,\n        **kwargs,\n    ):\n        if not organization:\n            organization = Factories.create_organization()\n\n        if not projects:\n            projects = []\n\n        action_kwargs = {\n            \"organization\": organization,\n            \"type\": ActionService.SENTRY_NOTIFICATION,\n            \"target_type\": ActionTarget.USER,\n            \"target_identifier\": \"1\",\n            \"target_display\": \"Sentry User\",\n            \"trigger_type\": ActionTrigger.AUDIT_LOG,\n            **kwargs,\n        }\n\n        action = NotificationAction.objects.create(**action_kwargs)\n        action.projects.add(*projects)\n        action.save()\n\n        return action\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_notification_settings_provider(*args, **kwargs) -> NotificationSettingProvider:\n        return NotificationSettingProvider.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_option(*args, **kwargs) -> UserOption:\n        return UserOption.objects.create(*args, **kwargs)\n\n    @staticmethod\n    def create_basic_auth_header(username: str, password: str = \"\") -> bytes:\n        return b\"Basic \" + b64encode(f\"{username}:{password}\".encode())\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def snooze_rule(**kwargs):\n        return RuleSnooze.objects.create(**kwargs)\n\n    @staticmethod\n    def create_request_access(\n        sso_state: RpcMemberSsoState | None = None,\n        permissions: list | None = None,\n        org_context: RpcUserOrganizationContext | None = None,\n        scopes_upper_bound: frozenset | None = frozenset(),\n    ) -> RpcBackedAccess:\n        if not sso_state:\n            sso_state = RpcMemberSsoState()\n        if not permissions:\n            permissions = []\n        if not org_context:\n            org_context = RpcUserOrganizationContext()\n\n        auth_state = RpcAuthState(sso_state=sso_state, permissions=permissions)\n        return RpcBackedAccess(\n            rpc_user_organization_context=org_context,\n            auth_state=auth_state,\n            scopes_upper_bound=scopes_upper_bound,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_webhook_payload(mailbox_name: str, region_name: str, **kwargs) -> WebhookPayload:\n        payload_kwargs = {\n            \"request_method\": \"POST\",\n            \"request_path\": \"/extensions/github/webhook/\",\n            \"request_headers\": '{\"Content-Type\": \"application/json\"}',\n            \"request_body\": \"{}\",\n            **kwargs,\n        }\n        return WebhookPayload.objects.create(\n            mailbox_name=mailbox_name, region_name=region_name, **payload_kwargs\n        )\n\n    @staticmethod\n    def create_uptime_subscription(\n        type: str,\n        subscription_id: str | None,\n        status: UptimeSubscription.Status,\n        url: str | None,\n        url_domain: str,\n        url_domain_suffix: str,\n        host_provider_id: str,\n        host_provider_name: str,\n        interval_seconds: IntervalSecondsLiteral,\n        timeout_ms: int,\n        method,\n        headers,\n        body,\n        date_updated: datetime,\n        uptime_status: UptimeStatus,\n        uptime_status_update_date: datetime,\n        trace_sampling: bool = False,\n    ):\n        if url is None:\n            url = petname.generate().title()\n            url = f\"http://{url}.com\"\n\n        return UptimeSubscription.objects.create(\n            type=type,\n            subscription_id=subscription_id,\n            status=status.value,\n            url=url,\n            url_domain=url_domain,\n            url_domain_suffix=url_domain_suffix,\n            host_provider_id=host_provider_id,\n            host_provider_name=host_provider_name,\n            interval_seconds=interval_seconds,\n            timeout_ms=timeout_ms,\n            date_updated=date_updated,\n            method=method,\n            headers=headers,\n            body=body,\n            trace_sampling=trace_sampling,\n            uptime_status=uptime_status,\n            uptime_status_update_date=uptime_status_update_date,\n        )\n\n    @staticmethod\n    def create_project_uptime_subscription(\n        project: Project,\n        env: Environment | None,\n        uptime_subscription: UptimeSubscription,\n        status: int,\n        mode: UptimeMonitorMode,\n        name: str | None,\n        owner: Actor | None,\n        id: int | None,\n    ):\n        if name is None:\n            name = petname.generate().title()\n        owner_team_id = None\n        owner_user_id = None\n        if owner:\n            if owner.is_team:\n                owner_team_id = owner.id\n            elif owner.is_user:\n                owner_user_id = owner.id\n\n        return ProjectUptimeSubscription.objects.create(\n            uptime_subscription=uptime_subscription,\n            project=project,\n            environment=env,\n            status=status,\n            mode=mode,\n            name=name,\n            owner_team_id=owner_team_id,\n            owner_user_id=owner_user_id,\n            pk=id,\n        )\n\n    @staticmethod\n    def create_uptime_subscription_region(\n        subscription: UptimeSubscription,\n        region_slug: str,\n        mode: UptimeSubscriptionRegion.RegionMode,\n    ) -> UptimeSubscriptionRegion:\n        return UptimeSubscriptionRegion.objects.create(\n            uptime_subscription=subscription,\n            region_slug=region_slug,\n            mode=mode,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard(\n        organization: Organization | None = None,\n        title: str | None = None,\n        created_by: User | None = None,\n        **kwargs,\n    ):\n        if organization is None:\n            organization = Factories.create_organization()\n        if created_by is None:\n            created_by = Factories.create_user()\n            Factories.create_member(organization=organization, user=created_by, role=\"owner\")\n        if title is None:\n            title = petname.generate(2, \" \", letters=10).title()\n        return Dashboard.objects.create(\n            organization=organization, title=title, created_by_id=created_by.id, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard_widget(\n        order: int,\n        dashboard: Dashboard | None = None,\n        title: str | None = None,\n        display_type: int | None = None,\n        **kwargs,\n    ):\n        if dashboard is None:\n            dashboard = Factories.create_dashboard()\n        if display_type is None:\n            display_type = DashboardWidgetDisplayTypes.AREA_CHART\n        if title is None:\n            title = petname.generate(2, \" \", letters=10).title()\n\n        return DashboardWidget.objects.create(\n            dashboard=dashboard, title=title, display_type=display_type, order=order, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard_widget_query(\n        order: int,\n        widget: DashboardWidget | None = None,\n        name: str | None = None,\n        **kwargs,\n    ):\n        if widget is None:\n            widget = Factories.create_dashboard_widget(order=order)\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        return DashboardWidgetQuery.objects.create(widget=widget, name=name, order=order, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_workflow(\n        name: str | None = None,\n        organization: Organization | None = None,\n        config: dict[str, Any] | None = None,\n        **kwargs,\n    ) -> Workflow:\n        if organization is None:\n            organization = Factories.create_organization()\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        if config is None:\n            config = {}\n        return Workflow.objects.create(\n            organization=organization, name=name, config=config, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition_group(\n        organization: Organization | None = None,\n        **kwargs,\n    ) -> DataConditionGroup:\n        if organization is None:\n            organization = Factories.create_organization()\n        return DataConditionGroup.objects.create(organization=organization, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_workflow_data_condition_group(\n        workflow: Workflow | None = None,\n        condition_group: DataConditionGroup | None = None,\n        **kwargs,\n    ) -> WorkflowDataConditionGroup:\n        if workflow is None:\n            workflow = Factories.create_workflow()\n\n        if not condition_group:\n            condition_group = Factories.create_data_condition_group()\n\n        return WorkflowDataConditionGroup.objects.create(\n            workflow=workflow, condition_group=condition_group\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition(\n        condition_group: DataConditionGroup | None = None, **kwargs\n    ) -> DataCondition:\n        if condition_group is None:\n            condition_group = Factories.create_data_condition_group()\n        return DataCondition.objects.create(condition_group=condition_group, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_source(\n        organization: Organization | None = None,\n        source_id: str | None = None,\n        type: str | None = None,\n        **kwargs,\n    ) -> DataSource:\n        if organization is None:\n            organization = Factories.create_organization()\n        if source_id is None:\n            source_id = str(random.randint(1, 10000))\n        if type is None:\n            type = data_source_type_registry.get_key(QuerySubscriptionDataSourceHandler)\n        return DataSource.objects.create(organization=organization, source_id=source_id, type=type)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector(\n        name: str | None = None,\n        config: dict | None = None,\n        **kwargs,\n    ) -> Detector:\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        if config is None:\n            config = default_detector_config_data.get(kwargs[\"type\"], {})\n\n        return Detector.objects.create(\n            name=name,\n            config=config,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector_state(\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> DetectorState:\n        if detector is None:\n            detector = Factories.create_detector()\n\n        return DetectorState.objects.create(detector=detector, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_source_detector(\n        data_source: DataSource | None = None,\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> DataSourceDetector:\n        if data_source is None:\n            data_source = Factories.create_data_source()\n        if detector is None:\n            detector = Factories.create_detector()\n        return DataSourceDetector.objects.create(data_source=data_source, detector=detector)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_action(\n        config: dict[str, Any] | None = None,\n        type: Action.Type | None = None,\n        data: dict[str, Any] | None = None,\n        **kwargs,\n    ) -> Action:\n        if config is None and type is None and data is None:\n            # Default to a slack action with nice defaults so someone can just do\n            # self.create_action() and have a sane default\n            config = {\n                \"target_identifier\": \"1\",\n                \"target_display\": \"Sentry User\",\n                \"target_type\": ActionTarget.SPECIFIC,\n            }\n\n            data = {\"notes\": \"bufos are great\", \"tags\": \"bufo-bot\"}\n\n        if config is None:\n            config = {}\n\n        if data is None:\n            data = {}\n\n        if type is None:\n            type = Action.Type.SLACK\n\n        return Action.objects.create(type=type, config=config, data=data, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector_workflow(\n        detector: Detector | None = None,\n        workflow: Workflow | None = None,\n        **kwargs,\n    ) -> DetectorWorkflow:\n        if detector is None:\n            detector = Factories.create_detector()\n        if workflow is None:\n            workflow = Factories.create_workflow()\n        return DetectorWorkflow.objects.create(detector=detector, workflow=workflow, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_workflow(\n        alert_rule_id: int | None = None,\n        rule_id: int | None = None,\n        workflow: Workflow | None = None,\n        **kwargs,\n    ) -> AlertRuleWorkflow:\n        if rule_id is None and alert_rule_id is None:\n            raise ValueError(\"Either rule_id or alert_rule_id must be provided\")\n\n        if rule_id is not None and alert_rule_id is not None:\n            raise ValueError(\"Only one of rule_id or alert_rule_id can be provided\")\n\n        if workflow is None:\n            workflow = Factories.create_workflow()\n\n        return AlertRuleWorkflow.objects.create(\n            alert_rule_id=alert_rule_id, rule_id=rule_id, workflow=workflow, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_group_open_period(\n        incident: Incident,\n        group_open_period: GroupOpenPeriod,\n        **kwargs,\n    ) -> IncidentGroupOpenPeriod:\n        return IncidentGroupOpenPeriod.objects.create(\n            incident_id=incident.id,\n            incident_identifier=incident.identifier,\n            group_open_period=group_open_period,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_detector(\n        alert_rule_id: int | None = None,\n        rule_id: int | None = None,\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> AlertRuleDetector:\n        if rule_id is None and alert_rule_id is None:\n            raise ValueError(\"Either rule_id or alert_rule_id must be provided\")\n\n        if rule_id is not None and alert_rule_id is not None:\n            raise ValueError(\"Only one of rule_id or alert_rule_id can be provided\")\n\n        if detector is None:\n            detector = Factories.create_detector()\n\n        return AlertRuleDetector.objects.create(\n            alert_rule_id=alert_rule_id, rule_id=rule_id, detector=detector, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_action_alert_rule_trigger_action(\n        alert_rule_trigger_action_id: int,\n        action: Action | None = None,\n        **kwargs,\n    ) -> ActionAlertRuleTriggerAction:\n        if action is None:\n            action = Factories.create_action()\n\n        return ActionAlertRuleTriggerAction.objects.create(\n            action=action, alert_rule_trigger_action_id=alert_rule_trigger_action_id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition_group_action(\n        action: Action | None = None,\n        condition_group: DataConditionGroup | None = None,\n        **kwargs,\n    ) -> DataConditionGroupAction:\n        if action is None:\n            action = Factories.create_action()\n        if condition_group is None:\n            condition_group = Factories.create_data_condition_group()\n        return DataConditionGroupAction.objects.create(\n            action=action, condition_group=condition_group, **kwargs\n        )\n",
    "file_path": "/Users/wangyue/Code/CodeReviewData/ReviewDataset/sentry-greptile/src/sentry/testutils/factories.py",
    "line_count": 2375,
    "encoding": "utf-8",
    "error": null
}

Message 3 [Assistant]:
--------------------------------------------------------------------------------
Content:
现在让我查看函数 `_set_sample_rate_from_error_sampling` 的完整定义和调用情况：
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "_set_sample_rate_from_error_sampling",
      "is_regex": false,
      "context_lines": 10
    },
    "id": "call_00_NkTSfYivKHAiLldynSO9x6JT",
    "type": "tool_call"
  }
]

Message 4 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_NkTSfYivKHAiLldynSO9x6JT
Result:
{
    "matches": [
        {
            "file": "src/sentry/testutils/factories.py",
            "line_number": 344,
            "matched_line": "def _set_sample_rate_from_error_sampling(normalized_data: MutableMapping[str, Any]) -> None:",
            "context_start_line": 334,
            "context_end_line": 354,
            "context": [
                "334:         manifest[\"org\"] = org",
                "335:     if release:",
                "336:         manifest[\"release\"] = release",
                "337:     if project:",
                "338:         manifest[\"project\"] = project",
                "339:     for path in extra_files or {}:",
                "340:         manifest[\"files\"][path] = {\"url\": path}",
                "341:     return orjson.dumps(manifest).decode()",
                "342: ",
                "343: ",
                "344: def _set_sample_rate_from_error_sampling(normalized_data: MutableMapping[str, Any]) -> None:",
                "345:     \"\"\"Set 'sample_rate' on normalized_data if contexts.error_sampling.client_sample_rate is present and valid.\"\"\"",
                "346:     client_sample_rate = None",
                "347:     try:",
                "348:         client_sample_rate = (",
                "349:             normalized_data.get(\"contexts\", {}).get(\"error_sampling\", {}).get(\"client_sample_rate\")",
                "350:         )",
                "351:     except Exception:",
                "352:         pass",
                "353:     if client_sample_rate:",
                "354:         try:"
            ]
        },
        {
            "file": "src/sentry/testutils/factories.py",
            "line_number": 1049,
            "matched_line": "        _set_sample_rate_from_error_sampling(normalized_data)",
            "context_start_line": 1039,
            "context_end_line": 1059,
            "context": [
                "1039:             data.update({\"exception\": [{\"value\": \"BadError\"}]})",
                "1040: ",
                "1041:         manager = EventManager(data, sent_at=sent_at)",
                "1042:         manager.normalize()",
                "1043:         if assert_no_errors:",
                "1044:             errors = manager.get_data().get(\"errors\")",
                "1045:             assert not errors, errors",
                "1046: ",
                "1047:         normalized_data = manager.get_data()",
                "1048: ",
                "1049:         _set_sample_rate_from_error_sampling(normalized_data)",
                "1050: ",
                "1051:         event = None",
                "1052: ",
                "1053:         # When fingerprint is present on transaction, inject performance problems",
                "1054:         if (",
                "1055:             normalized_data.get(\"type\") == \"transaction\"",
                "1056:             and normalized_data.get(\"fingerprint\") is not None",
                "1057:         ):",
                "1058:             with mock.patch(",
                "1059:                 \"sentry.event_manager._detect_performance_problems\","
            ]
        }
    ],
    "total": 2
}

Message 5 [Assistant]:
--------------------------------------------------------------------------------
Content:
现在让我查看调用这个函数的上下文，特别是 `store_event` 函数：
Tool Calls:
[
  {
    "name": "read_file",
    "args": {
      "file_path": "src/sentry/testutils/factories.py",
      "encoding": "utf-8"
    },
    "id": "call_00_HuXpOaMarWbWdTnRAxoicrSj",
    "type": "tool_call"
  }
]

Message 6 [Tool]:
--------------------------------------------------------------------------------
Tool: read_file
Tool Call ID: call_00_HuXpOaMarWbWdTnRAxoicrSj
Result:
{
    "content": "from __future__ import annotations\n\nimport contextlib\nimport copy\nimport io\nimport os\nimport random\nimport zipfile\nfrom base64 import b64encode\nfrom binascii import hexlify\nfrom collections.abc import Mapping, MutableMapping, Sequence\nfrom datetime import UTC, datetime\nfrom enum import Enum\nfrom hashlib import sha1\nfrom importlib import import_module\nfrom typing import Any\nfrom unittest import mock\nfrom uuid import uuid4\n\nimport orjson\nimport petname\nfrom django.conf import settings\nfrom django.contrib.auth.models import AnonymousUser\nfrom django.core.files.base import ContentFile\nfrom django.db import router, transaction\nfrom django.test.utils import override_settings\nfrom django.utils import timezone\nfrom django.utils.text import slugify\n\nfrom sentry.auth.access import RpcBackedAccess\nfrom sentry.auth.services.auth.model import RpcAuthState, RpcMemberSsoState\nfrom sentry.constants import SentryAppInstallationStatus, SentryAppStatus\nfrom sentry.event_manager import EventManager\nfrom sentry.eventstore.models import Event\nfrom sentry.hybridcloud.models.outbox import RegionOutbox, outbox_context\nfrom sentry.hybridcloud.models.webhookpayload import WebhookPayload\nfrom sentry.hybridcloud.outbox.category import OutboxCategory, OutboxScope\nfrom sentry.incidents.grouptype import MetricIssue\nfrom sentry.incidents.logic import (\n    create_alert_rule,\n    create_alert_rule_trigger,\n    create_alert_rule_trigger_action,\n    query_datasets_to_type,\n)\nfrom sentry.incidents.models.alert_rule import (\n    AlertRuleDetectionType,\n    AlertRuleThresholdType,\n    AlertRuleTriggerAction,\n)\nfrom sentry.incidents.models.incident import (\n    Incident,\n    IncidentActivity,\n    IncidentProject,\n    IncidentTrigger,\n    IncidentType,\n    TriggerStatus,\n)\nfrom sentry.integrations.models.doc_integration import DocIntegration\nfrom sentry.integrations.models.doc_integration_avatar import DocIntegrationAvatar\nfrom sentry.integrations.models.external_actor import ExternalActor\nfrom sentry.integrations.models.external_issue import ExternalIssue\nfrom sentry.integrations.models.integration import Integration\nfrom sentry.integrations.models.integration_external_project import IntegrationExternalProject\nfrom sentry.integrations.models.integration_feature import (\n    Feature,\n    IntegrationFeature,\n    IntegrationTypes,\n)\nfrom sentry.integrations.models.organization_integration import OrganizationIntegration\nfrom sentry.integrations.models.repository_project_path_config import RepositoryProjectPathConfig\nfrom sentry.integrations.types import ExternalProviders\nfrom sentry.issues.grouptype import get_group_type_by_type_id\nfrom sentry.models.activity import Activity\nfrom sentry.models.apikey import ApiKey\nfrom sentry.models.apitoken import ApiToken\nfrom sentry.models.artifactbundle import ArtifactBundle\nfrom sentry.models.authidentity import AuthIdentity\nfrom sentry.models.authprovider import AuthProvider\nfrom sentry.models.commit import Commit\nfrom sentry.models.commitauthor import CommitAuthor\nfrom sentry.models.commitfilechange import CommitFileChange\nfrom sentry.models.dashboard import Dashboard\nfrom sentry.models.dashboard_widget import (\n    DashboardWidget,\n    DashboardWidgetDisplayTypes,\n    DashboardWidgetQuery,\n)\nfrom sentry.models.debugfile import ProjectDebugFile\nfrom sentry.models.environment import Environment\nfrom sentry.models.files.control_file import ControlFile\nfrom sentry.models.files.file import File\nfrom sentry.models.group import Group\nfrom sentry.models.grouphistory import GroupHistory\nfrom sentry.models.grouplink import GroupLink\nfrom sentry.models.groupopenperiod import GroupOpenPeriod\nfrom sentry.models.grouprelease import GroupRelease\nfrom sentry.models.organization import Organization\nfrom sentry.models.organizationmapping import OrganizationMapping\nfrom sentry.models.organizationmember import OrganizationMember\nfrom sentry.models.organizationmemberinvite import OrganizationMemberInvite\nfrom sentry.models.organizationmemberteam import OrganizationMemberTeam\nfrom sentry.models.organizationslugreservation import OrganizationSlugReservation\nfrom sentry.models.orgauthtoken import OrgAuthToken\nfrom sentry.models.project import Project\nfrom sentry.models.projectbookmark import ProjectBookmark\nfrom sentry.models.projectcodeowners import ProjectCodeOwners\nfrom sentry.models.projecttemplate import ProjectTemplate\nfrom sentry.models.release import Release, ReleaseStatus\nfrom sentry.models.releasecommit import ReleaseCommit\nfrom sentry.models.releaseenvironment import ReleaseEnvironment\nfrom sentry.models.releasefile import ReleaseFile, update_artifact_index\nfrom sentry.models.releaseprojectenvironment import ReleaseProjectEnvironment\nfrom sentry.models.repository import Repository\nfrom sentry.models.rule import Rule\nfrom sentry.models.rulesnooze import RuleSnooze\nfrom sentry.models.savedsearch import SavedSearch\nfrom sentry.models.team import Team\nfrom sentry.models.userreport import UserReport\nfrom sentry.notifications.models.notificationaction import (\n    ActionService,\n    ActionTarget,\n    ActionTrigger,\n    NotificationAction,\n)\nfrom sentry.notifications.models.notificationsettingprovider import NotificationSettingProvider\nfrom sentry.organizations.services.organization import RpcOrganization, RpcUserOrganizationContext\nfrom sentry.performance_issues.performance_problem import PerformanceProblem\nfrom sentry.sentry_apps.installations import (\n    SentryAppInstallationCreator,\n    SentryAppInstallationTokenCreator,\n)\nfrom sentry.sentry_apps.logic import SentryAppCreator\nfrom sentry.sentry_apps.models.platformexternalissue import PlatformExternalIssue\nfrom sentry.sentry_apps.models.sentry_app import SentryApp\nfrom sentry.sentry_apps.models.sentry_app_avatar import SentryAppAvatar\nfrom sentry.sentry_apps.models.sentry_app_installation import SentryAppInstallation\nfrom sentry.sentry_apps.models.sentry_app_installation_for_provider import (\n    SentryAppInstallationForProvider,\n)\nfrom sentry.sentry_apps.models.servicehook import ServiceHook\nfrom sentry.sentry_apps.services.hook import hook_service\nfrom sentry.sentry_apps.token_exchange.grant_exchanger import GrantExchanger\nfrom sentry.signals import project_created\nfrom sentry.silo.base import SiloMode\nfrom sentry.snuba.dataset import Dataset\nfrom sentry.snuba.models import QuerySubscriptionDataSourceHandler\nfrom sentry.tempest.models import MessageType as TempestMessageType\nfrom sentry.tempest.models import TempestCredentials\nfrom sentry.testutils.outbox import outbox_runner\nfrom sentry.testutils.silo import assume_test_silo_mode\nfrom sentry.types.activity import ActivityType\nfrom sentry.types.actor import Actor\nfrom sentry.types.region import Region, get_local_region, get_region_by_name\nfrom sentry.types.token import AuthTokenType\nfrom sentry.uptime.models import (\n    IntervalSecondsLiteral,\n    ProjectUptimeSubscription,\n    UptimeStatus,\n    UptimeSubscription,\n    UptimeSubscriptionRegion,\n)\nfrom sentry.uptime.types import UptimeMonitorMode\nfrom sentry.users.models.identity import Identity, IdentityProvider, IdentityStatus\nfrom sentry.users.models.user import User\nfrom sentry.users.models.user_avatar import UserAvatar\nfrom sentry.users.models.user_option import UserOption\nfrom sentry.users.models.useremail import UserEmail\nfrom sentry.users.models.userpermission import UserPermission\nfrom sentry.users.models.userrole import UserRole\nfrom sentry.users.services.user import RpcUser\nfrom sentry.utils import loremipsum\nfrom sentry.workflow_engine.models import (\n    Action,\n    ActionAlertRuleTriggerAction,\n    AlertRuleDetector,\n    AlertRuleWorkflow,\n    DataCondition,\n    DataConditionGroup,\n    DataConditionGroupAction,\n    DataSource,\n    DataSourceDetector,\n    Detector,\n    DetectorState,\n    DetectorWorkflow,\n    IncidentGroupOpenPeriod,\n    Workflow,\n    WorkflowDataConditionGroup,\n)\nfrom sentry.workflow_engine.registry import data_source_type_registry\nfrom social_auth.models import UserSocialAuth\n\n\nclass EventType(Enum):\n    ERROR = \"error\"\n    DEFAULT = \"default\"\n\n\ndef get_fixture_path(*parts: str) -> str:\n    path = os.path.realpath(__file__)\n    for _ in range(4):  # src/sentry/testutils/{__file__}\n        path = os.path.dirname(path)\n    return os.path.join(path, \"fixtures\", *parts)\n\n\ndef make_sentence(words=None):\n    if words is None:\n        words = int(random.weibullvariate(8, 3))\n    return \" \".join(random.choice(loremipsum.words) for _ in range(words))\n\n\ndef make_word(words=None):\n    if words is None:\n        words = int(random.weibullvariate(8, 3))\n    return random.choice(loremipsum.words)\n\n\nDEFAULT_EVENT_DATA = {\n    \"extra\": {\n        \"loadavg\": [0.97607421875, 0.88330078125, 0.833984375],\n        \"sys.argv\": [\n            \"/Users/dcramer/.virtualenvs/sentry/bin/raven\",\n            \"test\",\n            \"https://ebc35f33e151401f9deac549978bda11:f3403f81e12e4c24942d505f086b2cad@sentry.io/1\",\n        ],\n        \"user\": \"dcramer\",\n    },\n    \"modules\": {\"raven\": \"3.1.13\"},\n    \"request\": {\n        \"cookies\": {},\n        \"data\": {},\n        \"env\": {},\n        \"headers\": {},\n        \"method\": \"GET\",\n        \"query_string\": \"\",\n        \"url\": \"http://example.com\",\n    },\n    \"stacktrace\": {\n        \"frames\": [\n            {\n                \"abs_path\": \"www/src/sentry/models/foo.py\",\n                \"context_line\": \"                        string_max_length=self.string_max_length)\",\n                \"filename\": \"sentry/models/foo.py\",\n                \"function\": \"build_msg\",\n                \"in_app\": True,\n                \"lineno\": 29,\n                \"module\": \"raven.base\",\n                \"post_context\": [\n                    \"                },\",\n                    \"            })\",\n                    \"\",\n                    \"        if 'stacktrace' in data:\",\n                    \"            if self.include_paths:\",\n                ],\n                \"pre_context\": [\n                    \"\",\n                    \"            data.update({\",\n                    \"                'stacktrace': {\",\n                    \"                    'frames': get_stack_info(frames,\",\n                    \"                        list_max_length=self.list_max_length,\",\n                ],\n                \"vars\": {\n                    \"culprit\": \"raven.scripts.runner\",\n                    \"date\": \"datetime.datetime(2013, 2, 14, 20, 6, 33, 479471)\",\n                    \"event_id\": \"598fb19363e745ec8be665e6ba88b1b2\",\n                    \"event_type\": \"raven.events.Message\",\n                    \"frames\": \"<generator object iter_stack_frames at 0x103fef050>\",\n                    \"handler\": \"<raven.events.Message object at 0x103feb710>\",\n                    \"k\": \"logentry\",\n                    \"public_key\": None,\n                    \"result\": {\n                        \"logentry\": \"{'message': 'This is a test message generated using ``raven test``', 'params': []}\"\n                    },\n                    \"self\": \"<raven.base.Client object at 0x104397f10>\",\n                    \"stack\": True,\n                    \"tags\": None,\n                    \"time_spent\": None,\n                },\n            },\n            {\n                \"abs_path\": \"/Users/dcramer/.virtualenvs/sentry/lib/python2.7/site-packages/raven/base.py\",\n                \"context_line\": \"                        string_max_length=self.string_max_length)\",\n                \"filename\": \"raven/base.py\",\n                \"function\": \"build_msg\",\n                \"in_app\": False,\n                \"lineno\": 290,\n                \"module\": \"raven.base\",\n                \"post_context\": [\n                    \"                },\",\n                    \"            })\",\n                    \"\",\n                    \"        if 'stacktrace' in data:\",\n                    \"            if self.include_paths:\",\n                ],\n                \"pre_context\": [\n                    \"\",\n                    \"            data.update({\",\n                    \"                'stacktrace': {\",\n                    \"                    'frames': get_stack_info(frames,\",\n                    \"                        list_max_length=self.list_max_length,\",\n                ],\n                \"vars\": {\n                    \"culprit\": \"raven.scripts.runner\",\n                    \"date\": \"datetime.datetime(2013, 2, 14, 20, 6, 33, 479471)\",\n                    \"event_id\": \"598fb19363e745ec8be665e6ba88b1b2\",\n                    \"event_type\": \"raven.events.Message\",\n                    \"frames\": \"<generator object iter_stack_frames at 0x103fef050>\",\n                    \"handler\": \"<raven.events.Message object at 0x103feb710>\",\n                    \"k\": \"logentry\",\n                    \"public_key\": None,\n                    \"result\": {\n                        \"logentry\": \"{'message': 'This is a test message generated using ``raven test``', 'params': []}\"\n                    },\n                    \"self\": \"<raven.base.Client object at 0x104397f10>\",\n                    \"stack\": True,\n                    \"tags\": None,\n                    \"time_spent\": None,\n                },\n            },\n        ]\n    },\n    \"tags\": [],\n    \"platform\": \"python\",\n}\n\ndefault_detector_config_data = {\n    MetricIssue.slug: {\"threshold_period\": 1, \"detection_type\": \"static\"}\n}\n\n\ndef _patch_artifact_manifest(path, org=None, release=None, project=None, extra_files=None):\n    with open(path, \"rb\") as fp:\n        manifest = orjson.loads(fp.read())\n    if org:\n        manifest[\"org\"] = org\n    if release:\n        manifest[\"release\"] = release\n    if project:\n        manifest[\"project\"] = project\n    for path in extra_files or {}:\n        manifest[\"files\"][path] = {\"url\": path}\n    return orjson.dumps(manifest).decode()\n\n\ndef _set_sample_rate_from_error_sampling(normalized_data: MutableMapping[str, Any]) -> None:\n    \"\"\"Set 'sample_rate' on normalized_data if contexts.error_sampling.client_sample_rate is present and valid.\"\"\"\n    client_sample_rate = None\n    try:\n        client_sample_rate = (\n            normalized_data.get(\"contexts\", {}).get(\"error_sampling\", {}).get(\"client_sample_rate\")\n        )\n    except Exception:\n        pass\n    if client_sample_rate:\n        try:\n            normalized_data[\"sample_rate\"] = float(client_sample_rate)\n        except Exception:\n            pass\n\n\n# TODO(dcramer): consider moving to something more scalable like factoryboy\nclass Factories:\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_organization(name=None, owner=None, region: Region | str | None = None, **kwargs):\n        if not name:\n            name = petname.generate(2, \" \", letters=10).title()\n\n        with contextlib.ExitStack() as ctx:\n            if region is None or SiloMode.get_current_mode() == SiloMode.MONOLITH:\n                region_name = get_local_region().name\n            else:\n                if isinstance(region, Region):\n                    region_name = region.name\n                else:\n                    region_obj = get_region_by_name(region)  # Verify it exists\n                    region_name = region_obj.name\n\n                ctx.enter_context(\n                    override_settings(SILO_MODE=SiloMode.REGION, SENTRY_REGION=region_name)\n                )\n\n            with outbox_context(flush=False):\n                org = Organization.objects.create(name=name, **kwargs)\n\n            with assume_test_silo_mode(SiloMode.CONTROL):\n                # Organization mapping creation relies on having a matching org slug reservation\n                OrganizationSlugReservation(\n                    organization_id=org.id,\n                    region_name=region_name,\n                    user_id=owner.id if owner else -1,\n                    slug=org.slug,\n                ).save(unsafe_write=True)\n\n            # Manually replicate org data after adding an org slug reservation\n            org.handle_async_replication(org.id)\n\n            # Flush remaining organization update outboxes accumulated by org create\n            RegionOutbox(\n                shard_identifier=org.id,\n                shard_scope=OutboxScope.ORGANIZATION_SCOPE,\n                category=OutboxCategory.ORGANIZATION_UPDATE,\n            ).drain_shard()\n\n        if owner:\n            Factories.create_member(organization=org, user_id=owner.id, role=\"owner\")\n        return org\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_org_mapping(org=None, **kwds):\n        if org:\n            kwds.setdefault(\"organization_id\", org.id)\n            kwds.setdefault(\"slug\", org.slug)\n            kwds.setdefault(\"name\", org.name)\n            kwds.setdefault(\"idempotency_key\", uuid4().hex)\n            kwds.setdefault(\"region_name\", \"na\")\n        return OrganizationMapping.objects.create(**kwds)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_member(teams=None, team_roles=None, **kwargs):\n        kwargs.setdefault(\"role\", \"member\")\n        teamRole = kwargs.pop(\"teamRole\", None)\n\n        # user_id will have precedence over user\n        user = kwargs.pop(\"user\", None)\n        user_id = kwargs.pop(\"user_id\", None)\n        if not user_id and user:\n            user_id = user.id\n        kwargs[\"user_id\"] = user_id\n\n        # inviter_id will have precedence over inviter\n        inviter = kwargs.pop(\"inviter\", None)\n        inviter_id = kwargs.pop(\"inviter_id\", None)\n        if not inviter_id and inviter:\n            inviter_id = inviter.id\n        kwargs[\"inviter_id\"] = inviter_id\n\n        om = OrganizationMember.objects.create(**kwargs)\n\n        if team_roles:\n            for team, role in team_roles:\n                Factories.create_team_membership(team=team, member=om, role=role)\n        elif teams:\n            for team in teams:\n                Factories.create_team_membership(team=team, member=om, role=teamRole)\n        return om\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_member_invite(\n        organization: Organization | None = None,\n        email: str | None = None,\n        **kwargs,\n    ) -> OrganizationMemberInvite:\n        if organization is None:\n            organization = Factories.create_organization()\n        if email is None:\n            email = f\"{petname.generate().title()}@email.com\"\n        om = OrganizationMember.objects.create(organization=organization)\n        return OrganizationMemberInvite.objects.create(\n            organization=organization, organization_member_id=om.id, email=email, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_team_membership(team, member=None, user=None, role=None):\n        if member is None:\n            member, created = OrganizationMember.objects.get_or_create(\n                user_id=user.id if user else None,\n                organization=team.organization,\n                defaults={\"role\": \"member\"},\n            )\n\n        return OrganizationMemberTeam.objects.create(\n            team=team, organizationmember=member, is_active=True, role=role\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_api_key(organization, **kwargs) -> ApiKey:\n        return ApiKey.objects.create(organization_id=organization.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_auth_provider(**kwargs):\n        return AuthProvider.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_auth_identity(**kwargs):\n        return AuthIdentity.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_auth_token(user, scope_list: list[str] | None = None, **kwargs) -> ApiToken:\n        if scope_list is None:\n            scope_list = []\n        return ApiToken.objects.create(\n            user=user,\n            scope_list=scope_list,\n            token_type=AuthTokenType.USER,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_org_auth_token(*args, **kwargs) -> OrgAuthToken:\n        return OrgAuthToken.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_team(organization, **kwargs):\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n        if not kwargs.get(\"slug\"):\n            kwargs[\"slug\"] = slugify(str(kwargs[\"name\"]))\n        members = kwargs.pop(\"members\", None)\n\n        team = Team.objects.create(organization=organization, **kwargs)\n        if members:\n            for user in members:\n                Factories.create_team_membership(team=team, user=user)\n        return team\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_environment(project, **kwargs):\n        name = kwargs.get(\"name\", petname.generate(3, \" \", letters=10)[:64])\n\n        organization = kwargs.get(\"organization\")\n        organization_id = organization.id if organization else project.organization_id\n\n        env = Environment.objects.create(organization_id=organization_id, name=name)\n        env.add_project(project, is_hidden=kwargs.get(\"is_hidden\"))\n        return env\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project(\n        organization=None, teams=None, fire_project_created=False, **kwargs\n    ) -> Project:\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n        if not kwargs.get(\"slug\"):\n            kwargs[\"slug\"] = slugify(str(kwargs[\"name\"]))\n        if not organization and teams:\n            organization = teams[0].organization\n\n        with transaction.atomic(router.db_for_write(Project)):\n            project = Project.objects.create(organization=organization, **kwargs)\n            if teams:\n                for team in teams:\n                    project.add_team(team)\n            if fire_project_created:\n                project_created.send(\n                    project=project, user=AnonymousUser(), default_rules=True, sender=Factories\n                )\n        return project\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_template(project=None, organization=None, **kwargs) -> ProjectTemplate:\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n\n        with transaction.atomic(router.db_for_write(Project)):\n            project_template = ProjectTemplate.objects.create(organization=organization, **kwargs)\n\n        return project_template\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_bookmark(project, user):\n        return ProjectBookmark.objects.create(project_id=project.id, user_id=user.id)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_rule(\n        project,\n        action_data=None,\n        allow_no_action_data=False,\n        condition_data=None,\n        name=\"Test Alert\",\n        action_match=\"all\",\n        filter_match=\"all\",\n        frequency=30,\n        **kwargs,\n    ):\n        actions = None\n        if not allow_no_action_data:\n            action_data = action_data or [\n                {\n                    \"id\": \"sentry.rules.actions.notify_event.NotifyEventAction\",\n                    \"name\": \"Send a notification (for all legacy integrations)\",\n                },\n                {\n                    \"id\": \"sentry.rules.actions.notify_event_service.NotifyEventServiceAction\",\n                    \"service\": \"mail\",\n                    \"name\": \"Send a notification via mail\",\n                },\n            ]\n            actions = action_data\n        condition_data = condition_data or [\n            {\n                \"id\": \"sentry.rules.conditions.first_seen_event.FirstSeenEventCondition\",\n                \"name\": \"A new issue is created\",\n            },\n            {\n                \"id\": \"sentry.rules.conditions.every_event.EveryEventCondition\",\n                \"name\": \"The event occurs\",\n            },\n        ]\n        data = {\n            \"conditions\": condition_data,\n            \"action_match\": action_match,\n            \"filter_match\": filter_match,\n            \"frequency\": frequency,\n        }\n        if actions:\n            data[\"actions\"] = actions\n\n        return Rule.objects.create(\n            label=name,\n            project=project,\n            data=data,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_slack_project_rule(project, integration_id, channel_id=None, channel_name=None):\n        action_data = [\n            {\n                \"id\": \"sentry.rules.actions.notify_event.SlackNotifyServiceAction\",\n                \"name\": \"Send a Slack notification\",\n                \"workspace\": integration_id,\n                \"channel_id\": channel_id or \"123453\",\n                \"channel\": channel_name or \"#general\",\n            }\n        ]\n        return Factories.create_project_rule(project, action_data)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_key(project):\n        return project.key_set.get_or_create()[0]\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_tempest_credentials(\n        project: Project,\n        created_by: User | None = None,\n        client_id: str | None = None,\n        client_secret: str | None = None,\n        message: str = \"\",\n        message_type: str | None = None,\n        latest_fetched_item_id: str | None = None,\n    ):\n        if client_id is None:\n            client_id = str(uuid4())\n        if client_secret is None:\n            client_secret = str(uuid4())\n        if message_type is None:\n            message_type = TempestMessageType.ERROR\n\n        return TempestCredentials.objects.create(\n            project=project,\n            created_by_id=created_by.id if created_by else None,\n            client_id=client_id,\n            client_secret=client_secret,\n            message=message,\n            message_type=message_type,\n            latest_fetched_item_id=latest_fetched_item_id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release(\n        project: Project,\n        user: User | None = None,\n        version: str | None = None,\n        date_added: datetime | None = None,\n        additional_projects: Sequence[Project] | None = None,\n        environments: Sequence[Environment] | None = None,\n        date_released: datetime | None = None,\n        adopted: datetime | None = None,\n        unadopted: datetime | None = None,\n        status: int | None = ReleaseStatus.OPEN,\n    ):\n        if version is None:\n            version = hexlify(os.urandom(20)).decode()\n\n        if date_added is None:\n            date_added = timezone.now()\n\n        if additional_projects is None:\n            additional_projects = []\n\n        release = Release.objects.create(\n            version=version,\n            organization_id=project.organization_id,\n            date_added=date_added,\n            date_released=date_released,\n            status=status,\n        )\n\n        release.add_project(project)\n        for additional_project in additional_projects:\n            release.add_project(additional_project)\n\n        for environment in environments or []:\n            ReleaseEnvironment.objects.create(\n                organization=project.organization, release=release, environment=environment\n            )\n            for project in [project, *additional_projects]:\n                ReleaseProjectEnvironment.objects.create(\n                    project=project,\n                    release=release,\n                    environment=environment,\n                    adopted=adopted,\n                    unadopted=unadopted,\n                )\n\n        Activity.objects.create(\n            type=ActivityType.RELEASE.value,\n            project=project,\n            ident=Activity.get_version_ident(version),\n            user_id=user.id if user else None,\n            data={\"version\": version},\n        )\n\n        # add commits\n        if user:\n            author = Factories.create_commit_author(project=project, user=user)\n            repo = Factories.create_repo(project, name=f\"organization-{project.slug}\")\n            commit = Factories.create_commit(\n                project=project,\n                repo=repo,\n                author=author,\n                release=release,\n                key=\"deadbeef\",\n                message=\"placeholder commit message\",\n            )\n\n            release.update(authors=[str(author.id)], commit_count=1, last_commit_id=commit.id)\n\n        return release\n\n    @staticmethod\n    def create_group_release(project: Project, group: Group, release: Release) -> GroupRelease:\n        return GroupRelease.objects.create(\n            project_id=project.id,\n            group_id=group.id,\n            release_id=release.id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release_file(release_id, file=None, name=None, dist_id=None):\n        if file is None:\n            file = Factories.create_file(\n                name=\"log.txt\",\n                size=32,\n                headers={\"Content-Type\": \"text/plain\"},\n                checksum=\"dc1e3f3e411979d336c3057cce64294f3420f93a\",\n            )\n\n        if name is None:\n            name = file.name\n\n        organization_id = Release.objects.get(pk=release_id).organization.id\n\n        return ReleaseFile.objects.create(\n            organization_id=organization_id,\n            release_id=release_id,\n            name=name,\n            file=file,\n            dist_id=dist_id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_artifact_bundle_zip(\n        org=None, release=None, project=None, extra_files=None, fixture_path=\"artifact_bundle\"\n    ):\n        bundle = io.BytesIO()\n        bundle_dir = get_fixture_path(fixture_path)\n        with zipfile.ZipFile(bundle, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n            for path, content in (extra_files or {}).items():\n                zipf.writestr(path, content)\n            for path, _, files in os.walk(bundle_dir):\n                for filename in files:\n                    fullpath = os.path.join(path, filename)\n                    relpath = os.path.relpath(fullpath, bundle_dir)\n                    if filename == \"manifest.json\":\n                        manifest = _patch_artifact_manifest(\n                            fullpath, org, release, project, extra_files\n                        )\n                        zipf.writestr(relpath, manifest)\n                    else:\n                        zipf.write(fullpath, relpath)\n\n        return bundle.getvalue()\n\n    @classmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release_archive(cls, org, release: str, project=None, dist=None):\n        bundle = cls.create_artifact_bundle_zip(org, release, project)\n        file = File.objects.create(name=\"release-artifacts.zip\")\n        file.putfile(ContentFile(bundle))\n        release_obj = Release.objects.get(organization__slug=org, version=release)\n        return update_artifact_index(release_obj, dist, file)\n\n    @classmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_artifact_bundle(\n        cls,\n        org,\n        bundle_id=None,\n        artifact_count=0,\n        fixture_path=\"artifact_bundle_debug_ids\",\n        date_uploaded=None,\n        date_last_modified=None,\n    ):\n        if date_uploaded is None:\n            date_uploaded = timezone.now()\n\n        bundle = cls.create_artifact_bundle_zip(org.slug, fixture_path=fixture_path)\n        file_ = File.objects.create(name=\"artifact-bundle.zip\")\n        file_.putfile(ContentFile(bundle))\n        # The 'artifact_count' should correspond to the 'bundle' contents but for the purpose of tests we can also\n        # mock it with an arbitrary value.\n        artifact_bundle = ArtifactBundle.objects.create(\n            organization_id=org.id,\n            bundle_id=bundle_id or uuid4(),\n            file=file_,\n            artifact_count=artifact_count,\n            date_uploaded=date_uploaded,\n            date_last_modified=date_last_modified,\n        )\n        return artifact_bundle\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_code_mapping(project, repo=None, organization_integration=None, **kwargs):\n        kwargs.setdefault(\"stack_root\", \"\")\n        kwargs.setdefault(\"source_root\", \"\")\n        kwargs.setdefault(\"default_branch\", \"master\")\n\n        if not repo:\n            repo = Factories.create_repo(project=project)\n        return RepositoryProjectPathConfig.objects.create(\n            project=project,\n            repository=repo,\n            organization_integration_id=organization_integration.id,\n            integration_id=organization_integration.integration_id,\n            organization_id=organization_integration.organization_id,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_repo(\n        project, name=None, provider=None, integration_id=None, url=None, external_id=None\n    ):\n        repo, _ = Repository.objects.get_or_create(\n            organization_id=project.organization_id,\n            name=name\n            or \"{}-{}\".format(petname.generate(2, \"\", letters=10), random.randint(1000, 9999)),\n            provider=provider,\n            integration_id=integration_id,\n            url=url,\n            external_id=external_id,\n        )\n        return repo\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit(\n        repo, project=None, author=None, release=None, message=None, key=None, date_added=None\n    ):\n        commit = Commit.objects.get_or_create(\n            organization_id=repo.organization_id,\n            repository_id=repo.id,\n            key=key or sha1(uuid4().hex.encode(\"utf-8\")).hexdigest(),\n            defaults={\n                \"message\": message or make_sentence(),\n                \"author\": author\n                or Factories.create_commit_author(organization_id=repo.organization_id),\n                \"date_added\": date_added or timezone.now(),\n            },\n        )[0]\n\n        if release:\n            assert project\n            ReleaseCommit.objects.create(\n                organization_id=repo.organization_id,\n                project_id=project.id,\n                release=release,\n                commit=commit,\n                order=1,\n            )\n\n        Factories.create_commit_file_change(commit=commit, filename=\"/models/foo.py\")\n        Factories.create_commit_file_change(commit=commit, filename=\"/worsematch/foo.py\")\n        Factories.create_commit_file_change(commit=commit, filename=\"/models/other.py\")\n\n        return commit\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit_author(organization_id=None, project=None, user=None, email=None):\n        if email:\n            user_email = email\n        else:\n            user_email = user.email if user else f\"{make_word()}@example.com\"\n        return CommitAuthor.objects.get_or_create(\n            organization_id=organization_id or project.organization_id,\n            email=user_email,\n            defaults={\"name\": user.name if user else make_word()},\n        )[0]\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit_file_change(commit, filename):\n        return CommitFileChange.objects.get_or_create(\n            organization_id=commit.organization_id, commit=commit, filename=filename, type=\"M\"\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user(\n        email=None, is_superuser=False, is_staff=False, is_active=True, **kwargs\n    ) -> User:\n        if email is None:\n            email = uuid4().hex + \"@example.com\"\n\n        kwargs.setdefault(\"username\", email)\n\n        user = User(\n            email=email, is_superuser=is_superuser, is_staff=is_staff, is_active=is_active, **kwargs\n        )\n        if kwargs.get(\"password\") is None:\n            user.set_password(\"admin\")\n        user.save()\n\n        # UserEmail is created by a signal\n        assert UserEmail.objects.filter(user=user, email=email).update(is_verified=True)\n\n        return user\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_useremail(user, email=None, **kwargs):\n        if not email:\n            email = uuid4().hex + \"@example.com\"\n\n        kwargs.setdefault(\"is_verified\", True)\n\n        useremail = UserEmail(user=user, email=email, **kwargs)\n        useremail.save()\n\n        return useremail\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_avatar(*args, **kwargs):\n        return UserAvatar.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_role(*args, **kwargs):\n        return UserRole.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_usersocialauth(\n        user: User,\n        provider: str | None = None,\n        uid: str | None = None,\n        extra_data: dict[str, Any] | None = None,\n    ):\n        if not provider:\n            provider = \"asana\"\n        if not uid:\n            uid = \"abc-123\"\n        usa = UserSocialAuth(user=user, provider=provider, uid=uid, extra_data=extra_data)\n        usa.save()\n        return usa\n\n    @staticmethod\n    def inject_performance_problems(jobs, _):\n        for job in jobs:\n            job[\"performance_problems\"] = []\n            for f in job[\"data\"][\"fingerprint\"]:\n                f_data = f.split(\"-\", 1)\n                if len(f_data) < 2:\n                    raise ValueError(\n                        \"Invalid performance fingerprint data. Format must be 'group_type-fingerprint'.\"\n                    )\n                group_type = get_group_type_by_type_id(int(f_data[0]))\n                perf_fingerprint = f_data[1]\n\n                job[\"performance_problems\"].append(\n                    PerformanceProblem(\n                        fingerprint=perf_fingerprint,\n                        op=\"db\",\n                        desc=\"\",\n                        type=group_type,\n                        parent_span_ids=None,\n                        cause_span_ids=None,\n                        offender_span_ids=[],\n                        evidence_data={},\n                        evidence_display=[],\n                    )\n                )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def store_event(\n        data,\n        project_id: int,\n        assert_no_errors: bool = True,\n        default_event_type: EventType | None = None,\n        sent_at: datetime | None = None,\n    ) -> Event:\n        \"\"\"\n        Like `create_event`, but closer to how events are actually\n        ingested. Prefer to use this method over `create_event`\n        \"\"\"\n\n        # this creates a basic message event\n        if default_event_type == EventType.DEFAULT:\n            data.update({\"stacktrace\": copy.deepcopy(DEFAULT_EVENT_DATA[\"stacktrace\"])})\n\n        # this creates an error event\n        elif default_event_type == EventType.ERROR:\n            data.update({\"exception\": [{\"value\": \"BadError\"}]})\n\n        manager = EventManager(data, sent_at=sent_at)\n        manager.normalize()\n        if assert_no_errors:\n            errors = manager.get_data().get(\"errors\")\n            assert not errors, errors\n\n        normalized_data = manager.get_data()\n\n        _set_sample_rate_from_error_sampling(normalized_data)\n\n        event = None\n\n        # When fingerprint is present on transaction, inject performance problems\n        if (\n            normalized_data.get(\"type\") == \"transaction\"\n            and normalized_data.get(\"fingerprint\") is not None\n        ):\n            with mock.patch(\n                \"sentry.event_manager._detect_performance_problems\",\n                Factories.inject_performance_problems,\n            ):\n                event = manager.save(project_id)\n\n        else:\n            event = manager.save(project_id)\n\n        if event.groups:\n            for group in event.groups:\n                group.save()\n\n        if event.group:\n            event.group.save()\n\n        return event\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_group(project, create_open_period=True, **kwargs):\n        from sentry.models.group import GroupStatus\n        from sentry.models.groupopenperiod import GroupOpenPeriod\n        from sentry.testutils.helpers.datetime import before_now\n        from sentry.types.group import GroupSubStatus\n\n        kwargs.setdefault(\"message\", \"Hello world\")\n        kwargs.setdefault(\"data\", {})\n        if \"type\" not in kwargs[\"data\"]:\n            kwargs[\"data\"].update({\"type\": \"default\", \"metadata\": {\"title\": kwargs[\"message\"]}})\n        if \"short_id\" not in kwargs:\n            kwargs[\"short_id\"] = project.next_short_id()\n        if \"metadata\" in kwargs:\n            metadata = kwargs.pop(\"metadata\")\n            kwargs[\"data\"].setdefault(\"metadata\", {}).update(metadata)\n        if \"status\" not in kwargs:\n            kwargs[\"status\"] = GroupStatus.UNRESOLVED\n            kwargs[\"substatus\"] = GroupSubStatus.NEW\n\n        group = Group.objects.create(project=project, **kwargs)\n        if create_open_period:\n            open_period = GroupOpenPeriod.objects.create(\n                group=group,\n                project=project,\n                date_started=group.first_seen or before_now(minutes=5),\n            )\n            if group.status == GroupStatus.RESOLVED:\n                open_period.update(\n                    date_ended=group.resolved_at if group.resolved_at else timezone.now()\n                )\n\n        return group\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_file(**kwargs):\n        return File.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_file_from_path(path, name=None, **kwargs):\n        if name is None:\n            name = os.path.basename(path)\n\n        file = Factories.create_file(name=name, **kwargs)\n        with open(path) as f:\n            file.putfile(f)\n        return file\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dif_file(\n        project,\n        debug_id=None,\n        object_name=None,\n        features=None,\n        data=None,\n        file=None,\n        cpu_name=None,\n        code_id=None,\n        **kwargs,\n    ):\n        if debug_id is None:\n            debug_id = str(uuid4())\n\n        if object_name is None:\n            object_name = \"%s.dSYM\" % debug_id\n\n        if features is not None:\n            if data is None:\n                data = {}\n            data[\"features\"] = features\n\n        if file is None:\n            file = Factories.create_file(\n                name=object_name,\n                size=42,\n                headers={\"Content-Type\": \"application/x-mach-binary\"},\n                checksum=\"dc1e3f3e411979d336c3057cce64294f3420f93a\",\n            )\n\n        return ProjectDebugFile.objects.create(\n            debug_id=debug_id,\n            code_id=code_id,\n            project_id=project.id,\n            object_name=object_name,\n            cpu_name=cpu_name or \"x86_64\",\n            file=file,\n            checksum=file.checksum,\n            data=data,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dif_from_path(path, object_name=None, **kwargs):\n        if object_name is None:\n            object_name = os.path.basename(path)\n\n        headers = {\"Content-Type\": \"application/x-mach-binary\"}\n        file = Factories.create_file_from_path(path, name=object_name, headers=headers)\n        return Factories.create_dif_file(file=file, object_name=object_name, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def add_user_permission(user, permission):\n        UserPermission.objects.create(user=user, permission=permission)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app(**kwargs):\n        published = kwargs.pop(\"published\", False)\n        args = Factories._sentry_app_kwargs(**kwargs)\n        user = args.pop(\"user\", None)\n        app = SentryAppCreator(is_internal=False, **args).run(user=user, request=None)\n\n        if published:\n            app.update(status=SentryAppStatus.PUBLISHED)\n\n        return app\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_avatar(*args, **kwargs):\n        return SentryAppAvatar.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_internal_integration(**kwargs) -> SentryApp:\n        args = Factories._sentry_app_kwargs(**kwargs)\n        args[\"verify_install\"] = False\n        user = args.pop(\"user\", None)\n        app = SentryAppCreator(is_internal=True, **args).run(\n            user=user, request=None, skip_default_auth_token=True\n        )\n        return app\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_internal_integration_token(\n        user,\n        internal_integration: SentryApp | None = None,\n        install: SentryAppInstallation | None = None,\n        request=None,\n    ) -> ApiToken:\n        if internal_integration and install:\n            raise ValueError(\"Only one of internal_integration or install arg can be provided\")\n        elif internal_integration is None and install is None:\n            raise ValueError(\"Must pass in either internal_integration or install arg\")\n\n        if internal_integration is not None and install is None:\n            # Fetch install from provided or created internal integration\n            with assume_test_silo_mode(SiloMode.CONTROL):\n                install = SentryAppInstallation.objects.get(\n                    sentry_app=internal_integration.id,\n                    organization_id=internal_integration.owner_id,\n                )\n        elif install is None:\n            raise AssertionError(\"unreachable\")\n\n        return SentryAppInstallationTokenCreator(sentry_app_installation=install).run(\n            user=user, request=request\n        )\n\n    @staticmethod\n    def _sentry_app_kwargs(**kwargs):\n        _kwargs = {\n            \"user\": kwargs.get(\"user\", Factories.create_user()),\n            \"name\": kwargs.get(\"name\", petname.generate(2, \" \", letters=10).title()),\n            \"organization_id\": kwargs.get(\n                \"organization_id\", kwargs.pop(\"organization\", Factories.create_organization()).id\n            ),\n            \"author\": kwargs.get(\"author\", \"A Company\"),\n            \"scopes\": kwargs.get(\"scopes\", ()),\n            \"verify_install\": kwargs.get(\"verify_install\", True),\n            \"webhook_url\": kwargs.get(\"webhook_url\", \"https://example.com/webhook\"),\n            \"events\": [],\n            \"schema\": {},\n        }\n\n        _kwargs.update(**kwargs)\n        return _kwargs\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_sentry_app_installation(\n        organization=None,\n        slug=None,\n        user=None,\n        status=None,\n        prevent_token_exchange=False,\n    ):\n        if not organization:\n            organization = Factories.create_organization()\n\n        Factories.create_project(organization=organization)\n\n        with assume_test_silo_mode(SiloMode.CONTROL):\n            install = SentryAppInstallationCreator(\n                slug=(slug or Factories.create_sentry_app(organization=organization).slug),\n                organization_id=organization.id,\n            ).run(\n                user=(user or Factories.create_user()),\n                request=None,\n            )\n\n            install.status = SentryAppInstallationStatus.INSTALLED if status is None else status\n            install.save()\n            if not prevent_token_exchange and (\n                install.sentry_app.status != SentryAppStatus.INTERNAL\n            ):\n                assert install.api_grant is not None\n                assert install.sentry_app.application is not None\n                assert install.sentry_app.proxy_user is not None\n                GrantExchanger(\n                    install=install,\n                    code=install.api_grant.code,\n                    client_id=install.sentry_app.application.client_id,\n                    user=install.sentry_app.proxy_user,\n                ).run()\n                install = SentryAppInstallation.objects.get(id=install.id)\n        return install\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_installation_for_provider(\n        sentry_app_id: int,\n        organization_id: int,\n        provider: str,\n    ) -> SentryAppInstallationForProvider:\n        installation = SentryAppInstallation.objects.get(\n            sentry_app_id=sentry_app_id, organization_id=organization_id\n        )\n        return SentryAppInstallationForProvider.objects.create(\n            organization_id=organization_id,\n            provider=provider,\n            sentry_app_installation=installation,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_stacktrace_link_schema():\n        return {\"type\": \"stacktrace-link\", \"uri\": \"/redirect/\"}\n\n    @staticmethod\n    def create_issue_link_schema():\n        return {\n            \"type\": \"issue-link\",\n            \"link\": {\n                \"uri\": \"/sentry/issues/link\",\n                \"required_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    }\n                ],\n            },\n            \"create\": {\n                \"uri\": \"/sentry/issues/create\",\n                \"required_fields\": [\n                    {\"type\": \"text\", \"name\": \"title\", \"label\": \"Title\"},\n                    {\"type\": \"text\", \"name\": \"summary\", \"label\": \"Summary\"},\n                ],\n                \"optional_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"points\",\n                        \"label\": \"Points\",\n                        \"options\": [[\"1\", \"1\"], [\"2\", \"2\"], [\"3\", \"3\"], [\"5\", \"5\"], [\"8\", \"8\"]],\n                    },\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    },\n                ],\n            },\n        }\n\n    @staticmethod\n    def create_alert_rule_action_schema():\n        return {\n            \"type\": \"alert-rule-action\",\n            \"title\": \"Create Task with App\",\n            \"settings\": {\n                \"type\": \"alert-rule-settings\",\n                \"uri\": \"/sentry/alert-rule\",\n                \"required_fields\": [\n                    {\"type\": \"text\", \"name\": \"title\", \"label\": \"Title\"},\n                    {\"type\": \"text\", \"name\": \"summary\", \"label\": \"Summary\"},\n                ],\n                \"optional_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"points\",\n                        \"label\": \"Points\",\n                        \"options\": [[\"1\", \"1\"], [\"2\", \"2\"], [\"3\", \"3\"], [\"5\", \"5\"], [\"8\", \"8\"]],\n                    },\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    },\n                ],\n            },\n        }\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_service_hook(\n        actor=None, org=None, project=None, events=None, url=None, project_ids=None, **kwargs\n    ):\n        if project:\n            if project_ids is not None:\n                raise ValueError(\"Cannot provide both project and project_ids\")\n            project_ids = [project.id]\n        if not actor:\n            actor = Factories.create_user()\n        if not org:\n            if project:\n                org = project.organization\n            else:\n                org = Factories.create_organization(owner=actor)\n        if project_ids is None:  # empty list for project_ids is valid and means no project filter\n            project_ids = [Factories.create_project(organization=org).id]\n        if events is None:\n            events = [\"event.created\"]\n        if not url:\n            url = \"https://example.com/sentry/webhook\"\n\n        app_id = kwargs.pop(\"application_id\", None)\n        if app_id is None and \"application\" in kwargs:\n            app_id = kwargs[\"application\"].id\n        installation_id = kwargs.pop(\"installation_id\", None)\n        if installation_id is None and \"installation\" in kwargs:\n            installation_id = kwargs[\"installation\"].id\n        hook_id = hook_service.create_service_hook(\n            application_id=app_id,\n            actor_id=actor.id,\n            installation_id=installation_id,\n            organization_id=org.id,\n            project_ids=project_ids,\n            events=events,\n            url=url,\n        ).id\n        return ServiceHook.objects.get(id=hook_id)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_feature(feature=None, sentry_app=None, description=None):\n        if not sentry_app:\n            sentry_app = Factories.create_sentry_app()\n\n        integration_feature = IntegrationFeature.objects.create(\n            target_id=sentry_app.id,\n            target_type=IntegrationTypes.SENTRY_APP.value,\n            feature=feature or Feature.API,\n        )\n\n        if description:\n            integration_feature.update(user_description=description)\n\n        return integration_feature\n\n    @staticmethod\n    def _doc_integration_kwargs(**kwargs):\n        _kwargs = {\n            \"name\": kwargs.get(\"name\", petname.generate(2, \" \", letters=10).title()),\n            \"author\": kwargs.get(\"author\", \"me\"),\n            \"description\": kwargs.get(\"description\", \"hi im a description\"),\n            \"url\": kwargs.get(\"url\", \"https://sentry.io\"),\n            \"popularity\": kwargs.get(\"popularity\", 1),\n            \"is_draft\": kwargs.get(\"is_draft\", True),\n            \"metadata\": kwargs.get(\"metadata\", {}),\n        }\n        _kwargs[\"slug\"] = slugify(_kwargs[\"name\"])\n        _kwargs.update(**kwargs)\n        return _kwargs\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration(features=None, has_avatar: bool = False, **kwargs) -> DocIntegration:\n        doc = DocIntegration.objects.create(**Factories._doc_integration_kwargs(**kwargs))\n        if features:\n            Factories.create_doc_integration_features(features=features, doc_integration=doc)\n        if has_avatar:\n            Factories.create_doc_integration_avatar(doc_integration=doc)\n        return doc\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration_features(\n        features=None, doc_integration=None\n    ) -> list[IntegrationFeature]:\n        if not features:\n            features = [Feature.API]\n        if not doc_integration:\n            doc_integration = Factories.create_doc_integration()\n        return IntegrationFeature.objects.bulk_create(\n            [\n                IntegrationFeature(\n                    target_id=doc_integration.id,\n                    target_type=IntegrationTypes.DOC_INTEGRATION.value,\n                    feature=feature,\n                )\n                for feature in features\n            ]\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration_avatar(doc_integration=None, **kwargs) -> DocIntegrationAvatar:\n        if not doc_integration:\n            doc_integration = Factories.create_doc_integration()\n        photo = ControlFile.objects.create(name=\"test.png\", type=\"avatar.file\")\n        photo.putfile(io.BytesIO(b\"imaginethiswasphotobytes\"))\n\n        return DocIntegrationAvatar.objects.create(\n            doc_integration=doc_integration, avatar_type=0, control_file_id=photo.id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_userreport(\n        project: Project, event_id: str | None = None, **kwargs: Any\n    ) -> UserReport:\n        event = Factories.store_event(\n            data={\n                \"timestamp\": datetime.now(UTC).isoformat(),\n                \"event_id\": event_id or \"a\" * 32,\n                \"message\": \"testing\",\n            },\n            project_id=project.id,\n        )\n        assert event.group is not None\n\n        return UserReport.objects.create(\n            group_id=event.group.id,\n            event_id=event.event_id,\n            project_id=project.id,\n            name=\"Jane Bloggs\",\n            email=\"jane@example.com\",\n            comments=\"the application crashed\",\n            **kwargs,\n        )\n\n    @staticmethod\n    def create_session():\n        engine = import_module(settings.SESSION_ENGINE)\n\n        session = engine.SessionStore()\n        session.save()\n        return session\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_platform_external_issue(\n        group=None, service_type=None, display_name=None, web_url=None\n    ):\n        return PlatformExternalIssue.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            service_type=service_type,\n            display_name=display_name,\n            web_url=web_url,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_integration_external_issue(group=None, integration=None, key=None, **kwargs):\n        external_issue = ExternalIssue.objects.create(\n            organization_id=group.organization.id, integration_id=integration.id, key=key, **kwargs\n        )\n\n        GroupLink.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            linked_type=GroupLink.LinkedType.issue,\n            linked_id=external_issue.id,\n            relationship=GroupLink.Relationship.references,\n        )\n\n        return external_issue\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_integration_external_project(\n        organization_id: int, integration_id: int, *args: Any, **kwargs: Any\n    ) -> IntegrationExternalProject:\n        oi = OrganizationIntegration.objects.get(\n            organization_id=organization_id, integration_id=integration_id\n        )\n        return IntegrationExternalProject.objects.create(\n            organization_integration_id=oi.id, *args, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident(\n        organization,\n        projects,\n        detection_uuid=None,\n        status=1,\n        title=None,\n        query=\"test query\",\n        date_started=None,\n        date_detected=None,\n        date_closed=None,\n        alert_rule=None,\n        subscription=None,\n    ):\n        if not title:\n            title = petname.generate(2, \" \", letters=10).title()\n        if alert_rule is None:\n            alert_rule = Factories.create_alert_rule(\n                organization, projects, query=query, time_window=1\n            )\n\n        incident = Incident.objects.create(\n            organization=organization,\n            detection_uuid=detection_uuid,\n            status=status,\n            title=title,\n            alert_rule=alert_rule,\n            date_started=date_started or timezone.now(),\n            date_detected=date_detected or timezone.now(),\n            date_closed=timezone.now() if date_closed is not None else date_closed,\n            type=IncidentType.ALERT_TRIGGERED.value,\n            subscription=subscription,\n        )\n        for project in projects:\n            IncidentProject.objects.create(incident=incident, project=project)\n\n        return incident\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_activity(incident, type, comment=None, user_id=None):\n        return IncidentActivity.objects.create(\n            incident=incident, type=type, comment=comment, user_id=user_id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule(\n        organization,\n        projects,\n        name=None,\n        owner=None,\n        query=\"level:error\",\n        aggregate=\"count()\",\n        time_window=10,\n        threshold_period=1,\n        environment=None,\n        date_added=None,\n        query_type=None,\n        dataset=Dataset.Events,\n        threshold_type=AlertRuleThresholdType.ABOVE,\n        resolve_threshold=None,\n        user=None,\n        event_types=None,\n        comparison_delta=None,\n        description=None,\n        sensitivity=None,\n        seasonality=None,\n        detection_type=AlertRuleDetectionType.STATIC,\n    ):\n        if not name:\n            name = petname.generate(2, \" \", letters=10).title()\n\n        if query_type is None:\n            query_type = query_datasets_to_type[dataset]\n\n        alert_rule = create_alert_rule(\n            organization,\n            projects,\n            name,\n            query,\n            aggregate,\n            time_window,\n            threshold_type,\n            threshold_period,\n            owner=owner,\n            resolve_threshold=resolve_threshold,\n            query_type=query_type,\n            dataset=dataset,\n            environment=environment,\n            user=user,\n            event_types=event_types,\n            comparison_delta=comparison_delta,\n            description=description,\n            sensitivity=sensitivity,\n            seasonality=seasonality,\n            detection_type=detection_type,\n        )\n\n        if date_added is not None:\n            alert_rule.update(date_added=date_added)\n\n        return alert_rule\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_trigger(alert_rule, label=None, alert_threshold=100):\n        if not label:\n            label = petname.generate(2, \" \", letters=10).title()\n\n        return create_alert_rule_trigger(alert_rule, label, alert_threshold)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_trigger(incident, alert_rule_trigger, status=None):\n        if status is None:\n            status = TriggerStatus.ACTIVE.value\n\n        return IncidentTrigger.objects.create(\n            alert_rule_trigger=alert_rule_trigger, incident=incident, status=status\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_trigger_action(\n        trigger,\n        type=AlertRuleTriggerAction.Type.EMAIL,\n        target_type=AlertRuleTriggerAction.TargetType.USER,\n        target_identifier=None,\n        integration=None,\n        sentry_app=None,\n        sentry_app_config=None,\n    ):\n        return create_alert_rule_trigger_action(\n            trigger,\n            type,\n            target_type,\n            target_identifier,\n            integration.id if integration else None,\n            sentry_app.id if sentry_app else None,\n            sentry_app_config=sentry_app_config,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_external_user(user: User, **kwargs: Any) -> ExternalActor:\n        kwargs.setdefault(\"provider\", ExternalProviders.GITHUB.value)\n        kwargs.setdefault(\"external_name\", \"\")\n\n        return ExternalActor.objects.create(user_id=user.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_external_team(team: Team, **kwargs: Any) -> ExternalActor:\n        kwargs.setdefault(\"provider\", ExternalProviders.GITHUB.value)\n        kwargs.setdefault(\"external_name\", \"@getsentry/ecosystem\")\n\n        return ExternalActor.objects.create(team_id=team.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_codeowners(project, code_mapping, **kwargs):\n        kwargs.setdefault(\"raw\", \"\")\n\n        return ProjectCodeOwners.objects.create(\n            project=project, repository_project_path_config=code_mapping, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_slack_integration(\n        organization: Organization, external_id: str, **kwargs: Any\n    ) -> Integration:\n        integration = Integration.objects.create(\n            provider=\"slack\",\n            name=\"Team A\",\n            external_id=external_id,\n            metadata={\n                \"access_token\": \"xoxp-xxxxxxxxx-xxxxxxxxxx-xxxxxxxxxxxx\",\n                \"installation_type\": \"born_as_bot\",\n            },\n        )\n        integration.add_organization(organization)\n        return integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_integration(\n        organization: Organization,\n        external_id: str,\n        oi_params: Mapping[str, Any] | None = None,\n        **integration_params: Any,\n    ) -> Integration:\n        integration = Integration.objects.create(external_id=external_id, **integration_params)\n        with outbox_runner():\n            organization_integration = integration.add_organization(organization)\n            assert organization_integration is not None\n        organization_integration.update(**(oi_params or {}))\n\n        return integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_provider_integration(**integration_params: Any) -> Integration:\n        return Integration.objects.create(**integration_params)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_provider_integration_for(\n        organization: Organization | RpcOrganization,\n        user: User | RpcUser | None,\n        **integration_params: Any,\n    ) -> tuple[Integration, OrganizationIntegration]:\n        integration = Integration.objects.create(**integration_params)\n        org_integration = integration.add_organization(organization, user)\n        assert org_integration is not None\n        return integration, org_integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity_integration(\n        user: User | RpcUser,\n        organization: Organization | RpcOrganization,\n        integration_params: Mapping[Any, Any],\n        identity_params: Mapping[Any, Any],\n    ) -> tuple[Integration, OrganizationIntegration, Identity, IdentityProvider]:\n        # Avoid common pitfalls in tests\n        assert \"provider\" in integration_params\n        assert \"external_id\" in integration_params\n        assert \"external_id\" in identity_params\n\n        integration = Factories.create_provider_integration(**integration_params)\n        identity_provider = Factories.create_identity_provider(integration=integration)\n        identity = Factories.create_identity(\n            user=user, identity_provider=identity_provider, **identity_params\n        )\n        organization_integration = integration.add_organization(\n            organization_id=organization.id, user=user, default_auth_id=identity.id\n        )\n        assert organization_integration is not None\n        return integration, organization_integration, identity, identity_provider\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_organization_integration(**integration_params: Any) -> OrganizationIntegration:\n        return OrganizationIntegration.objects.create(**integration_params)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity_provider(\n        integration: Integration | None = None,\n        config: dict[str, Any] | None = None,\n        **kwargs: Any,\n    ) -> IdentityProvider:\n        if integration is not None:\n            integration_values = dict(\n                type=integration.provider,\n                external_id=integration.external_id,\n            )\n            if any((key in kwargs) for key in integration_values):\n                raise ValueError(\n                    \"Values from integration should not be in kwargs: \"\n                    + repr(list(integration_values.keys()))\n                )\n            kwargs.update(integration_values)\n\n        return IdentityProvider.objects.create(config=config or {}, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity(\n        user: User | RpcUser, identity_provider: IdentityProvider, external_id: str, **kwargs: Any\n    ) -> Identity:\n        return Identity.objects.create(\n            external_id=external_id,\n            idp=identity_provider,\n            user_id=user.id,\n            status=IdentityStatus.VALID,\n            scopes=[],\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_group_history(\n        group: Group,\n        status: int,\n        release: Release | None = None,\n        user_id: int | None = None,\n        team_id: int | None = None,\n        prev_history: GroupHistory | None = None,\n        date_added: datetime | None = None,\n    ) -> GroupHistory:\n        prev_history_date = None\n        if prev_history:\n            prev_history_date = prev_history.date_added\n\n        kwargs = {}\n        if date_added:\n            kwargs[\"date_added\"] = date_added\n        return GroupHistory.objects.create(\n            organization=group.organization,\n            group=group,\n            project=group.project,\n            release=release,\n            user_id=user_id,\n            team_id=team_id,\n            status=status,\n            prev_history=prev_history,\n            prev_history_date=prev_history_date,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_comment(issue, project, user, text=\"hello world\"):\n        data = {\"text\": text}\n        return Activity.objects.create(\n            project=project,\n            group=issue,\n            type=ActivityType.NOTE.value,\n            user_id=user.id,\n            data=data,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_saved_search(name: str, **kwargs):\n        if \"owner\" in kwargs:\n            owner = kwargs.pop(\"owner\")\n            kwargs[\"owner_id\"] = owner.id if not isinstance(owner, int) else owner\n        return SavedSearch.objects.create(name=name, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_notification_action(\n        organization: Organization | None = None,\n        projects: list[Project] | None = None,\n        **kwargs,\n    ):\n        if not organization:\n            organization = Factories.create_organization()\n\n        if not projects:\n            projects = []\n\n        action_kwargs = {\n            \"organization\": organization,\n            \"type\": ActionService.SENTRY_NOTIFICATION,\n            \"target_type\": ActionTarget.USER,\n            \"target_identifier\": \"1\",\n            \"target_display\": \"Sentry User\",\n            \"trigger_type\": ActionTrigger.AUDIT_LOG,\n            **kwargs,\n        }\n\n        action = NotificationAction.objects.create(**action_kwargs)\n        action.projects.add(*projects)\n        action.save()\n\n        return action\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_notification_settings_provider(*args, **kwargs) -> NotificationSettingProvider:\n        return NotificationSettingProvider.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_option(*args, **kwargs) -> UserOption:\n        return UserOption.objects.create(*args, **kwargs)\n\n    @staticmethod\n    def create_basic_auth_header(username: str, password: str = \"\") -> bytes:\n        return b\"Basic \" + b64encode(f\"{username}:{password}\".encode())\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def snooze_rule(**kwargs):\n        return RuleSnooze.objects.create(**kwargs)\n\n    @staticmethod\n    def create_request_access(\n        sso_state: RpcMemberSsoState | None = None,\n        permissions: list | None = None,\n        org_context: RpcUserOrganizationContext | None = None,\n        scopes_upper_bound: frozenset | None = frozenset(),\n    ) -> RpcBackedAccess:\n        if not sso_state:\n            sso_state = RpcMemberSsoState()\n        if not permissions:\n            permissions = []\n        if not org_context:\n            org_context = RpcUserOrganizationContext()\n\n        auth_state = RpcAuthState(sso_state=sso_state, permissions=permissions)\n        return RpcBackedAccess(\n            rpc_user_organization_context=org_context,\n            auth_state=auth_state,\n            scopes_upper_bound=scopes_upper_bound,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_webhook_payload(mailbox_name: str, region_name: str, **kwargs) -> WebhookPayload:\n        payload_kwargs = {\n            \"request_method\": \"POST\",\n            \"request_path\": \"/extensions/github/webhook/\",\n            \"request_headers\": '{\"Content-Type\": \"application/json\"}',\n            \"request_body\": \"{}\",\n            **kwargs,\n        }\n        return WebhookPayload.objects.create(\n            mailbox_name=mailbox_name, region_name=region_name, **payload_kwargs\n        )\n\n    @staticmethod\n    def create_uptime_subscription(\n        type: str,\n        subscription_id: str | None,\n        status: UptimeSubscription.Status,\n        url: str | None,\n        url_domain: str,\n        url_domain_suffix: str,\n        host_provider_id: str,\n        host_provider_name: str,\n        interval_seconds: IntervalSecondsLiteral,\n        timeout_ms: int,\n        method,\n        headers,\n        body,\n        date_updated: datetime,\n        uptime_status: UptimeStatus,\n        uptime_status_update_date: datetime,\n        trace_sampling: bool = False,\n    ):\n        if url is None:\n            url = petname.generate().title()\n            url = f\"http://{url}.com\"\n\n        return UptimeSubscription.objects.create(\n            type=type,\n            subscription_id=subscription_id,\n            status=status.value,\n            url=url,\n            url_domain=url_domain,\n            url_domain_suffix=url_domain_suffix,\n            host_provider_id=host_provider_id,\n            host_provider_name=host_provider_name,\n            interval_seconds=interval_seconds,\n            timeout_ms=timeout_ms,\n            date_updated=date_updated,\n            method=method,\n            headers=headers,\n            body=body,\n            trace_sampling=trace_sampling,\n            uptime_status=uptime_status,\n            uptime_status_update_date=uptime_status_update_date,\n        )\n\n    @staticmethod\n    def create_project_uptime_subscription(\n        project: Project,\n        env: Environment | None,\n        uptime_subscription: UptimeSubscription,\n        status: int,\n        mode: UptimeMonitorMode,\n        name: str | None,\n        owner: Actor | None,\n        id: int | None,\n    ):\n        if name is None:\n            name = petname.generate().title()\n        owner_team_id = None\n        owner_user_id = None\n        if owner:\n            if owner.is_team:\n                owner_team_id = owner.id\n            elif owner.is_user:\n                owner_user_id = owner.id\n\n        return ProjectUptimeSubscription.objects.create(\n            uptime_subscription=uptime_subscription,\n            project=project,\n            environment=env,\n            status=status,\n            mode=mode,\n            name=name,\n            owner_team_id=owner_team_id,\n            owner_user_id=owner_user_id,\n            pk=id,\n        )\n\n    @staticmethod\n    def create_uptime_subscription_region(\n        subscription: UptimeSubscription,\n        region_slug: str,\n        mode: UptimeSubscriptionRegion.RegionMode,\n    ) -> UptimeSubscriptionRegion:\n        return UptimeSubscriptionRegion.objects.create(\n            uptime_subscription=subscription,\n            region_slug=region_slug,\n            mode=mode,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard(\n        organization: Organization | None = None,\n        title: str | None = None,\n        created_by: User | None = None,\n        **kwargs,\n    ):\n        if organization is None:\n            organization = Factories.create_organization()\n        if created_by is None:\n            created_by = Factories.create_user()\n            Factories.create_member(organization=organization, user=created_by, role=\"owner\")\n        if title is None:\n            title = petname.generate(2, \" \", letters=10).title()\n        return Dashboard.objects.create(\n            organization=organization, title=title, created_by_id=created_by.id, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard_widget(\n        order: int,\n        dashboard: Dashboard | None = None,\n        title: str | None = None,\n        display_type: int | None = None,\n        **kwargs,\n    ):\n        if dashboard is None:\n            dashboard = Factories.create_dashboard()\n        if display_type is None:\n            display_type = DashboardWidgetDisplayTypes.AREA_CHART\n        if title is None:\n            title = petname.generate(2, \" \", letters=10).title()\n\n        return DashboardWidget.objects.create(\n            dashboard=dashboard, title=title, display_type=display_type, order=order, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard_widget_query(\n        order: int,\n        widget: DashboardWidget | None = None,\n        name: str | None = None,\n        **kwargs,\n    ):\n        if widget is None:\n            widget = Factories.create_dashboard_widget(order=order)\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        return DashboardWidgetQuery.objects.create(widget=widget, name=name, order=order, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_workflow(\n        name: str | None = None,\n        organization: Organization | None = None,\n        config: dict[str, Any] | None = None,\n        **kwargs,\n    ) -> Workflow:\n        if organization is None:\n            organization = Factories.create_organization()\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        if config is None:\n            config = {}\n        return Workflow.objects.create(\n            organization=organization, name=name, config=config, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition_group(\n        organization: Organization | None = None,\n        **kwargs,\n    ) -> DataConditionGroup:\n        if organization is None:\n            organization = Factories.create_organization()\n        return DataConditionGroup.objects.create(organization=organization, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_workflow_data_condition_group(\n        workflow: Workflow | None = None,\n        condition_group: DataConditionGroup | None = None,\n        **kwargs,\n    ) -> WorkflowDataConditionGroup:\n        if workflow is None:\n            workflow = Factories.create_workflow()\n\n        if not condition_group:\n            condition_group = Factories.create_data_condition_group()\n\n        return WorkflowDataConditionGroup.objects.create(\n            workflow=workflow, condition_group=condition_group\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition(\n        condition_group: DataConditionGroup | None = None, **kwargs\n    ) -> DataCondition:\n        if condition_group is None:\n            condition_group = Factories.create_data_condition_group()\n        return DataCondition.objects.create(condition_group=condition_group, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_source(\n        organization: Organization | None = None,\n        source_id: str | None = None,\n        type: str | None = None,\n        **kwargs,\n    ) -> DataSource:\n        if organization is None:\n            organization = Factories.create_organization()\n        if source_id is None:\n            source_id = str(random.randint(1, 10000))\n        if type is None:\n            type = data_source_type_registry.get_key(QuerySubscriptionDataSourceHandler)\n        return DataSource.objects.create(organization=organization, source_id=source_id, type=type)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector(\n        name: str | None = None,\n        config: dict | None = None,\n        **kwargs,\n    ) -> Detector:\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        if config is None:\n            config = default_detector_config_data.get(kwargs[\"type\"], {})\n\n        return Detector.objects.create(\n            name=name,\n            config=config,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector_state(\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> DetectorState:\n        if detector is None:\n            detector = Factories.create_detector()\n\n        return DetectorState.objects.create(detector=detector, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_source_detector(\n        data_source: DataSource | None = None,\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> DataSourceDetector:\n        if data_source is None:\n            data_source = Factories.create_data_source()\n        if detector is None:\n            detector = Factories.create_detector()\n        return DataSourceDetector.objects.create(data_source=data_source, detector=detector)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_action(\n        config: dict[str, Any] | None = None,\n        type: Action.Type | None = None,\n        data: dict[str, Any] | None = None,\n        **kwargs,\n    ) -> Action:\n        if config is None and type is None and data is None:\n            # Default to a slack action with nice defaults so someone can just do\n            # self.create_action() and have a sane default\n            config = {\n                \"target_identifier\": \"1\",\n                \"target_display\": \"Sentry User\",\n                \"target_type\": ActionTarget.SPECIFIC,\n            }\n\n            data = {\"notes\": \"bufos are great\", \"tags\": \"bufo-bot\"}\n\n        if config is None:\n            config = {}\n\n        if data is None:\n            data = {}\n\n        if type is None:\n            type = Action.Type.SLACK\n\n        return Action.objects.create(type=type, config=config, data=data, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector_workflow(\n        detector: Detector | None = None,\n        workflow: Workflow | None = None,\n        **kwargs,\n    ) -> DetectorWorkflow:\n        if detector is None:\n            detector = Factories.create_detector()\n        if workflow is None:\n            workflow = Factories.create_workflow()\n        return DetectorWorkflow.objects.create(detector=detector, workflow=workflow, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_workflow(\n        alert_rule_id: int | None = None,\n        rule_id: int | None = None,\n        workflow: Workflow | None = None,\n        **kwargs,\n    ) -> AlertRuleWorkflow:\n        if rule_id is None and alert_rule_id is None:\n            raise ValueError(\"Either rule_id or alert_rule_id must be provided\")\n\n        if rule_id is not None and alert_rule_id is not None:\n            raise ValueError(\"Only one of rule_id or alert_rule_id can be provided\")\n\n        if workflow is None:\n            workflow = Factories.create_workflow()\n\n        return AlertRuleWorkflow.objects.create(\n            alert_rule_id=alert_rule_id, rule_id=rule_id, workflow=workflow, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_group_open_period(\n        incident: Incident,\n        group_open_period: GroupOpenPeriod,\n        **kwargs,\n    ) -> IncidentGroupOpenPeriod:\n        return IncidentGroupOpenPeriod.objects.create(\n            incident_id=incident.id,\n            incident_identifier=incident.identifier,\n            group_open_period=group_open_period,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_detector(\n        alert_rule_id: int | None = None,\n        rule_id: int | None = None,\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> AlertRuleDetector:\n        if rule_id is None and alert_rule_id is None:\n            raise ValueError(\"Either rule_id or alert_rule_id must be provided\")\n\n        if rule_id is not None and alert_rule_id is not None:\n            raise ValueError(\"Only one of rule_id or alert_rule_id can be provided\")\n\n        if detector is None:\n            detector = Factories.create_detector()\n\n        return AlertRuleDetector.objects.create(\n            alert_rule_id=alert_rule_id, rule_id=rule_id, detector=detector, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_action_alert_rule_trigger_action(\n        alert_rule_trigger_action_id: int,\n        action: Action | None = None,\n        **kwargs,\n    ) -> ActionAlertRuleTriggerAction:\n        if action is None:\n            action = Factories.create_action()\n\n        return ActionAlertRuleTriggerAction.objects.create(\n            action=action, alert_rule_trigger_action_id=alert_rule_trigger_action_id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition_group_action(\n        action: Action | None = None,\n        condition_group: DataConditionGroup | None = None,\n        **kwargs,\n    ) -> DataConditionGroupAction:\n        if action is None:\n            action = Factories.create_action()\n        if condition_group is None:\n            condition_group = Factories.create_data_condition_group()\n        return DataConditionGroupAction.objects.create(\n            action=action, condition_group=condition_group, **kwargs\n        )\n",
    "file_path": "/Users/wangyue/Code/CodeReviewData/ReviewDataset/sentry-greptile/src/sentry/testutils/factories.py",
    "line_count": 2375,
    "encoding": "utf-8",
    "error": null
}

Message 7 [Assistant]:
--------------------------------------------------------------------------------
Content:
让我查看 `store_event` 函数的完整实现，以了解 `normalized_data` 的来源：
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "def store_event",
      "is_regex": false,
      "context_lines": 50
    },
    "id": "call_00_EuFngkbhX8e9R1HGSuY7b5p5",
    "type": "tool_call"
  }
]

Message 8 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_EuFngkbhX8e9R1HGSuY7b5p5
Result:
{
    "matches": [
        {
            "file": "tests/snuba/search/test_backend.py",
            "line_number": 97,
            "matched_line": "    def store_event(self, data, *args, **kwargs):",
            "context_start_line": 47,
            "context_end_line": 147,
            "context": [
                "47: ",
                "48: class SharedSnubaMixin(SnubaTestCase):",
                "49:     @property",
                "50:     def backend(self) -> SnubaSearchBackendBase:",
                "51:         raise NotImplementedError(self)",
                "52: ",
                "53:     def build_search_filter(self, query, projects=None, user=None, environments=None):",
                "54:         user = user if user is not None else self.user",
                "55:         projects = projects if projects is not None else [self.project]",
                "56:         return convert_query_values(parse_search_query(query), projects, user, environments)",
                "57: ",
                "58:     def make_query(",
                "59:         self,",
                "60:         projects=None,",
                "61:         search_filter_query=None,",
                "62:         user=None,",
                "63:         environments=None,",
                "64:         sort_by=\"date\",",
                "65:         limit=None,",
                "66:         count_hits=False,",
                "67:         date_from=None,",
                "68:         date_to=None,",
                "69:         cursor=None,",
                "70:         aggregate_kwargs=None,",
                "71:     ):",
                "72:         search_filters = []",
                "73:         projects = projects if projects is not None else [self.project]",
                "74:         if search_filter_query is not None:",
                "75:             search_filters = self.build_search_filter(",
                "76:                 search_filter_query, projects, user=user, environments=environments",
                "77:             )",
                "78: ",
                "79:         kwargs = {}",
                "80:         if limit is not None:",
                "81:             kwargs[\"limit\"] = limit",
                "82:         if aggregate_kwargs:",
                "83:             kwargs[\"aggregate_kwargs\"] = {\"trends\": {**aggregate_kwargs}}",
                "84: ",
                "85:         return self.backend.query(",
                "86:             projects,",
                "87:             search_filters=search_filters,",
                "88:             environments=environments,",
                "89:             count_hits=count_hits,",
                "90:             sort_by=sort_by,",
                "91:             date_from=date_from,",
                "92:             date_to=date_to,",
                "93:             cursor=cursor,",
                "94:             **kwargs,",
                "95:         )",
                "96: ",
                "97:     def store_event(self, data, *args, **kwargs):",
                "98:         event = super().store_event(data, *args, **kwargs)",
                "99:         environment_name = data.get(\"environment\")",
                "100:         if environment_name:",
                "101:             GroupEnvironment.objects.filter(",
                "102:                 group_id=event.group_id,",
                "103:                 environment__name=environment_name,",
                "104:                 first_seen__gt=event.datetime,",
                "105:             ).update(first_seen=event.datetime)",
                "106:         return event",
                "107: ",
                "108: ",
                "109: class EventsDatasetTestSetup(SharedSnubaMixin):",
                "110:     @property",
                "111:     def backend(self):",
                "112:         return EventsDatasetSnubaSearchBackend()",
                "113: ",
                "114:     def setUp(self):",
                "115:         super().setUp()",
                "116:         self.base_datetime = before_now(days=3).replace(microsecond=0)",
                "117: ",
                "118:         event1_timestamp = (self.base_datetime - timedelta(days=21)).isoformat()",
                "119:         self.event1 = self.store_event(",
                "120:             data={",
                "121:                 \"fingerprint\": [\"put-me-in-group1\"],",
                "122:                 \"event_id\": \"a\" * 32,",
                "123:                 \"message\": \"foo. Indeed, this message is intended to be greater than 256 characters such that we can put this unique string identifier after that point in the string. The purpose of this is in order to verify we are using snuba to search messages instead of Postgres (postgres truncates at 256 characters and clickhouse does not). santryrox.\",",
                "124:                 \"environment\": \"production\",",
                "125:                 \"tags\": {\"server\": \"example.com\", \"sentry:user\": \"event1@example.com\"},",
                "126:                 \"timestamp\": event1_timestamp,",
                "127:                 \"stacktrace\": {\"frames\": [{\"module\": \"group1\"}]},",
                "128:                 \"level\": \"fatal\",",
                "129:             },",
                "130:             project_id=self.project.id,",
                "131:         )",
                "132:         self.event3 = self.store_event(",
                "133:             data={",
                "134:                 \"fingerprint\": [\"put-me-in-group1\"],",
                "135:                 \"event_id\": \"c\" * 32,",
                "136:                 \"message\": \"group1\",",
                "137:                 \"environment\": \"production\",",
                "138:                 \"tags\": {\"server\": \"example.com\", \"sentry:user\": \"event3@example.com\"},",
                "139:                 \"timestamp\": self.base_datetime.isoformat(),",
                "140:                 \"stacktrace\": {\"frames\": [{\"module\": \"group1\"}]},",
                "141:                 \"level\": \"fatal\",",
                "142:             },",
                "143:             project_id=self.project.id,",
                "144:         )",
                "145: ",
                "146:         self.group1 = Group.objects.get(id=self.event1.group.id)",
                "147:         assert self.group1.id == self.event1.group.id"
            ]
        },
        {
            "file": "tests/snuba/api/endpoints/test_organization_events_vitals.py",
            "line_number": 27,
            "matched_line": "    def store_event(self, data, measurements=None, **kwargs):",
            "context_start_line": 1,
            "context_end_line": 77,
            "context": [
                "1: from datetime import timedelta",
                "2: ",
                "3: import pytest",
                "4: from django.urls import reverse",
                "5: ",
                "6: from sentry.models.transaction_threshold import ProjectTransactionThreshold, TransactionMetric",
                "7: from sentry.testutils.cases import APITestCase, MetricsEnhancedPerformanceTestCase, SnubaTestCase",
                "8: from sentry.testutils.helpers.datetime import before_now",
                "9: from sentry.utils.samples import load_data",
                "10: ",
                "11: pytestmark = pytest.mark.sentry_metrics",
                "12: ",
                "13: ",
                "14: class OrganizationEventsVitalsEndpointTest(APITestCase, SnubaTestCase):",
                "15:     def setUp(self):",
                "16:         super().setUp()",
                "17:         self.start = before_now(days=1).replace(hour=10, minute=0, second=0, microsecond=0)",
                "18:         self.end = self.start + timedelta(hours=6)",
                "19: ",
                "20:         self.transaction_data = load_data(\"transaction\", timestamp=self.start)",
                "21:         self.query: dict[str, str | list[str]] = {",
                "22:             \"start\": self.start.isoformat(),",
                "23:             \"end\": self.end.isoformat(),",
                "24:         }",
                "25:         self.features = {}",
                "26: ",
                "27:     def store_event(self, data, measurements=None, **kwargs):",
                "28:         if measurements:",
                "29:             for vital, value in measurements.items():",
                "30:                 data[\"measurements\"][vital][\"value\"] = value",
                "31: ",
                "32:         return super().store_event(",
                "33:             data.copy(),",
                "34:             project_id=self.project.id,",
                "35:         )",
                "36: ",
                "37:     def do_request(self, query=None, features=None):",
                "38:         if features is None:",
                "39:             features = {\"organizations:discover-basic\": True}",
                "40:         features.update(self.features)",
                "41:         if query is None:",
                "42:             query = self.query",
                "43: ",
                "44:         self.login_as(user=self.user)",
                "45:         url = reverse(",
                "46:             \"sentry-api-0-organization-events-vitals\",",
                "47:             kwargs={\"organization_id_or_slug\": self.organization.slug},",
                "48:         )",
                "49: ",
                "50:         with self.feature(features):",
                "51:             return self.client.get(url, query, format=\"json\")",
                "52: ",
                "53:     def test_no_projects(self):",
                "54:         response = self.do_request()",
                "55:         assert response.status_code == 200, response.content",
                "56:         assert len(response.data) == 0",
                "57: ",
                "58:     def test_no_vitals(self):",
                "59:         self.store_event(",
                "60:             self.transaction_data,",
                "61:             project_id=self.project.id,",
                "62:         )",
                "63: ",
                "64:         self.query.update({\"vital\": []})",
                "65:         response = self.do_request()",
                "66:         assert response.status_code == 400, response.content",
                "67:         assert \"Need to pass at least one vital\" == response.data[\"detail\"]",
                "68: ",
                "69:     def test_bad_vital(self):",
                "70:         self.store_event(",
                "71:             self.transaction_data,",
                "72:             project_id=self.project.id,",
                "73:         )",
                "74: ",
                "75:         self.query.update({\"vital\": [\"foobar\"]})",
                "76:         response = self.do_request()",
                "77:         assert response.status_code == 400, response.content"
            ]
        },
        {
            "file": "tests/sentry/tasks/test_weekly_reports.py",
            "line_number": 63,
            "matched_line": "    def store_event_outcomes(",
            "context_start_line": 13,
            "context_end_line": 113,
            "context": [
                "13: from sentry.issues.grouptype import MonitorIncidentType, PerformanceNPlusOneGroupType",
                "14: from sentry.models.group import GroupStatus",
                "15: from sentry.models.grouphistory import GroupHistoryStatus",
                "16: from sentry.models.organizationmember import OrganizationMember",
                "17: from sentry.models.project import Project",
                "18: from sentry.models.team import TeamStatus",
                "19: from sentry.notifications.models.notificationsettingoption import NotificationSettingOption",
                "20: from sentry.silo.base import SiloMode",
                "21: from sentry.silo.safety import unguarded_write",
                "22: from sentry.snuba.referrer import Referrer",
                "23: from sentry.tasks.summaries.utils import (",
                "24:     ONE_DAY,",
                "25:     OrganizationReportContext,",
                "26:     ProjectContext,",
                "27:     organization_project_issue_substatus_summaries,",
                "28:     project_key_errors,",
                "29:     user_project_ownership,",
                "30: )",
                "31: from sentry.tasks.summaries.weekly_reports import (",
                "32:     OrganizationReportBatch,",
                "33:     group_status_to_color,",
                "34:     prepare_organization_report,",
                "35:     prepare_template_context,",
                "36:     schedule_organizations,",
                "37: )",
                "38: from sentry.testutils.cases import OutcomesSnubaTest, PerformanceIssueTestCase, SnubaTestCase",
                "39: from sentry.testutils.factories import EventType",
                "40: from sentry.testutils.helpers import with_feature",
                "41: from sentry.testutils.helpers.datetime import before_now, freeze_time",
                "42: from sentry.testutils.outbox import outbox_runner",
                "43: from sentry.testutils.silo import assume_test_silo_mode",
                "44: from sentry.types.group import GroupSubStatus",
                "45: from sentry.users.services.user_option import user_option_service",
                "46: from sentry.utils import redis",
                "47: from sentry.utils.dates import floor_to_utc_day",
                "48: from sentry.utils.outcomes import Outcome",
                "49: ",
                "50: DISABLED_ORGANIZATIONS_USER_OPTION_KEY = \"reports:disabled-organizations\"",
                "51: ",
                "52: ",
                "53: class WeeklyReportsTest(OutcomesSnubaTest, SnubaTestCase, PerformanceIssueTestCase):",
                "54:     def setUp(self):",
                "55:         super().setUp()",
                "56:         self.now = timezone.now()",
                "57:         self.timestamp = floor_to_utc_day(self.now).timestamp()",
                "58:         self.two_days_ago = self.now - timedelta(days=2)",
                "59:         self.three_days_ago = self.now - timedelta(days=3)",
                "60: ",
                "61:     _dummy_batch_id = \"20bd6c5b-7fac-4f31-9548-d6f8bb63226d\"",
                "62: ",
                "63:     def store_event_outcomes(",
                "64:         self,",
                "65:         organization_id,",
                "66:         project_id,",
                "67:         timestamp,",
                "68:         num_times,",
                "69:         outcome=Outcome.ACCEPTED,",
                "70:         category=DataCategory.ERROR,",
                "71:     ):",
                "72:         self.store_outcomes(",
                "73:             {",
                "74:                 \"org_id\": organization_id,",
                "75:                 \"project_id\": project_id,",
                "76:                 \"outcome\": outcome,",
                "77:                 \"category\": category,",
                "78:                 \"timestamp\": timestamp,",
                "79:                 \"key_id\": 1,",
                "80:             },",
                "81:             num_times=num_times,",
                "82:         )",
                "83: ",
                "84:     @freeze_time(before_now(days=2).replace(hour=0, minute=0, second=0, microsecond=0))",
                "85:     def test_integration(self):",
                "86:         with unguarded_write(using=router.db_for_write(Project)):",
                "87:             Project.objects.all().delete()",
                "88:         project = self.create_project(",
                "89:             organization=self.organization,",
                "90:             teams=[self.team],",
                "91:             date_added=self.now - timedelta(days=90),",
                "92:         )",
                "93:         member_set = set(project.teams.get().member_set.all())",
                "94:         self.store_event(",
                "95:             data={",
                "96:                 \"timestamp\": before_now(days=1).isoformat(),",
                "97:             },",
                "98:             project_id=project.id,",
                "99:         )",
                "100: ",
                "101:         with self.tasks():",
                "102:             schedule_organizations(timestamp=self.now.timestamp())",
                "103:             assert len(mail.outbox) == len(member_set) == 1",
                "104: ",
                "105:             message = mail.outbox[0]",
                "106:             assert self.organization.name in message.subject",
                "107: ",
                "108:     @freeze_time(before_now(days=2).replace(hour=0, minute=0, second=0, microsecond=0))",
                "109:     def test_with_empty_string_user_option(self):",
                "110:         project = self.create_project(",
                "111:             organization=self.organization,",
                "112:             teams=[self.team],",
                "113:             date_added=self.now - timedelta(days=90),"
            ]
        },
        {
            "file": "tests/sentry/tasks/test_daily_summary.py",
            "line_number": 45,
            "matched_line": "    def store_event_and_outcomes(",
            "context_start_line": 1,
            "context_end_line": 95,
            "context": [
                "1: from datetime import UTC, datetime, timedelta",
                "2: from typing import cast",
                "3: from unittest import mock",
                "4: from urllib.parse import urlencode",
                "5: ",
                "6: import orjson",
                "7: import pytest",
                "8: import responses",
                "9: from django.conf import settings",
                "10: ",
                "11: from sentry.constants import DataCategory",
                "12: from sentry.integrations.types import ExternalProviders",
                "13: from sentry.issues.grouptype import PerformanceNPlusOneGroupType",
                "14: from sentry.models.activity import Activity",
                "15: from sentry.models.group import GroupStatus",
                "16: from sentry.notifications.notifications.daily_summary import DailySummaryNotification",
                "17: from sentry.tasks.summaries.daily_summary import (",
                "18:     build_summary_data,",
                "19:     build_top_projects_map,",
                "20:     deliver_summary,",
                "21:     prepare_summary_data,",
                "22:     schedule_organizations,",
                "23: )",
                "24: from sentry.tasks.summaries.utils import ONE_DAY, DailySummaryProjectContext",
                "25: from sentry.testutils.cases import (",
                "26:     OutcomesSnubaTest,",
                "27:     PerformanceIssueTestCase,",
                "28:     SlackActivityNotificationTest,",
                "29:     SnubaTestCase,",
                "30: )",
                "31: from sentry.testutils.factories import EventType",
                "32: from sentry.testutils.helpers.datetime import before_now, freeze_time",
                "33: from sentry.testutils.helpers.features import with_feature",
                "34: from sentry.types.activity import ActivityType",
                "35: from sentry.types.actor import Actor",
                "36: from sentry.types.group import GroupSubStatus",
                "37: from sentry.users.services.user_option import user_option_service",
                "38: from sentry.utils.outcomes import Outcome",
                "39: ",
                "40: ",
                "41: @freeze_time(before_now(days=2).replace(hour=0, minute=5, second=0, microsecond=0))",
                "42: class DailySummaryTest(",
                "43:     OutcomesSnubaTest, SnubaTestCase, PerformanceIssueTestCase, SlackActivityNotificationTest",
                "44: ):",
                "45:     def store_event_and_outcomes(",
                "46:         self,",
                "47:         project_id,",
                "48:         timestamp,",
                "49:         fingerprint,",
                "50:         category,",
                "51:         release=None,",
                "52:         resolve=True,",
                "53:         level=\"error\",",
                "54:     ):",
                "55:         if category == DataCategory.ERROR:",
                "56:             data = {",
                "57:                 \"timestamp\": timestamp.isoformat(),",
                "58:                 \"fingerprint\": [fingerprint],",
                "59:                 \"level\": level,",
                "60:                 \"exception\": {",
                "61:                     \"values\": [",
                "62:                         {",
                "63:                             \"type\": \"IntegrationError\",",
                "64:                             \"value\": \"Identity not found.\",",
                "65:                         }",
                "66:                     ]",
                "67:                 },",
                "68:             }",
                "69:             if release:",
                "70:                 data[\"release\"] = release",
                "71: ",
                "72:             event = self.store_event(",
                "73:                 data=data,",
                "74:                 project_id=project_id,",
                "75:                 assert_no_errors=False,",
                "76:                 default_event_type=EventType.DEFAULT,",
                "77:             )",
                "78:         elif category == DataCategory.TRANSACTION:",
                "79:             event = self.create_performance_issue()",
                "80: ",
                "81:         self.store_outcomes(",
                "82:             {",
                "83:                 \"org_id\": self.organization.id,",
                "84:                 \"project_id\": project_id,",
                "85:                 \"outcome\": Outcome.ACCEPTED,",
                "86:                 \"category\": category,",
                "87:                 \"timestamp\": timestamp,",
                "88:                 \"key_id\": 1,",
                "89:             },",
                "90:             num_times=1,",
                "91:         )",
                "92: ",
                "93:         group = event.group",
                "94:         if resolve:",
                "95:             group.status = GroupStatus.RESOLVED"
            ]
        },
        {
            "file": "tests/sentry/utils/sdk_crashes/conftest.py",
            "line_number": 12,
            "matched_line": "def store_event(default_project, factories):",
            "context_start_line": 1,
            "context_end_line": 37,
            "context": [
                "1: import pytest",
                "2: ",
                "3: from sentry.utils.sdk_crashes.path_replacer import FixedPathReplacer",
                "4: from sentry.utils.sdk_crashes.sdk_crash_detection_config import (",
                "5:     SDKCrashDetectionConfig,",
                "6:     SDKFrameConfig,",
                "7:     SdkName,",
                "8: )",
                "9: ",
                "10: ",
                "11: @pytest.fixture",
                "12: def store_event(default_project, factories):",
                "13:     def inner(data):",
                "14:         return factories.store_event(data=data, project_id=default_project.id)",
                "15: ",
                "16:     return inner",
                "17: ",
                "18: ",
                "19: @pytest.fixture",
                "20: def empty_cocoa_config() -> SDKCrashDetectionConfig:",
                "21:     return SDKCrashDetectionConfig(",
                "22:         sdk_name=SdkName.Cocoa,",
                "23:         project_id=0,",
                "24:         sample_rate=0.0,",
                "25:         organization_allowlist=[],",
                "26:         sdk_names={},",
                "27:         report_fatal_errors=False,",
                "28:         ignore_mechanism_type=set(),",
                "29:         allow_mechanism_type=set(),",
                "30:         system_library_path_patterns=set(),",
                "31:         sdk_frame_config=SDKFrameConfig(",
                "32:             function_patterns=set(),",
                "33:             path_patterns=set(),",
                "34:             path_replacer=FixedPathReplacer(path=\"\"),",
                "35:         ),",
                "36:         sdk_crash_ignore_matchers=set(),",
                "37:     )"
            ]
        },
        {
            "file": "tests/sentry/utils/sdk_crashes/test_event_stripper.py",
            "line_number": 22,
            "matched_line": "def store_event(default_project, factories):",
            "context_start_line": 1,
            "context_end_line": 72,
            "context": [
                "1: from collections.abc import Sequence",
                "2: ",
                "3: import pytest",
                "4: ",
                "5: from fixtures.sdk_crash_detection.crash_event_cocoa import (",
                "6:     get_crash_event,",
                "7:     get_crash_event_with_frames,",
                "8:     get_frames,",
                "9: )",
                "10: from sentry.testutils.helpers.options import override_options",
                "11: from sentry.testutils.pytest.fixtures import django_db_all",
                "12: from sentry.utils.safe import get_path, set_path",
                "13: from sentry.utils.sdk_crashes.event_stripper import strip_event_data",
                "14: from sentry.utils.sdk_crashes.sdk_crash_detection_config import (",
                "15:     SDKCrashDetectionConfig,",
                "16:     build_sdk_crash_detection_configs,",
                "17: )",
                "18: from sentry.utils.sdk_crashes.sdk_crash_detector import SDKCrashDetector",
                "19: ",
                "20: ",
                "21: @pytest.fixture",
                "22: def store_event(default_project, factories):",
                "23:     def inner(data):",
                "24:         return factories.store_event(data=data, project_id=default_project.id)",
                "25: ",
                "26:     return inner",
                "27: ",
                "28: ",
                "29: @pytest.fixture",
                "30: def configs() -> Sequence[SDKCrashDetectionConfig]:",
                "31:     with override_options(",
                "32:         {",
                "33:             \"issues.sdk_crash_detection.cocoa.project_id\": 1234,",
                "34:             \"issues.sdk_crash_detection.cocoa.sample_rate\": 1.0,",
                "35:             \"issues.sdk_crash_detection.react-native.project_id\": 2,",
                "36:             \"issues.sdk_crash_detection.react-native.sample_rate\": 0.2,",
                "37:             \"issues.sdk_crash_detection.react-native.organization_allowlist\": [1],",
                "38:             \"issues.sdk_crash_detection.java.project_id\": 3,",
                "39:             \"issues.sdk_crash_detection.java.sample_rate\": 0.3,",
                "40:             \"issues.sdk_crash_detection.java.organization_allowlist\": [2],",
                "41:         }",
                "42:     ):",
                "43:         return build_sdk_crash_detection_configs()",
                "44: ",
                "45: ",
                "46: @pytest.fixture",
                "47: def store_and_strip_event(configs, store_event):",
                "48:     def inner(data, config=configs[0]):",
                "49:         event = store_event(data=data)",
                "50:         return strip_event_data(event.data, SDKCrashDetector(config=config))",
                "51: ",
                "52:     return inner",
                "53: ",
                "54: ",
                "55: @django_db_all",
                "56: @pytest.mark.snuba",
                "57: def test_strip_event_data_keeps_allowed_keys(store_and_strip_event):",
                "58:     stripped_event_data = store_and_strip_event(data=get_crash_event())",
                "59: ",
                "60:     keys_removed = {\"tags\", \"user\", \"threads\", \"breadcrumbs\", \"environment\"}",
                "61:     for key in keys_removed:",
                "62:         assert stripped_event_data.get(key) is None, f\"key {key} should be removed\"",
                "63: ",
                "64:     keys_kept = {",
                "65:         \"type\",",
                "66:         \"timestamp\",",
                "67:         \"platform\",",
                "68:         \"sdk\",",
                "69:         \"exception\",",
                "70:         \"contexts\",",
                "71:     }",
                "72: "
            ]
        },
        {
            "file": "tests/sentry/notifications/utils/test_participants.py",
            "line_number": 266,
            "matched_line": "    def store_event_owners(self, filename: str) -> Event:",
            "context_start_line": 216,
            "context_end_line": 316,
            "context": [
                "216:         with assume_test_silo_mode(SiloMode.CONTROL):",
                "217:             NotificationSettingProvider.objects.create(",
                "218:                 team_id=self.team.id,",
                "219:                 scope_type=\"team\",",
                "220:                 scope_identifier=self.team.id,",
                "221:                 provider=\"slack\",",
                "222:                 type=\"workflow\",",
                "223:                 value=\"always\",",
                "224:             )",
                "225: ",
                "226:         assert get_send_to(",
                "227:             project=self.project,",
                "228:             target_type=ActionTargetType.TEAM,",
                "229:             target_identifier=self.team.id,",
                "230:             notification_type_enum=NotificationSettingEnum.WORKFLOW,",
                "231:         ) == {",
                "232:             ExternalProviders.SLACK: {Actor.from_orm_team(self.team)},",
                "233:         }",
                "234: ",
                "235:     def test_other_project_team(self):",
                "236:         user_2 = self.create_user()",
                "237:         team_2 = self.create_team(self.organization, members=[user_2])",
                "238:         project_2 = self.create_project(organization=self.organization, teams=[team_2])",
                "239: ",
                "240:         self.assert_recipients_are(",
                "241:             self.get_send_to_team(project_2, team_2.id), email=[user_2.id], slack=[user_2.id]",
                "242:         )",
                "243:         assert self.get_send_to_team(self.project, team_2.id) == {}",
                "244: ",
                "245:     def test_other_org_team(self):",
                "246:         org_2 = self.create_organization()",
                "247:         user_2 = self.create_user()",
                "248:         team_2 = self.create_team(org_2, members=[user_2])",
                "249:         project_2 = self.create_project(organization=org_2, teams=[team_2])",
                "250: ",
                "251:         self.assert_recipients_are(",
                "252:             self.get_send_to_team(project_2, team_2.id), email=[user_2.id], slack=[user_2.id]",
                "253:         )",
                "254:         assert self.get_send_to_team(self.project, team_2.id) == {}",
                "255: ",
                "256: ",
                "257: class GetSendToOwnersTest(_ParticipantsTest):",
                "258:     def get_send_to_owners(self, event: Event) -> Mapping[ExternalProviders, set[Actor]]:",
                "259:         return get_send_to(",
                "260:             self.project,",
                "261:             target_type=ActionTargetType.ISSUE_OWNERS,",
                "262:             target_identifier=None,",
                "263:             event=event,",
                "264:         )",
                "265: ",
                "266:     def store_event_owners(self, filename: str) -> Event:",
                "267:         return super().store_event(data=make_event_data(filename), project_id=self.project.id)",
                "268: ",
                "269:     def setUp(self):",
                "270:         self.user2 = self.create_user(email=\"baz@example.com\", is_active=True)",
                "271:         self.user3 = self.create_user(email=\"bar@example.com\", is_active=True)",
                "272:         self.user_suspect_committer = self.create_user(",
                "273:             email=\"suspectcommitter@example.com\", is_active=True",
                "274:         )",
                "275: ",
                "276:         self.team2 = self.create_team(",
                "277:             organization=self.organization, members=[self.user, self.user2]",
                "278:         )",
                "279:         self.team_suspect_committer = self.create_team(",
                "280:             organization=self.organization, members=[self.user_suspect_committer]",
                "281:         )",
                "282:         self.project.add_team(self.team2)",
                "283:         self.project.add_team(self.team_suspect_committer)",
                "284:         self.repo = Repository.objects.create(",
                "285:             organization_id=self.organization.id, name=self.organization.id",
                "286:         )",
                "287: ",
                "288:         user_ids = list(self.project.member_set.values_list(\"user_id\", flat=True))",
                "289:         with assume_test_silo_mode(SiloMode.CONTROL):",
                "290:             users = [Owner(\"user\", user.email) for user in User.objects.filter(id__in=user_ids)]",
                "291:         ProjectOwnership.objects.create(",
                "292:             project_id=self.project.id,",
                "293:             schema=dump_schema(",
                "294:                 [",
                "295:                     grammar.Rule(Matcher(\"path\", \"*.py\"), [Owner(\"team\", self.team2.slug)]),",
                "296:                     grammar.Rule(Matcher(\"path\", \"*.jsx\"), [Owner(\"user\", self.user.email)]),",
                "297:                     grammar.Rule(Matcher(\"path\", \"*.jx\"), [Owner(\"user\", self.user3.email)]),",
                "298:                     grammar.Rule(Matcher(\"path\", \"*.java\"), [Owner(\"user\", self.user.email)]),",
                "299:                     grammar.Rule(",
                "300:                         Matcher(\"path\", \"*.cbl\"),",
                "301:                         users,",
                "302:                     ),",
                "303:                     grammar.Rule(Matcher(\"path\", \"*.lol\"), []),",
                "304:                 ]",
                "305:             ),",
                "306:             fallthrough=True,",
                "307:         )",
                "308:         with assume_test_silo_mode(SiloMode.CONTROL):",
                "309:             self.integration.add_organization(self.project.organization, self.user)",
                "310: ",
                "311:     def create_sample_commit(self, user: User) -> Commit:",
                "312:         return self.create_commit(",
                "313:             project=self.project,",
                "314:             repo=self.repo,",
                "315:             author=self.create_commit_author(project=self.project, user=user),",
                "316:             key=\"a\" * 40,"
            ]
        },
        {
            "file": "tests/sentry/notifications/utils/test_participants.py",
            "line_number": 830,
            "matched_line": "    def store_event(self, filename: str, project: Project) -> Event:",
            "context_start_line": 780,
            "context_end_line": 880,
            "context": [
                "780:             ),",
                "781:             # test with fallthrough off to ensure the new fallthrough logic is used",
                "782:             fallthrough=False,",
                "783:         )",
                "784: ",
                "785:         # turn off slack for teams",
                "786:         with assume_test_silo_mode(SiloMode.CONTROL):",
                "787:             for user in [self.user, self.user2, self.user3]:",
                "788:                 NotificationSettingProvider.objects.create(",
                "789:                     user_id=user.id,",
                "790:                     scope_type=\"user\",",
                "791:                     scope_identifier=user.id,",
                "792:                     provider=\"slack\",",
                "793:                     type=\"alerts\",",
                "794:                     value=\"never\",",
                "795:                 )",
                "796:         with assume_test_silo_mode(SiloMode.CONTROL):",
                "797:             # disable Slack",
                "798:             NotificationSettingProvider.objects.create(",
                "799:                 team_id=self.team.id,",
                "800:                 scope_type=\"team\",",
                "801:                 scope_identifier=self.team.id,",
                "802:                 provider=\"slack\",",
                "803:                 type=\"alerts\",",
                "804:                 value=\"never\",",
                "805:             )",
                "806:             NotificationSettingProvider.objects.create(",
                "807:                 user_id=self.user.id,",
                "808:                 scope_type=\"user\",",
                "809:                 scope_identifier=self.user.id,",
                "810:                 provider=\"slack\",",
                "811:                 type=\"alerts\",",
                "812:                 value=\"never\",",
                "813:             )",
                "814:             NotificationSettingOption.objects.all().delete()",
                "815: ",
                "816:     def get_send_to_fallthrough(",
                "817:         self,",
                "818:         event: Event,",
                "819:         project: Project,",
                "820:         fallthrough_choice: FallthroughChoiceType | None = None,",
                "821:     ) -> Mapping[ExternalProviders, set[Actor]]:",
                "822:         return get_send_to(",
                "823:             project,",
                "824:             target_type=ActionTargetType.ISSUE_OWNERS,",
                "825:             target_identifier=None,",
                "826:             event=event,",
                "827:             fallthrough_choice=fallthrough_choice,",
                "828:         )",
                "829: ",
                "830:     def store_event(self, filename: str, project: Project) -> Event:",
                "831:         return super().store_event(data=make_event_data(filename), project_id=project.id)",
                "832: ",
                "833:     def test_invalid_fallthrough_choice(self):",
                "834:         with pytest.raises(NotImplementedError) as e:",
                "835:             get_fallthrough_recipients(self.project, \"invalid\")  # type: ignore[arg-type]",
                "836:         assert str(e.value).startswith(\"Unknown fallthrough choice: invalid\")",
                "837: ",
                "838:     def test_fallthrough_setting_on(self):",
                "839:         \"\"\"",
                "840:         Test that the new fallthrough choice takes precedence even if the fallthrough setting is on.",
                "841:         \"\"\"",
                "842:         ProjectOwnership.objects.get(project_id=self.project.id).update(fallthrough=True)",
                "843: ",
                "844:         event = self.store_event(\"empty.lol\", self.project)",
                "845:         self.assert_recipients_are(",
                "846:             self.get_send_to_fallthrough(event, self.project, FallthroughChoiceType.ALL_MEMBERS),",
                "847:             email=[self.user.id, self.user2.id],",
                "848:         )",
                "849: ",
                "850:         event = self.store_event(\"empty.lol\", self.project)",
                "851:         assert self.get_send_to_fallthrough(event, self.project, FallthroughChoiceType.NO_ONE) == {}",
                "852: ",
                "853:     def test_no_fallthrough(self):",
                "854:         \"\"\"",
                "855:         Test the new fallthrough choice when no fallthrough choice is provided.\"\"\"",
                "856:         event = self.store_event(\"none.lol\", self.project)",
                "857:         assert self.get_send_to_fallthrough(event, self.project, fallthrough_choice=None) == {}",
                "858: ",
                "859:     def test_no_owners(self):",
                "860:         \"\"\"",
                "861:         Test the fallthrough when there is no ProjectOwnership set.",
                "862:         \"\"\"",
                "863:         project_without_team = self.create_project(",
                "864:             name=\"no-teams\", teams=None, organization=self.organization",
                "865:         )",
                "866:         event = self.store_event(\"empty.unknown\", project_without_team)",
                "867:         ret = self.get_send_to_fallthrough(",
                "868:             event, project_without_team, FallthroughChoiceType.ALL_MEMBERS",
                "869:         )",
                "870:         assert ret == {}",
                "871: ",
                "872:     def test_fallthrough_no_one(self):",
                "873:         event = self.store_event(\"empty.lol\", self.project)",
                "874:         assert self.get_send_to_fallthrough(event, self.project, FallthroughChoiceType.NO_ONE) == {}",
                "875: ",
                "876:     def test_fallthrough_all_members_no_owner(self):",
                "877:         empty_project = self.create_project(organization=self.organization)",
                "878:         ProjectOwnership.objects.create(",
                "879:             project_id=empty_project.id,",
                "880:             schema=dump_schema("
            ]
        },
        {
            "file": "src/sentry/testutils/cases.py",
            "line_number": 1058,
            "matched_line": "    def store_event(self, *args, **kwargs):",
            "context_start_line": 1008,
            "context_end_line": 1108,
            "context": [
                "1008:             \"sentry-organization-integrations-setup\",",
                "1009:             kwargs={",
                "1010:                 \"organization_slug\": self.organization.slug,",
                "1011:                 \"provider_id\": self.provider.key,",
                "1012:             },",
                "1013:         )",
                "1014: ",
                "1015:         self.setup_path = reverse(",
                "1016:             \"sentry-extension-setup\", kwargs={\"provider_id\": self.provider.key}",
                "1017:         )",
                "1018:         self.configure_path = f\"/extensions/{self.provider.key}/configure/\"",
                "1019: ",
                "1020:         self.pipeline.initialize()",
                "1021:         self.save_session()",
                "1022: ",
                "1023:     def assertDialogSuccess(self, resp):",
                "1024:         assert b'window.opener.postMessage({\"success\":true' in resp.content",
                "1025: ",
                "1026: ",
                "1027: @pytest.mark.snuba",
                "1028: @requires_snuba",
                "1029: class SnubaTestCase(BaseTestCase):",
                "1030:     \"\"\"",
                "1031:     Mixin for enabling test case classes to talk to snuba",
                "1032:     Useful when you are working on acceptance tests or integration",
                "1033:     tests that require snuba.",
                "1034:     \"\"\"",
                "1035: ",
                "1036:     # We need Django to flush all databases.",
                "1037:     databases: set[str] | str = \"__all__\"",
                "1038: ",
                "1039:     def setUp(self):",
                "1040:         super().setUp()",
                "1041:         self.init_snuba()",
                "1042: ",
                "1043:     @pytest.fixture(autouse=True)",
                "1044:     def initialize(self, reset_snuba, call_snuba):",
                "1045:         self.call_snuba = call_snuba",
                "1046: ",
                "1047:     def create_project(self, **kwargs) -> Project:",
                "1048:         if \"flags\" not in kwargs:",
                "1049:             # We insert events directly into snuba in tests, so we need to set has_transactions to True so the",
                "1050:             # application knows that events have been sent",
                "1051:             kwargs[\"flags\"] = Project.flags.has_transactions",
                "1052:         return super().create_project(**kwargs)",
                "1053: ",
                "1054:     def init_snuba(self):",
                "1055:         self.snuba_eventstream = SnubaEventStream()",
                "1056:         self.snuba_tagstore = SnubaTagStorage()",
                "1057: ",
                "1058:     def store_event(self, *args, **kwargs):",
                "1059:         \"\"\"",
                "1060:         Simulates storing an event for testing.",
                "1061: ",
                "1062:         To set event title:",
                "1063:         - use \"message\": \"{title}\" field for errors",
                "1064:         - use \"transaction\": \"{title}\" field for transactions",
                "1065:         More info on event payloads: https://develop.sentry.dev/sdk/event-payloads/",
                "1066:         \"\"\"",
                "1067:         with mock.patch(\"sentry.eventstream.insert\", self.snuba_eventstream.insert):",
                "1068:             stored_event = Factories.store_event(*args, **kwargs)",
                "1069: ",
                "1070:             # Error groups",
                "1071:             stored_group = stored_event.group",
                "1072:             if stored_group is not None:",
                "1073:                 self.store_group(stored_group)",
                "1074: ",
                "1075:             # Performance groups",
                "1076:             stored_groups = stored_event.groups",
                "1077:             if stored_groups is not None:",
                "1078:                 for group in stored_groups:",
                "1079:                     self.store_group(group)",
                "1080:             return stored_event",
                "1081: ",
                "1082:     def wait_for_event_count(self, project_id, total, attempts=2):",
                "1083:         \"\"\"",
                "1084:         Wait until the event count reaches the provided value or until attempts is reached.",
                "1085: ",
                "1086:         Useful when you're storing several events and need to ensure that snuba/clickhouse",
                "1087:         state has settled.",
                "1088:         \"\"\"",
                "1089:         # Verify that events have settled in snuba's storage.",
                "1090:         # While snuba is synchronous, clickhouse isn't entirely synchronous.",
                "1091:         attempt = 0",
                "1092:         snuba_filter = eventstore.Filter(project_ids=[project_id])",
                "1093:         last_events_seen = 0",
                "1094: ",
                "1095:         while attempt < attempts:",
                "1096:             events = eventstore.backend.get_events(",
                "1097:                 snuba_filter, referrer=\"test.wait_for_event_count\"",
                "1098:             )",
                "1099:             last_events_seen = len(events)",
                "1100:             if len(events) >= total:",
                "1101:                 break",
                "1102:             attempt += 1",
                "1103:             time.sleep(0.05)",
                "1104:         if attempt == attempts:",
                "1105:             assert (",
                "1106:                 False",
                "1107:             ), f\"Could not ensure that {total} event(s) were persisted within {attempt} attempt(s). Event count is instead currently {last_events_seen}.\"",
                "1108: "
            ]
        },
        {
            "file": "src/sentry/testutils/factories.py",
            "line_number": 1021,
            "matched_line": "    def store_event(",
            "context_start_line": 971,
            "context_end_line": 1071,
            "context": [
                "971:     @staticmethod",
                "972:     @assume_test_silo_mode(SiloMode.CONTROL)",
                "973:     def create_user_role(*args, **kwargs):",
                "974:         return UserRole.objects.create(*args, **kwargs)",
                "975: ",
                "976:     @staticmethod",
                "977:     @assume_test_silo_mode(SiloMode.CONTROL)",
                "978:     def create_usersocialauth(",
                "979:         user: User,",
                "980:         provider: str | None = None,",
                "981:         uid: str | None = None,",
                "982:         extra_data: dict[str, Any] | None = None,",
                "983:     ):",
                "984:         if not provider:",
                "985:             provider = \"asana\"",
                "986:         if not uid:",
                "987:             uid = \"abc-123\"",
                "988:         usa = UserSocialAuth(user=user, provider=provider, uid=uid, extra_data=extra_data)",
                "989:         usa.save()",
                "990:         return usa",
                "991: ",
                "992:     @staticmethod",
                "993:     def inject_performance_problems(jobs, _):",
                "994:         for job in jobs:",
                "995:             job[\"performance_problems\"] = []",
                "996:             for f in job[\"data\"][\"fingerprint\"]:",
                "997:                 f_data = f.split(\"-\", 1)",
                "998:                 if len(f_data) < 2:",
                "999:                     raise ValueError(",
                "1000:                         \"Invalid performance fingerprint data. Format must be 'group_type-fingerprint'.\"",
                "1001:                     )",
                "1002:                 group_type = get_group_type_by_type_id(int(f_data[0]))",
                "1003:                 perf_fingerprint = f_data[1]",
                "1004: ",
                "1005:                 job[\"performance_problems\"].append(",
                "1006:                     PerformanceProblem(",
                "1007:                         fingerprint=perf_fingerprint,",
                "1008:                         op=\"db\",",
                "1009:                         desc=\"\",",
                "1010:                         type=group_type,",
                "1011:                         parent_span_ids=None,",
                "1012:                         cause_span_ids=None,",
                "1013:                         offender_span_ids=[],",
                "1014:                         evidence_data={},",
                "1015:                         evidence_display=[],",
                "1016:                     )",
                "1017:                 )",
                "1018: ",
                "1019:     @staticmethod",
                "1020:     @assume_test_silo_mode(SiloMode.REGION)",
                "1021:     def store_event(",
                "1022:         data,",
                "1023:         project_id: int,",
                "1024:         assert_no_errors: bool = True,",
                "1025:         default_event_type: EventType | None = None,",
                "1026:         sent_at: datetime | None = None,",
                "1027:     ) -> Event:",
                "1028:         \"\"\"",
                "1029:         Like `create_event`, but closer to how events are actually",
                "1030:         ingested. Prefer to use this method over `create_event`",
                "1031:         \"\"\"",
                "1032: ",
                "1033:         # this creates a basic message event",
                "1034:         if default_event_type == EventType.DEFAULT:",
                "1035:             data.update({\"stacktrace\": copy.deepcopy(DEFAULT_EVENT_DATA[\"stacktrace\"])})",
                "1036: ",
                "1037:         # this creates an error event",
                "1038:         elif default_event_type == EventType.ERROR:",
                "1039:             data.update({\"exception\": [{\"value\": \"BadError\"}]})",
                "1040: ",
                "1041:         manager = EventManager(data, sent_at=sent_at)",
                "1042:         manager.normalize()",
                "1043:         if assert_no_errors:",
                "1044:             errors = manager.get_data().get(\"errors\")",
                "1045:             assert not errors, errors",
                "1046: ",
                "1047:         normalized_data = manager.get_data()",
                "1048: ",
                "1049:         _set_sample_rate_from_error_sampling(normalized_data)",
                "1050: ",
                "1051:         event = None",
                "1052: ",
                "1053:         # When fingerprint is present on transaction, inject performance problems",
                "1054:         if (",
                "1055:             normalized_data.get(\"type\") == \"transaction\"",
                "1056:             and normalized_data.get(\"fingerprint\") is not None",
                "1057:         ):",
                "1058:             with mock.patch(",
                "1059:                 \"sentry.event_manager._detect_performance_problems\",",
                "1060:                 Factories.inject_performance_problems,",
                "1061:             ):",
                "1062:                 event = manager.save(project_id)",
                "1063: ",
                "1064:         else:",
                "1065:             event = manager.save(project_id)",
                "1066: ",
                "1067:         if event.groups:",
                "1068:             for group in event.groups:",
                "1069:                 group.save()",
                "1070: ",
                "1071:         if event.group:"
            ]
        }
    ],
    "total": 10
}

Message 9 [Assistant]:
--------------------------------------------------------------------------------
Content:
让我查看 Factories 类中的 store_event 方法：
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "def store_event",
      "is_regex": false,
      "context_lines": 100,
      "max_results": 5
    },
    "id": "call_00_e0FNPAe1Ojx1WpDcqt7XW2LQ",
    "type": "tool_call"
  }
]

Message 10 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_e0FNPAe1Ojx1WpDcqt7XW2LQ
Result:
{
    "matches": [
        {
            "file": "tests/snuba/search/test_backend.py",
            "line_number": 97,
            "matched_line": "    def store_event(self, data, *args, **kwargs):",
            "context_start_line": 1,
            "context_end_line": 197,
            "context": [
                "1: import time",
                "2: import uuid",
                "3: from datetime import UTC, datetime, timedelta",
                "4: from typing import Any",
                "5: from unittest import mock",
                "6: ",
                "7: import pytest",
                "8: from django.utils import timezone",
                "9: from sentry_kafka_schemas.schema_types.group_attributes_v1 import GroupAttributesSnapshot",
                "10: ",
                "11: from sentry import options",
                "12: from sentry.api.issue_search import convert_query_values, issue_search_config, parse_search_query",
                "13: from sentry.exceptions import InvalidSearchQuery",
                "14: from sentry.grouping.grouptype import ErrorGroupType",
                "15: from sentry.incidents.grouptype import MetricIssue",
                "16: from sentry.issues.grouptype import (",
                "17:     FeedbackGroup,",
                "18:     NoiseConfig,",
                "19:     PerformanceNPlusOneGroupType,",
                "20:     PerformanceRenderBlockingAssetSpanGroupType,",
                "21: )",
                "22: from sentry.issues.ingest import send_issue_occurrence_to_eventstream",
                "23: from sentry.models.environment import Environment",
                "24: from sentry.models.group import Group, GroupStatus",
                "25: from sentry.models.groupassignee import GroupAssignee",
                "26: from sentry.models.groupbookmark import GroupBookmark",
                "27: from sentry.models.groupenvironment import GroupEnvironment",
                "28: from sentry.models.grouphistory import GroupHistoryStatus, record_group_history",
                "29: from sentry.models.groupowner import GroupOwner",
                "30: from sentry.models.groupsubscription import GroupSubscription",
                "31: from sentry.search.snuba.backend import EventsDatasetSnubaSearchBackend, SnubaSearchBackendBase",
                "32: from sentry.search.snuba.executors import TrendsSortWeights",
                "33: from sentry.seer.seer_utils import FixabilityScoreThresholds",
                "34: from sentry.snuba.dataset import Dataset",
                "35: from sentry.testutils.cases import SnubaTestCase, TestCase, TransactionTestCase",
                "36: from sentry.testutils.helpers import Feature, apply_feature_flag_on_cls",
                "37: from sentry.testutils.helpers.datetime import before_now",
                "38: from sentry.types.group import GroupSubStatus, PriorityLevel",
                "39: from sentry.utils import json",
                "40: from sentry.utils.snuba import SENTRY_SNUBA_MAP",
                "41: from tests.sentry.issues.test_utils import OccurrenceTestMixin",
                "42: ",
                "43: ",
                "44: def date_to_query_format(date):",
                "45:     return date.strftime(\"%Y-%m-%dT%H:%M:%S\")",
                "46: ",
                "47: ",
                "48: class SharedSnubaMixin(SnubaTestCase):",
                "49:     @property",
                "50:     def backend(self) -> SnubaSearchBackendBase:",
                "51:         raise NotImplementedError(self)",
                "52: ",
                "53:     def build_search_filter(self, query, projects=None, user=None, environments=None):",
                "54:         user = user if user is not None else self.user",
                "55:         projects = projects if projects is not None else [self.project]",
                "56:         return convert_query_values(parse_search_query(query), projects, user, environments)",
                "57: ",
                "58:     def make_query(",
                "59:         self,",
                "60:         projects=None,",
                "61:         search_filter_query=None,",
                "62:         user=None,",
                "63:         environments=None,",
                "64:         sort_by=\"date\",",
                "65:         limit=None,",
                "66:         count_hits=False,",
                "67:         date_from=None,",
                "68:         date_to=None,",
                "69:         cursor=None,",
                "70:         aggregate_kwargs=None,",
                "71:     ):",
                "72:         search_filters = []",
                "73:         projects = projects if projects is not None else [self.project]",
                "74:         if search_filter_query is not None:",
                "75:             search_filters = self.build_search_filter(",
                "76:                 search_filter_query, projects, user=user, environments=environments",
                "77:             )",
                "78: ",
                "79:         kwargs = {}",
                "80:         if limit is not None:",
                "81:             kwargs[\"limit\"] = limit",
                "82:         if aggregate_kwargs:",
                "83:             kwargs[\"aggregate_kwargs\"] = {\"trends\": {**aggregate_kwargs}}",
                "84: ",
                "85:         return self.backend.query(",
                "86:             projects,",
                "87:             search_filters=search_filters,",
                "88:             environments=environments,",
                "89:             count_hits=count_hits,",
                "90:             sort_by=sort_by,",
                "91:             date_from=date_from,",
                "92:             date_to=date_to,",
                "93:             cursor=cursor,",
                "94:             **kwargs,",
                "95:         )",
                "96: ",
                "97:     def store_event(self, data, *args, **kwargs):",
                "98:         event = super().store_event(data, *args, **kwargs)",
                "99:         environment_name = data.get(\"environment\")",
                "100:         if environment_name:",
                "101:             GroupEnvironment.objects.filter(",
                "102:                 group_id=event.group_id,",
                "103:                 environment__name=environment_name,",
                "104:                 first_seen__gt=event.datetime,",
                "105:             ).update(first_seen=event.datetime)",
                "106:         return event",
                "107: ",
                "108: ",
                "109: class EventsDatasetTestSetup(SharedSnubaMixin):",
                "110:     @property",
                "111:     def backend(self):",
                "112:         return EventsDatasetSnubaSearchBackend()",
                "113: ",
                "114:     def setUp(self):",
                "115:         super().setUp()",
                "116:         self.base_datetime = before_now(days=3).replace(microsecond=0)",
                "117: ",
                "118:         event1_timestamp = (self.base_datetime - timedelta(days=21)).isoformat()",
                "119:         self.event1 = self.store_event(",
                "120:             data={",
                "121:                 \"fingerprint\": [\"put-me-in-group1\"],",
                "122:                 \"event_id\": \"a\" * 32,",
                "123:                 \"message\": \"foo. Indeed, this message is intended to be greater than 256 characters such that we can put this unique string identifier after that point in the string. The purpose of this is in order to verify we are using snuba to search messages instead of Postgres (postgres truncates at 256 characters and clickhouse does not). santryrox.\",",
                "124:                 \"environment\": \"production\",",
                "125:                 \"tags\": {\"server\": \"example.com\", \"sentry:user\": \"event1@example.com\"},",
                "126:                 \"timestamp\": event1_timestamp,",
                "127:                 \"stacktrace\": {\"frames\": [{\"module\": \"group1\"}]},",
                "128:                 \"level\": \"fatal\",",
                "129:             },",
                "130:             project_id=self.project.id,",
                "131:         )",
                "132:         self.event3 = self.store_event(",
                "133:             data={",
                "134:                 \"fingerprint\": [\"put-me-in-group1\"],",
                "135:                 \"event_id\": \"c\" * 32,",
                "136:                 \"message\": \"group1\",",
                "137:                 \"environment\": \"production\",",
                "138:                 \"tags\": {\"server\": \"example.com\", \"sentry:user\": \"event3@example.com\"},",
                "139:                 \"timestamp\": self.base_datetime.isoformat(),",
                "140:                 \"stacktrace\": {\"frames\": [{\"module\": \"group1\"}]},",
                "141:                 \"level\": \"fatal\",",
                "142:             },",
                "143:             project_id=self.project.id,",
                "144:         )",
                "145: ",
                "146:         self.group1 = Group.objects.get(id=self.event1.group.id)",
                "147:         assert self.group1.id == self.event1.group.id",
                "148:         assert self.group1.id == self.event3.group.id",
                "149: ",
                "150:         assert self.group1.first_seen == self.event1.datetime",
                "151:         assert self.group1.last_seen == self.event3.datetime",
                "152: ",
                "153:         self.group1.times_seen = 5",
                "154:         self.group1.status = GroupStatus.UNRESOLVED",
                "155:         self.group1.substatus = GroupSubStatus.ONGOING",
                "156:         self.group1.priority = PriorityLevel.HIGH",
                "157:         self.group1.update(type=ErrorGroupType.type_id)",
                "158:         self.group1.seer_fixability_score = FixabilityScoreThresholds.HIGH.value + 0.01",
                "159:         self.group1.seer_autofix_last_triggered = self.event3.datetime",
                "160:         self.group1.save()",
                "161:         self.store_group(self.group1)",
                "162: ",
                "163:         self.event2 = self.store_event(",
                "164:             data={",
                "165:                 \"fingerprint\": [\"put-me-in-group2\"],",
                "166:                 \"event_id\": \"b\" * 32,",
                "167:                 \"timestamp\": (self.base_datetime - timedelta(days=20)).isoformat(),",
                "168:                 \"message\": \"bar\",",
                "169:                 \"stacktrace\": {\"frames\": [{\"module\": \"group2\"}]},",
                "170:                 \"environment\": \"staging\",",
                "171:                 \"tags\": {",
                "172:                     \"server\": \"example.com\",",
                "173:                     \"url\": \"http://example.com\",",
                "174:                     \"sentry:user\": \"event2@example.com\",",
                "175:                 },",
                "176:                 \"level\": \"error\",",
                "177:             },",
                "178:             project_id=self.project.id,",
                "179:         )",
                "180: ",
                "181:         self.group2 = Group.objects.get(id=self.event2.group.id)",
                "182:         assert self.group2.id == self.event2.group.id",
                "183:         assert self.group2.first_seen == self.group2.last_seen == self.event2.datetime",
                "184: ",
                "185:         self.group2.status = GroupStatus.RESOLVED",
                "186:         self.group2.substatus = None",
                "187:         self.group2.times_seen = 10",
                "188:         self.group2.update(type=ErrorGroupType.type_id)",
                "189:         self.group2.priority = PriorityLevel.HIGH",
                "190:         self.group2.seer_fixability_score = FixabilityScoreThresholds.MEDIUM.value",
                "191:         self.group2.seer_autofix_last_triggered = self.event2.datetime",
                "192:         self.group2.save()",
                "193:         self.store_group(self.group2)",
                "194: ",
                "195:         GroupBookmark.objects.create(",
                "196:             user_id=self.user.id, group=self.group2, project=self.group2.project",
                "197:         )"
            ]
        },
        {
            "file": "tests/snuba/api/endpoints/test_organization_events_vitals.py",
            "line_number": 27,
            "matched_line": "    def store_event(self, data, measurements=None, **kwargs):",
            "context_start_line": 1,
            "context_end_line": 127,
            "context": [
                "1: from datetime import timedelta",
                "2: ",
                "3: import pytest",
                "4: from django.urls import reverse",
                "5: ",
                "6: from sentry.models.transaction_threshold import ProjectTransactionThreshold, TransactionMetric",
                "7: from sentry.testutils.cases import APITestCase, MetricsEnhancedPerformanceTestCase, SnubaTestCase",
                "8: from sentry.testutils.helpers.datetime import before_now",
                "9: from sentry.utils.samples import load_data",
                "10: ",
                "11: pytestmark = pytest.mark.sentry_metrics",
                "12: ",
                "13: ",
                "14: class OrganizationEventsVitalsEndpointTest(APITestCase, SnubaTestCase):",
                "15:     def setUp(self):",
                "16:         super().setUp()",
                "17:         self.start = before_now(days=1).replace(hour=10, minute=0, second=0, microsecond=0)",
                "18:         self.end = self.start + timedelta(hours=6)",
                "19: ",
                "20:         self.transaction_data = load_data(\"transaction\", timestamp=self.start)",
                "21:         self.query: dict[str, str | list[str]] = {",
                "22:             \"start\": self.start.isoformat(),",
                "23:             \"end\": self.end.isoformat(),",
                "24:         }",
                "25:         self.features = {}",
                "26: ",
                "27:     def store_event(self, data, measurements=None, **kwargs):",
                "28:         if measurements:",
                "29:             for vital, value in measurements.items():",
                "30:                 data[\"measurements\"][vital][\"value\"] = value",
                "31: ",
                "32:         return super().store_event(",
                "33:             data.copy(),",
                "34:             project_id=self.project.id,",
                "35:         )",
                "36: ",
                "37:     def do_request(self, query=None, features=None):",
                "38:         if features is None:",
                "39:             features = {\"organizations:discover-basic\": True}",
                "40:         features.update(self.features)",
                "41:         if query is None:",
                "42:             query = self.query",
                "43: ",
                "44:         self.login_as(user=self.user)",
                "45:         url = reverse(",
                "46:             \"sentry-api-0-organization-events-vitals\",",
                "47:             kwargs={\"organization_id_or_slug\": self.organization.slug},",
                "48:         )",
                "49: ",
                "50:         with self.feature(features):",
                "51:             return self.client.get(url, query, format=\"json\")",
                "52: ",
                "53:     def test_no_projects(self):",
                "54:         response = self.do_request()",
                "55:         assert response.status_code == 200, response.content",
                "56:         assert len(response.data) == 0",
                "57: ",
                "58:     def test_no_vitals(self):",
                "59:         self.store_event(",
                "60:             self.transaction_data,",
                "61:             project_id=self.project.id,",
                "62:         )",
                "63: ",
                "64:         self.query.update({\"vital\": []})",
                "65:         response = self.do_request()",
                "66:         assert response.status_code == 400, response.content",
                "67:         assert \"Need to pass at least one vital\" == response.data[\"detail\"]",
                "68: ",
                "69:     def test_bad_vital(self):",
                "70:         self.store_event(",
                "71:             self.transaction_data,",
                "72:             project_id=self.project.id,",
                "73:         )",
                "74: ",
                "75:         self.query.update({\"vital\": [\"foobar\"]})",
                "76:         response = self.do_request()",
                "77:         assert response.status_code == 400, response.content",
                "78:         assert \"foobar is not a valid vital\" == response.data[\"detail\"]",
                "79: ",
                "80:     def test_simple(self):",
                "81:         data = self.transaction_data.copy()",
                "82:         for lcp in [2000, 3000, 5000]:",
                "83:             self.store_event(",
                "84:                 data,",
                "85:                 {\"lcp\": lcp},",
                "86:                 project_id=self.project.id,",
                "87:             )",
                "88: ",
                "89:         self.query.update({\"vital\": [\"measurements.lcp\"]})",
                "90:         response = self.do_request()",
                "91:         assert response.status_code == 200, response.content",
                "92:         assert not response.data[\"meta\"][\"isMetricsData\"]",
                "93:         assert response.data[\"measurements.lcp\"] == {",
                "94:             \"good\": 1,",
                "95:             \"meh\": 1,",
                "96:             \"poor\": 1,",
                "97:             \"total\": 3,",
                "98:             \"p75\": 4000,",
                "99:         }",
                "100: ",
                "101:     def test_simple_with_refining_user_misery_filter(self):",
                "102:         project1 = self.create_project(organization=self.organization)",
                "103:         project2 = self.create_project(organization=self.organization)",
                "104:         ProjectTransactionThreshold.objects.create(",
                "105:             project=project1,",
                "106:             organization=project1.organization,",
                "107:             threshold=100,",
                "108:             metric=TransactionMetric.LCP.value,",
                "109:         )",
                "110: ",
                "111:         ProjectTransactionThreshold.objects.create(",
                "112:             project=project2,",
                "113:             organization=project2.organization,",
                "114:             threshold=1000,",
                "115:             metric=TransactionMetric.LCP.value,",
                "116:         )",
                "117: ",
                "118:         data = self.transaction_data.copy()",
                "119: ",
                "120:         for project in [project1, project2]:",
                "121:             for lcp in [2000, 3000, 5000]:",
                "122:                 self.store_event(",
                "123:                     data,",
                "124:                     {\"lcp\": lcp},",
                "125:                     project_id=project.id,",
                "126:                 )",
                "127: "
            ]
        },
        {
            "file": "tests/sentry/tasks/test_weekly_reports.py",
            "line_number": 63,
            "matched_line": "    def store_event_outcomes(",
            "context_start_line": 1,
            "context_end_line": 163,
            "context": [
                "1: from datetime import timedelta",
                "2: from typing import cast",
                "3: from unittest import mock",
                "4: ",
                "5: import pytest",
                "6: from django.core import mail",
                "7: from django.core.mail.message import EmailMultiAlternatives",
                "8: from django.db import router",
                "9: from django.db.models import F",
                "10: from django.utils import timezone",
                "11: ",
                "12: from sentry.constants import DataCategory",
                "13: from sentry.issues.grouptype import MonitorIncidentType, PerformanceNPlusOneGroupType",
                "14: from sentry.models.group import GroupStatus",
                "15: from sentry.models.grouphistory import GroupHistoryStatus",
                "16: from sentry.models.organizationmember import OrganizationMember",
                "17: from sentry.models.project import Project",
                "18: from sentry.models.team import TeamStatus",
                "19: from sentry.notifications.models.notificationsettingoption import NotificationSettingOption",
                "20: from sentry.silo.base import SiloMode",
                "21: from sentry.silo.safety import unguarded_write",
                "22: from sentry.snuba.referrer import Referrer",
                "23: from sentry.tasks.summaries.utils import (",
                "24:     ONE_DAY,",
                "25:     OrganizationReportContext,",
                "26:     ProjectContext,",
                "27:     organization_project_issue_substatus_summaries,",
                "28:     project_key_errors,",
                "29:     user_project_ownership,",
                "30: )",
                "31: from sentry.tasks.summaries.weekly_reports import (",
                "32:     OrganizationReportBatch,",
                "33:     group_status_to_color,",
                "34:     prepare_organization_report,",
                "35:     prepare_template_context,",
                "36:     schedule_organizations,",
                "37: )",
                "38: from sentry.testutils.cases import OutcomesSnubaTest, PerformanceIssueTestCase, SnubaTestCase",
                "39: from sentry.testutils.factories import EventType",
                "40: from sentry.testutils.helpers import with_feature",
                "41: from sentry.testutils.helpers.datetime import before_now, freeze_time",
                "42: from sentry.testutils.outbox import outbox_runner",
                "43: from sentry.testutils.silo import assume_test_silo_mode",
                "44: from sentry.types.group import GroupSubStatus",
                "45: from sentry.users.services.user_option import user_option_service",
                "46: from sentry.utils import redis",
                "47: from sentry.utils.dates import floor_to_utc_day",
                "48: from sentry.utils.outcomes import Outcome",
                "49: ",
                "50: DISABLED_ORGANIZATIONS_USER_OPTION_KEY = \"reports:disabled-organizations\"",
                "51: ",
                "52: ",
                "53: class WeeklyReportsTest(OutcomesSnubaTest, SnubaTestCase, PerformanceIssueTestCase):",
                "54:     def setUp(self):",
                "55:         super().setUp()",
                "56:         self.now = timezone.now()",
                "57:         self.timestamp = floor_to_utc_day(self.now).timestamp()",
                "58:         self.two_days_ago = self.now - timedelta(days=2)",
                "59:         self.three_days_ago = self.now - timedelta(days=3)",
                "60: ",
                "61:     _dummy_batch_id = \"20bd6c5b-7fac-4f31-9548-d6f8bb63226d\"",
                "62: ",
                "63:     def store_event_outcomes(",
                "64:         self,",
                "65:         organization_id,",
                "66:         project_id,",
                "67:         timestamp,",
                "68:         num_times,",
                "69:         outcome=Outcome.ACCEPTED,",
                "70:         category=DataCategory.ERROR,",
                "71:     ):",
                "72:         self.store_outcomes(",
                "73:             {",
                "74:                 \"org_id\": organization_id,",
                "75:                 \"project_id\": project_id,",
                "76:                 \"outcome\": outcome,",
                "77:                 \"category\": category,",
                "78:                 \"timestamp\": timestamp,",
                "79:                 \"key_id\": 1,",
                "80:             },",
                "81:             num_times=num_times,",
                "82:         )",
                "83: ",
                "84:     @freeze_time(before_now(days=2).replace(hour=0, minute=0, second=0, microsecond=0))",
                "85:     def test_integration(self):",
                "86:         with unguarded_write(using=router.db_for_write(Project)):",
                "87:             Project.objects.all().delete()",
                "88:         project = self.create_project(",
                "89:             organization=self.organization,",
                "90:             teams=[self.team],",
                "91:             date_added=self.now - timedelta(days=90),",
                "92:         )",
                "93:         member_set = set(project.teams.get().member_set.all())",
                "94:         self.store_event(",
                "95:             data={",
                "96:                 \"timestamp\": before_now(days=1).isoformat(),",
                "97:             },",
                "98:             project_id=project.id,",
                "99:         )",
                "100: ",
                "101:         with self.tasks():",
                "102:             schedule_organizations(timestamp=self.now.timestamp())",
                "103:             assert len(mail.outbox) == len(member_set) == 1",
                "104: ",
                "105:             message = mail.outbox[0]",
                "106:             assert self.organization.name in message.subject",
                "107: ",
                "108:     @freeze_time(before_now(days=2).replace(hour=0, minute=0, second=0, microsecond=0))",
                "109:     def test_with_empty_string_user_option(self):",
                "110:         project = self.create_project(",
                "111:             organization=self.organization,",
                "112:             teams=[self.team],",
                "113:             date_added=self.now - timedelta(days=90),",
                "114:         )",
                "115:         self.store_event(data={\"timestamp\": before_now(days=1).isoformat()}, project_id=project.id)",
                "116:         member_set = set(project.teams.get().member_set.all())",
                "117:         for member in member_set:",
                "118:             # some users have an empty string value set for this key, presumably cleared.",
                "119:             user_option_service.set_option(",
                "120:                 user_id=member.user_id, key=\"reports:disabled-organizations\", value=\"\"",
                "121:             )",
                "122: ",
                "123:         with self.tasks():",
                "124:             schedule_organizations(timestamp=self.now.timestamp())",
                "125:             assert len(mail.outbox) == len(member_set) == 1",
                "126: ",
                "127:             message = mail.outbox[0]",
                "128:             assert self.organization.name in message.subject",
                "129: ",
                "130:     @with_feature(\"system:multi-region\")",
                "131:     @freeze_time(before_now(days=2).replace(hour=0, minute=0, second=0, microsecond=0))",
                "132:     def test_message_links_customer_domains(self):",
                "133:         with unguarded_write(using=router.db_for_write(Project)):",
                "134:             Project.objects.all().delete()",
                "135: ",
                "136:         project = self.create_project(",
                "137:             organization=self.organization,",
                "138:             teams=[self.team],",
                "139:             date_added=self.now - timedelta(days=90),",
                "140:         )",
                "141:         self.store_event(",
                "142:             data={",
                "143:                 \"timestamp\": before_now(days=1).isoformat(),",
                "144:             },",
                "145:             project_id=project.id,",
                "146:         )",
                "147:         with self.tasks():",
                "148:             schedule_organizations(timestamp=self.now.timestamp())",
                "149:             assert len(mail.outbox) == 1",
                "150: ",
                "151:             message = mail.outbox[0]",
                "152:             assert isinstance(message, EmailMultiAlternatives)",
                "153:             assert self.organization.name in message.subject",
                "154:             html = message.alternatives[0][0]",
                "155: ",
                "156:             assert isinstance(html, str)",
                "157:             assert (",
                "158:                 f\"http://{self.organization.slug}.testserver/issues/?referrer=weekly_report\" in html",
                "159:             )",
                "160: ",
                "161:     def _set_option_value(self, value):",
                "162:         with assume_test_silo_mode(SiloMode.CONTROL):",
                "163:             NotificationSettingOption.objects.update_or_create("
            ]
        },
        {
            "file": "tests/sentry/tasks/test_daily_summary.py",
            "line_number": 45,
            "matched_line": "    def store_event_and_outcomes(",
            "context_start_line": 1,
            "context_end_line": 145,
            "context": [
                "1: from datetime import UTC, datetime, timedelta",
                "2: from typing import cast",
                "3: from unittest import mock",
                "4: from urllib.parse import urlencode",
                "5: ",
                "6: import orjson",
                "7: import pytest",
                "8: import responses",
                "9: from django.conf import settings",
                "10: ",
                "11: from sentry.constants import DataCategory",
                "12: from sentry.integrations.types import ExternalProviders",
                "13: from sentry.issues.grouptype import PerformanceNPlusOneGroupType",
                "14: from sentry.models.activity import Activity",
                "15: from sentry.models.group import GroupStatus",
                "16: from sentry.notifications.notifications.daily_summary import DailySummaryNotification",
                "17: from sentry.tasks.summaries.daily_summary import (",
                "18:     build_summary_data,",
                "19:     build_top_projects_map,",
                "20:     deliver_summary,",
                "21:     prepare_summary_data,",
                "22:     schedule_organizations,",
                "23: )",
                "24: from sentry.tasks.summaries.utils import ONE_DAY, DailySummaryProjectContext",
                "25: from sentry.testutils.cases import (",
                "26:     OutcomesSnubaTest,",
                "27:     PerformanceIssueTestCase,",
                "28:     SlackActivityNotificationTest,",
                "29:     SnubaTestCase,",
                "30: )",
                "31: from sentry.testutils.factories import EventType",
                "32: from sentry.testutils.helpers.datetime import before_now, freeze_time",
                "33: from sentry.testutils.helpers.features import with_feature",
                "34: from sentry.types.activity import ActivityType",
                "35: from sentry.types.actor import Actor",
                "36: from sentry.types.group import GroupSubStatus",
                "37: from sentry.users.services.user_option import user_option_service",
                "38: from sentry.utils.outcomes import Outcome",
                "39: ",
                "40: ",
                "41: @freeze_time(before_now(days=2).replace(hour=0, minute=5, second=0, microsecond=0))",
                "42: class DailySummaryTest(",
                "43:     OutcomesSnubaTest, SnubaTestCase, PerformanceIssueTestCase, SlackActivityNotificationTest",
                "44: ):",
                "45:     def store_event_and_outcomes(",
                "46:         self,",
                "47:         project_id,",
                "48:         timestamp,",
                "49:         fingerprint,",
                "50:         category,",
                "51:         release=None,",
                "52:         resolve=True,",
                "53:         level=\"error\",",
                "54:     ):",
                "55:         if category == DataCategory.ERROR:",
                "56:             data = {",
                "57:                 \"timestamp\": timestamp.isoformat(),",
                "58:                 \"fingerprint\": [fingerprint],",
                "59:                 \"level\": level,",
                "60:                 \"exception\": {",
                "61:                     \"values\": [",
                "62:                         {",
                "63:                             \"type\": \"IntegrationError\",",
                "64:                             \"value\": \"Identity not found.\",",
                "65:                         }",
                "66:                     ]",
                "67:                 },",
                "68:             }",
                "69:             if release:",
                "70:                 data[\"release\"] = release",
                "71: ",
                "72:             event = self.store_event(",
                "73:                 data=data,",
                "74:                 project_id=project_id,",
                "75:                 assert_no_errors=False,",
                "76:                 default_event_type=EventType.DEFAULT,",
                "77:             )",
                "78:         elif category == DataCategory.TRANSACTION:",
                "79:             event = self.create_performance_issue()",
                "80: ",
                "81:         self.store_outcomes(",
                "82:             {",
                "83:                 \"org_id\": self.organization.id,",
                "84:                 \"project_id\": project_id,",
                "85:                 \"outcome\": Outcome.ACCEPTED,",
                "86:                 \"category\": category,",
                "87:                 \"timestamp\": timestamp,",
                "88:                 \"key_id\": 1,",
                "89:             },",
                "90:             num_times=1,",
                "91:         )",
                "92: ",
                "93:         group = event.group",
                "94:         if resolve:",
                "95:             group.status = GroupStatus.RESOLVED",
                "96:             group.substatus = None",
                "97:             group.resolved_at = timestamp + timedelta(minutes=1)",
                "98:             group.save()",
                "99:         return group",
                "100: ",
                "101:     def setUp(self):",
                "102:         responses.add_passthru(settings.SENTRY_SNUBA)",
                "103:         super().setUp()",
                "104:         self.now = datetime.now(UTC)",
                "105:         self.two_hours_ago = self.now - timedelta(hours=2)",
                "106:         self.two_days_ago = self.now - timedelta(days=2)",
                "107:         self.three_days_ago = self.now - timedelta(days=3)",
                "108:         self.project.first_event = self.three_days_ago",
                "109:         self.project.save()",
                "110:         self.project2 = self.create_project(",
                "111:             name=\"foo\", organization=self.organization, teams=[self.team]",
                "112:         )",
                "113:         self.project2.first_event = self.three_days_ago",
                "114:         user_option_service.set_option(user_id=self.user.id, key=\"timezone\", value=\"Etc/GMT+8\")",
                "115:         self.release = self.create_release(project=self.project, date_added=self.now)",
                "116: ",
                "117:     def populate_event_data(",
                "118:         self, use_release=True, performance_issues=True, regressed_issue=True, escalated_issue=True",
                "119:     ):",
                "120:         for _ in range(6):",
                "121:             self.group1 = self.store_event_and_outcomes(",
                "122:                 self.project.id,",
                "123:                 self.three_days_ago,",
                "124:                 fingerprint=\"group-1\",",
                "125:                 category=DataCategory.ERROR,",
                "126:                 resolve=False,",
                "127:             )",
                "128:         for _ in range(4):",
                "129:             self.store_event_and_outcomes(",
                "130:                 self.project.id,",
                "131:                 self.two_days_ago,",
                "132:                 fingerprint=\"group-1\",",
                "133:                 category=DataCategory.ERROR,",
                "134:                 resolve=False,",
                "135:             )",
                "136:         for _ in range(3):",
                "137:             self.store_event_and_outcomes(",
                "138:                 self.project.id,",
                "139:                 self.now,",
                "140:                 fingerprint=\"group-1\",",
                "141:                 category=DataCategory.ERROR,",
                "142:                 resolve=False,",
                "143:             )",
                "144: ",
                "145:         # create an issue first seen in the release and set it to regressed"
            ]
        },
        {
            "file": "tests/sentry/utils/sdk_crashes/conftest.py",
            "line_number": 12,
            "matched_line": "def store_event(default_project, factories):",
            "context_start_line": 1,
            "context_end_line": 37,
            "context": [
                "1: import pytest",
                "2: ",
                "3: from sentry.utils.sdk_crashes.path_replacer import FixedPathReplacer",
                "4: from sentry.utils.sdk_crashes.sdk_crash_detection_config import (",
                "5:     SDKCrashDetectionConfig,",
                "6:     SDKFrameConfig,",
                "7:     SdkName,",
                "8: )",
                "9: ",
                "10: ",
                "11: @pytest.fixture",
                "12: def store_event(default_project, factories):",
                "13:     def inner(data):",
                "14:         return factories.store_event(data=data, project_id=default_project.id)",
                "15: ",
                "16:     return inner",
                "17: ",
                "18: ",
                "19: @pytest.fixture",
                "20: def empty_cocoa_config() -> SDKCrashDetectionConfig:",
                "21:     return SDKCrashDetectionConfig(",
                "22:         sdk_name=SdkName.Cocoa,",
                "23:         project_id=0,",
                "24:         sample_rate=0.0,",
                "25:         organization_allowlist=[],",
                "26:         sdk_names={},",
                "27:         report_fatal_errors=False,",
                "28:         ignore_mechanism_type=set(),",
                "29:         allow_mechanism_type=set(),",
                "30:         system_library_path_patterns=set(),",
                "31:         sdk_frame_config=SDKFrameConfig(",
                "32:             function_patterns=set(),",
                "33:             path_patterns=set(),",
                "34:             path_replacer=FixedPathReplacer(path=\"\"),",
                "35:         ),",
                "36:         sdk_crash_ignore_matchers=set(),",
                "37:     )"
            ]
        }
    ],
    "total": 5
}

Message 11 [Assistant]:
--------------------------------------------------------------------------------
Content:
让我搜索 Factories 类中的 store_event 方法：
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "class Factories:",
      "is_regex": false,
      "context_lines": 200
    },
    "id": "call_00_UI2QGCnoTn5HRAcv2ZUJGNBR",
    "type": "tool_call"
  }
]

Message 12 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_UI2QGCnoTn5HRAcv2ZUJGNBR
Result:
{
    "matches": [
        {
            "file": "src/sentry/testutils/factories.py",
            "line_number": 361,
            "matched_line": "class Factories:",
            "context_start_line": 161,
            "context_end_line": 561,
            "context": [
                "161: )",
                "162: from sentry.uptime.types import UptimeMonitorMode",
                "163: from sentry.users.models.identity import Identity, IdentityProvider, IdentityStatus",
                "164: from sentry.users.models.user import User",
                "165: from sentry.users.models.user_avatar import UserAvatar",
                "166: from sentry.users.models.user_option import UserOption",
                "167: from sentry.users.models.useremail import UserEmail",
                "168: from sentry.users.models.userpermission import UserPermission",
                "169: from sentry.users.models.userrole import UserRole",
                "170: from sentry.users.services.user import RpcUser",
                "171: from sentry.utils import loremipsum",
                "172: from sentry.workflow_engine.models import (",
                "173:     Action,",
                "174:     ActionAlertRuleTriggerAction,",
                "175:     AlertRuleDetector,",
                "176:     AlertRuleWorkflow,",
                "177:     DataCondition,",
                "178:     DataConditionGroup,",
                "179:     DataConditionGroupAction,",
                "180:     DataSource,",
                "181:     DataSourceDetector,",
                "182:     Detector,",
                "183:     DetectorState,",
                "184:     DetectorWorkflow,",
                "185:     IncidentGroupOpenPeriod,",
                "186:     Workflow,",
                "187:     WorkflowDataConditionGroup,",
                "188: )",
                "189: from sentry.workflow_engine.registry import data_source_type_registry",
                "190: from social_auth.models import UserSocialAuth",
                "191: ",
                "192: ",
                "193: class EventType(Enum):",
                "194:     ERROR = \"error\"",
                "195:     DEFAULT = \"default\"",
                "196: ",
                "197: ",
                "198: def get_fixture_path(*parts: str) -> str:",
                "199:     path = os.path.realpath(__file__)",
                "200:     for _ in range(4):  # src/sentry/testutils/{__file__}",
                "201:         path = os.path.dirname(path)",
                "202:     return os.path.join(path, \"fixtures\", *parts)",
                "203: ",
                "204: ",
                "205: def make_sentence(words=None):",
                "206:     if words is None:",
                "207:         words = int(random.weibullvariate(8, 3))",
                "208:     return \" \".join(random.choice(loremipsum.words) for _ in range(words))",
                "209: ",
                "210: ",
                "211: def make_word(words=None):",
                "212:     if words is None:",
                "213:         words = int(random.weibullvariate(8, 3))",
                "214:     return random.choice(loremipsum.words)",
                "215: ",
                "216: ",
                "217: DEFAULT_EVENT_DATA = {",
                "218:     \"extra\": {",
                "219:         \"loadavg\": [0.97607421875, 0.88330078125, 0.833984375],",
                "220:         \"sys.argv\": [",
                "221:             \"/Users/dcramer/.virtualenvs/sentry/bin/raven\",",
                "222:             \"test\",",
                "223:             \"https://ebc35f33e151401f9deac549978bda11:f3403f81e12e4c24942d505f086b2cad@sentry.io/1\",",
                "224:         ],",
                "225:         \"user\": \"dcramer\",",
                "226:     },",
                "227:     \"modules\": {\"raven\": \"3.1.13\"},",
                "228:     \"request\": {",
                "229:         \"cookies\": {},",
                "230:         \"data\": {},",
                "231:         \"env\": {},",
                "232:         \"headers\": {},",
                "233:         \"method\": \"GET\",",
                "234:         \"query_string\": \"\",",
                "235:         \"url\": \"http://example.com\",",
                "236:     },",
                "237:     \"stacktrace\": {",
                "238:         \"frames\": [",
                "239:             {",
                "240:                 \"abs_path\": \"www/src/sentry/models/foo.py\",",
                "241:                 \"context_line\": \"                        string_max_length=self.string_max_length)\",",
                "242:                 \"filename\": \"sentry/models/foo.py\",",
                "243:                 \"function\": \"build_msg\",",
                "244:                 \"in_app\": True,",
                "245:                 \"lineno\": 29,",
                "246:                 \"module\": \"raven.base\",",
                "247:                 \"post_context\": [",
                "248:                     \"                },\",",
                "249:                     \"            })\",",
                "250:                     \"\",",
                "251:                     \"        if 'stacktrace' in data:\",",
                "252:                     \"            if self.include_paths:\",",
                "253:                 ],",
                "254:                 \"pre_context\": [",
                "255:                     \"\",",
                "256:                     \"            data.update({\",",
                "257:                     \"                'stacktrace': {\",",
                "258:                     \"                    'frames': get_stack_info(frames,\",",
                "259:                     \"                        list_max_length=self.list_max_length,\",",
                "260:                 ],",
                "261:                 \"vars\": {",
                "262:                     \"culprit\": \"raven.scripts.runner\",",
                "263:                     \"date\": \"datetime.datetime(2013, 2, 14, 20, 6, 33, 479471)\",",
                "264:                     \"event_id\": \"598fb19363e745ec8be665e6ba88b1b2\",",
                "265:                     \"event_type\": \"raven.events.Message\",",
                "266:                     \"frames\": \"<generator object iter_stack_frames at 0x103fef050>\",",
                "267:                     \"handler\": \"<raven.events.Message object at 0x103feb710>\",",
                "268:                     \"k\": \"logentry\",",
                "269:                     \"public_key\": None,",
                "270:                     \"result\": {",
                "271:                         \"logentry\": \"{'message': 'This is a test message generated using ``raven test``', 'params': []}\"",
                "272:                     },",
                "273:                     \"self\": \"<raven.base.Client object at 0x104397f10>\",",
                "274:                     \"stack\": True,",
                "275:                     \"tags\": None,",
                "276:                     \"time_spent\": None,",
                "277:                 },",
                "278:             },",
                "279:             {",
                "280:                 \"abs_path\": \"/Users/dcramer/.virtualenvs/sentry/lib/python2.7/site-packages/raven/base.py\",",
                "281:                 \"context_line\": \"                        string_max_length=self.string_max_length)\",",
                "282:                 \"filename\": \"raven/base.py\",",
                "283:                 \"function\": \"build_msg\",",
                "284:                 \"in_app\": False,",
                "285:                 \"lineno\": 290,",
                "286:                 \"module\": \"raven.base\",",
                "287:                 \"post_context\": [",
                "288:                     \"                },\",",
                "289:                     \"            })\",",
                "290:                     \"\",",
                "291:                     \"        if 'stacktrace' in data:\",",
                "292:                     \"            if self.include_paths:\",",
                "293:                 ],",
                "294:                 \"pre_context\": [",
                "295:                     \"\",",
                "296:                     \"            data.update({\",",
                "297:                     \"                'stacktrace': {\",",
                "298:                     \"                    'frames': get_stack_info(frames,\",",
                "299:                     \"                        list_max_length=self.list_max_length,\",",
                "300:                 ],",
                "301:                 \"vars\": {",
                "302:                     \"culprit\": \"raven.scripts.runner\",",
                "303:                     \"date\": \"datetime.datetime(2013, 2, 14, 20, 6, 33, 479471)\",",
                "304:                     \"event_id\": \"598fb19363e745ec8be665e6ba88b1b2\",",
                "305:                     \"event_type\": \"raven.events.Message\",",
                "306:                     \"frames\": \"<generator object iter_stack_frames at 0x103fef050>\",",
                "307:                     \"handler\": \"<raven.events.Message object at 0x103feb710>\",",
                "308:                     \"k\": \"logentry\",",
                "309:                     \"public_key\": None,",
                "310:                     \"result\": {",
                "311:                         \"logentry\": \"{'message': 'This is a test message generated using ``raven test``', 'params': []}\"",
                "312:                     },",
                "313:                     \"self\": \"<raven.base.Client object at 0x104397f10>\",",
                "314:                     \"stack\": True,",
                "315:                     \"tags\": None,",
                "316:                     \"time_spent\": None,",
                "317:                 },",
                "318:             },",
                "319:         ]",
                "320:     },",
                "321:     \"tags\": [],",
                "322:     \"platform\": \"python\",",
                "323: }",
                "324: ",
                "325: default_detector_config_data = {",
                "326:     MetricIssue.slug: {\"threshold_period\": 1, \"detection_type\": \"static\"}",
                "327: }",
                "328: ",
                "329: ",
                "330: def _patch_artifact_manifest(path, org=None, release=None, project=None, extra_files=None):",
                "331:     with open(path, \"rb\") as fp:",
                "332:         manifest = orjson.loads(fp.read())",
                "333:     if org:",
                "334:         manifest[\"org\"] = org",
                "335:     if release:",
                "336:         manifest[\"release\"] = release",
                "337:     if project:",
                "338:         manifest[\"project\"] = project",
                "339:     for path in extra_files or {}:",
                "340:         manifest[\"files\"][path] = {\"url\": path}",
                "341:     return orjson.dumps(manifest).decode()",
                "342: ",
                "343: ",
                "344: def _set_sample_rate_from_error_sampling(normalized_data: MutableMapping[str, Any]) -> None:",
                "345:     \"\"\"Set 'sample_rate' on normalized_data if contexts.error_sampling.client_sample_rate is present and valid.\"\"\"",
                "346:     client_sample_rate = None",
                "347:     try:",
                "348:         client_sample_rate = (",
                "349:             normalized_data.get(\"contexts\", {}).get(\"error_sampling\", {}).get(\"client_sample_rate\")",
                "350:         )",
                "351:     except Exception:",
                "352:         pass",
                "353:     if client_sample_rate:",
                "354:         try:",
                "355:             normalized_data[\"sample_rate\"] = float(client_sample_rate)",
                "356:         except Exception:",
                "357:             pass",
                "358: ",
                "359: ",
                "360: # TODO(dcramer): consider moving to something more scalable like factoryboy",
                "361: class Factories:",
                "362:     @staticmethod",
                "363:     @assume_test_silo_mode(SiloMode.REGION)",
                "364:     def create_organization(name=None, owner=None, region: Region | str | None = None, **kwargs):",
                "365:         if not name:",
                "366:             name = petname.generate(2, \" \", letters=10).title()",
                "367: ",
                "368:         with contextlib.ExitStack() as ctx:",
                "369:             if region is None or SiloMode.get_current_mode() == SiloMode.MONOLITH:",
                "370:                 region_name = get_local_region().name",
                "371:             else:",
                "372:                 if isinstance(region, Region):",
                "373:                     region_name = region.name",
                "374:                 else:",
                "375:                     region_obj = get_region_by_name(region)  # Verify it exists",
                "376:                     region_name = region_obj.name",
                "377: ",
                "378:                 ctx.enter_context(",
                "379:                     override_settings(SILO_MODE=SiloMode.REGION, SENTRY_REGION=region_name)",
                "380:                 )",
                "381: ",
                "382:             with outbox_context(flush=False):",
                "383:                 org = Organization.objects.create(name=name, **kwargs)",
                "384: ",
                "385:             with assume_test_silo_mode(SiloMode.CONTROL):",
                "386:                 # Organization mapping creation relies on having a matching org slug reservation",
                "387:                 OrganizationSlugReservation(",
                "388:                     organization_id=org.id,",
                "389:                     region_name=region_name,",
                "390:                     user_id=owner.id if owner else -1,",
                "391:                     slug=org.slug,",
                "392:                 ).save(unsafe_write=True)",
                "393: ",
                "394:             # Manually replicate org data after adding an org slug reservation",
                "395:             org.handle_async_replication(org.id)",
                "396: ",
                "397:             # Flush remaining organization update outboxes accumulated by org create",
                "398:             RegionOutbox(",
                "399:                 shard_identifier=org.id,",
                "400:                 shard_scope=OutboxScope.ORGANIZATION_SCOPE,",
                "401:                 category=OutboxCategory.ORGANIZATION_UPDATE,",
                "402:             ).drain_shard()",
                "403: ",
                "404:         if owner:",
                "405:             Factories.create_member(organization=org, user_id=owner.id, role=\"owner\")",
                "406:         return org",
                "407: ",
                "408:     @staticmethod",
                "409:     @assume_test_silo_mode(SiloMode.CONTROL)",
                "410:     def create_org_mapping(org=None, **kwds):",
                "411:         if org:",
                "412:             kwds.setdefault(\"organization_id\", org.id)",
                "413:             kwds.setdefault(\"slug\", org.slug)",
                "414:             kwds.setdefault(\"name\", org.name)",
                "415:             kwds.setdefault(\"idempotency_key\", uuid4().hex)",
                "416:             kwds.setdefault(\"region_name\", \"na\")",
                "417:         return OrganizationMapping.objects.create(**kwds)",
                "418: ",
                "419:     @staticmethod",
                "420:     @assume_test_silo_mode(SiloMode.REGION)",
                "421:     def create_member(teams=None, team_roles=None, **kwargs):",
                "422:         kwargs.setdefault(\"role\", \"member\")",
                "423:         teamRole = kwargs.pop(\"teamRole\", None)",
                "424: ",
                "425:         # user_id will have precedence over user",
                "426:         user = kwargs.pop(\"user\", None)",
                "427:         user_id = kwargs.pop(\"user_id\", None)",
                "428:         if not user_id and user:",
                "429:             user_id = user.id",
                "430:         kwargs[\"user_id\"] = user_id",
                "431: ",
                "432:         # inviter_id will have precedence over inviter",
                "433:         inviter = kwargs.pop(\"inviter\", None)",
                "434:         inviter_id = kwargs.pop(\"inviter_id\", None)",
                "435:         if not inviter_id and inviter:",
                "436:             inviter_id = inviter.id",
                "437:         kwargs[\"inviter_id\"] = inviter_id",
                "438: ",
                "439:         om = OrganizationMember.objects.create(**kwargs)",
                "440: ",
                "441:         if team_roles:",
                "442:             for team, role in team_roles:",
                "443:                 Factories.create_team_membership(team=team, member=om, role=role)",
                "444:         elif teams:",
                "445:             for team in teams:",
                "446:                 Factories.create_team_membership(team=team, member=om, role=teamRole)",
                "447:         return om",
                "448: ",
                "449:     @staticmethod",
                "450:     @assume_test_silo_mode(SiloMode.REGION)",
                "451:     def create_member_invite(",
                "452:         organization: Organization | None = None,",
                "453:         email: str | None = None,",
                "454:         **kwargs,",
                "455:     ) -> OrganizationMemberInvite:",
                "456:         if organization is None:",
                "457:             organization = Factories.create_organization()",
                "458:         if email is None:",
                "459:             email = f\"{petname.generate().title()}@email.com\"",
                "460:         om = OrganizationMember.objects.create(organization=organization)",
                "461:         return OrganizationMemberInvite.objects.create(",
                "462:             organization=organization, organization_member_id=om.id, email=email, **kwargs",
                "463:         )",
                "464: ",
                "465:     @staticmethod",
                "466:     @assume_test_silo_mode(SiloMode.REGION)",
                "467:     def create_team_membership(team, member=None, user=None, role=None):",
                "468:         if member is None:",
                "469:             member, created = OrganizationMember.objects.get_or_create(",
                "470:                 user_id=user.id if user else None,",
                "471:                 organization=team.organization,",
                "472:                 defaults={\"role\": \"member\"},",
                "473:             )",
                "474: ",
                "475:         return OrganizationMemberTeam.objects.create(",
                "476:             team=team, organizationmember=member, is_active=True, role=role",
                "477:         )",
                "478: ",
                "479:     @staticmethod",
                "480:     @assume_test_silo_mode(SiloMode.CONTROL)",
                "481:     def create_api_key(organization, **kwargs) -> ApiKey:",
                "482:         return ApiKey.objects.create(organization_id=organization.id, **kwargs)",
                "483: ",
                "484:     @staticmethod",
                "485:     @assume_test_silo_mode(SiloMode.CONTROL)",
                "486:     def create_auth_provider(**kwargs):",
                "487:         return AuthProvider.objects.create(**kwargs)",
                "488: ",
                "489:     @staticmethod",
                "490:     @assume_test_silo_mode(SiloMode.CONTROL)",
                "491:     def create_auth_identity(**kwargs):",
                "492:         return AuthIdentity.objects.create(**kwargs)",
                "493: ",
                "494:     @staticmethod",
                "495:     @assume_test_silo_mode(SiloMode.CONTROL)",
                "496:     def create_user_auth_token(user, scope_list: list[str] | None = None, **kwargs) -> ApiToken:",
                "497:         if scope_list is None:",
                "498:             scope_list = []",
                "499:         return ApiToken.objects.create(",
                "500:             user=user,",
                "501:             scope_list=scope_list,",
                "502:             token_type=AuthTokenType.USER,",
                "503:             **kwargs,",
                "504:         )",
                "505: ",
                "506:     @staticmethod",
                "507:     @assume_test_silo_mode(SiloMode.CONTROL)",
                "508:     def create_org_auth_token(*args, **kwargs) -> OrgAuthToken:",
                "509:         return OrgAuthToken.objects.create(*args, **kwargs)",
                "510: ",
                "511:     @staticmethod",
                "512:     @assume_test_silo_mode(SiloMode.REGION)",
                "513:     def create_team(organization, **kwargs):",
                "514:         if not kwargs.get(\"name\"):",
                "515:             kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()",
                "516:         if not kwargs.get(\"slug\"):",
                "517:             kwargs[\"slug\"] = slugify(str(kwargs[\"name\"]))",
                "518:         members = kwargs.pop(\"members\", None)",
                "519: ",
                "520:         team = Team.objects.create(organization=organization, **kwargs)",
                "521:         if members:",
                "522:             for user in members:",
                "523:                 Factories.create_team_membership(team=team, user=user)",
                "524:         return team",
                "525: ",
                "526:     @staticmethod",
                "527:     @assume_test_silo_mode(SiloMode.REGION)",
                "528:     def create_environment(project, **kwargs):",
                "529:         name = kwargs.get(\"name\", petname.generate(3, \" \", letters=10)[:64])",
                "530: ",
                "531:         organization = kwargs.get(\"organization\")",
                "532:         organization_id = organization.id if organization else project.organization_id",
                "533: ",
                "534:         env = Environment.objects.create(organization_id=organization_id, name=name)",
                "535:         env.add_project(project, is_hidden=kwargs.get(\"is_hidden\"))",
                "536:         return env",
                "537: ",
                "538:     @staticmethod",
                "539:     @assume_test_silo_mode(SiloMode.REGION)",
                "540:     def create_project(",
                "541:         organization=None, teams=None, fire_project_created=False, **kwargs",
                "542:     ) -> Project:",
                "543:         if not kwargs.get(\"name\"):",
                "544:             kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()",
                "545:         if not kwargs.get(\"slug\"):",
                "546:             kwargs[\"slug\"] = slugify(str(kwargs[\"name\"]))",
                "547:         if not organization and teams:",
                "548:             organization = teams[0].organization",
                "549: ",
                "550:         with transaction.atomic(router.db_for_write(Project)):",
                "551:             project = Project.objects.create(organization=organization, **kwargs)",
                "552:             if teams:",
                "553:                 for team in teams:",
                "554:                     project.add_team(team)",
                "555:             if fire_project_created:",
                "556:                 project_created.send(",
                "557:                     project=project, user=AnonymousUser(), default_rules=True, sender=Factories",
                "558:                 )",
                "559:         return project",
                "560: ",
                "561:     @staticmethod"
            ]
        }
    ],
    "total": 1
}

Message 13 [Assistant]:
--------------------------------------------------------------------------------
Content:
让我查看 Factories 类中的 store_event 方法：
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "def store_event",
      "is_regex": false,
      "context_lines": 150,
      "max_results": 3
    },
    "id": "call_00_RtS36d7ZW1eDCvS4ALGiGJlW",
    "type": "tool_call"
  }
]

Message 14 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_RtS36d7ZW1eDCvS4ALGiGJlW
Result:
{
    "matches": [
        {
            "file": "tests/snuba/search/test_backend.py",
            "line_number": 97,
            "matched_line": "    def store_event(self, data, *args, **kwargs):",
            "context_start_line": 1,
            "context_end_line": 247,
            "context": [
                "1: import time",
                "2: import uuid",
                "3: from datetime import UTC, datetime, timedelta",
                "4: from typing import Any",
                "5: from unittest import mock",
                "6: ",
                "7: import pytest",
                "8: from django.utils import timezone",
                "9: from sentry_kafka_schemas.schema_types.group_attributes_v1 import GroupAttributesSnapshot",
                "10: ",
                "11: from sentry import options",
                "12: from sentry.api.issue_search import convert_query_values, issue_search_config, parse_search_query",
                "13: from sentry.exceptions import InvalidSearchQuery",
                "14: from sentry.grouping.grouptype import ErrorGroupType",
                "15: from sentry.incidents.grouptype import MetricIssue",
                "16: from sentry.issues.grouptype import (",
                "17:     FeedbackGroup,",
                "18:     NoiseConfig,",
                "19:     PerformanceNPlusOneGroupType,",
                "20:     PerformanceRenderBlockingAssetSpanGroupType,",
                "21: )",
                "22: from sentry.issues.ingest import send_issue_occurrence_to_eventstream",
                "23: from sentry.models.environment import Environment",
                "24: from sentry.models.group import Group, GroupStatus",
                "25: from sentry.models.groupassignee import GroupAssignee",
                "26: from sentry.models.groupbookmark import GroupBookmark",
                "27: from sentry.models.groupenvironment import GroupEnvironment",
                "28: from sentry.models.grouphistory import GroupHistoryStatus, record_group_history",
                "29: from sentry.models.groupowner import GroupOwner",
                "30: from sentry.models.groupsubscription import GroupSubscription",
                "31: from sentry.search.snuba.backend import EventsDatasetSnubaSearchBackend, SnubaSearchBackendBase",
                "32: from sentry.search.snuba.executors import TrendsSortWeights",
                "33: from sentry.seer.seer_utils import FixabilityScoreThresholds",
                "34: from sentry.snuba.dataset import Dataset",
                "35: from sentry.testutils.cases import SnubaTestCase, TestCase, TransactionTestCase",
                "36: from sentry.testutils.helpers import Feature, apply_feature_flag_on_cls",
                "37: from sentry.testutils.helpers.datetime import before_now",
                "38: from sentry.types.group import GroupSubStatus, PriorityLevel",
                "39: from sentry.utils import json",
                "40: from sentry.utils.snuba import SENTRY_SNUBA_MAP",
                "41: from tests.sentry.issues.test_utils import OccurrenceTestMixin",
                "42: ",
                "43: ",
                "44: def date_to_query_format(date):",
                "45:     return date.strftime(\"%Y-%m-%dT%H:%M:%S\")",
                "46: ",
                "47: ",
                "48: class SharedSnubaMixin(SnubaTestCase):",
                "49:     @property",
                "50:     def backend(self) -> SnubaSearchBackendBase:",
                "51:         raise NotImplementedError(self)",
                "52: ",
                "53:     def build_search_filter(self, query, projects=None, user=None, environments=None):",
                "54:         user = user if user is not None else self.user",
                "55:         projects = projects if projects is not None else [self.project]",
                "56:         return convert_query_values(parse_search_query(query), projects, user, environments)",
                "57: ",
                "58:     def make_query(",
                "59:         self,",
                "60:         projects=None,",
                "61:         search_filter_query=None,",
                "62:         user=None,",
                "63:         environments=None,",
                "64:         sort_by=\"date\",",
                "65:         limit=None,",
                "66:         count_hits=False,",
                "67:         date_from=None,",
                "68:         date_to=None,",
                "69:         cursor=None,",
                "70:         aggregate_kwargs=None,",
                "71:     ):",
                "72:         search_filters = []",
                "73:         projects = projects if projects is not None else [self.project]",
                "74:         if search_filter_query is not None:",
                "75:             search_filters = self.build_search_filter(",
                "76:                 search_filter_query, projects, user=user, environments=environments",
                "77:             )",
                "78: ",
                "79:         kwargs = {}",
                "80:         if limit is not None:",
                "81:             kwargs[\"limit\"] = limit",
                "82:         if aggregate_kwargs:",
                "83:             kwargs[\"aggregate_kwargs\"] = {\"trends\": {**aggregate_kwargs}}",
                "84: ",
                "85:         return self.backend.query(",
                "86:             projects,",
                "87:             search_filters=search_filters,",
                "88:             environments=environments,",
                "89:             count_hits=count_hits,",
                "90:             sort_by=sort_by,",
                "91:             date_from=date_from,",
                "92:             date_to=date_to,",
                "93:             cursor=cursor,",
                "94:             **kwargs,",
                "95:         )",
                "96: ",
                "97:     def store_event(self, data, *args, **kwargs):",
                "98:         event = super().store_event(data, *args, **kwargs)",
                "99:         environment_name = data.get(\"environment\")",
                "100:         if environment_name:",
                "101:             GroupEnvironment.objects.filter(",
                "102:                 group_id=event.group_id,",
                "103:                 environment__name=environment_name,",
                "104:                 first_seen__gt=event.datetime,",
                "105:             ).update(first_seen=event.datetime)",
                "106:         return event",
                "107: ",
                "108: ",
                "109: class EventsDatasetTestSetup(SharedSnubaMixin):",
                "110:     @property",
                "111:     def backend(self):",
                "112:         return EventsDatasetSnubaSearchBackend()",
                "113: ",
                "114:     def setUp(self):",
                "115:         super().setUp()",
                "116:         self.base_datetime = before_now(days=3).replace(microsecond=0)",
                "117: ",
                "118:         event1_timestamp = (self.base_datetime - timedelta(days=21)).isoformat()",
                "119:         self.event1 = self.store_event(",
                "120:             data={",
                "121:                 \"fingerprint\": [\"put-me-in-group1\"],",
                "122:                 \"event_id\": \"a\" * 32,",
                "123:                 \"message\": \"foo. Indeed, this message is intended to be greater than 256 characters such that we can put this unique string identifier after that point in the string. The purpose of this is in order to verify we are using snuba to search messages instead of Postgres (postgres truncates at 256 characters and clickhouse does not). santryrox.\",",
                "124:                 \"environment\": \"production\",",
                "125:                 \"tags\": {\"server\": \"example.com\", \"sentry:user\": \"event1@example.com\"},",
                "126:                 \"timestamp\": event1_timestamp,",
                "127:                 \"stacktrace\": {\"frames\": [{\"module\": \"group1\"}]},",
                "128:                 \"level\": \"fatal\",",
                "129:             },",
                "130:             project_id=self.project.id,",
                "131:         )",
                "132:         self.event3 = self.store_event(",
                "133:             data={",
                "134:                 \"fingerprint\": [\"put-me-in-group1\"],",
                "135:                 \"event_id\": \"c\" * 32,",
                "136:                 \"message\": \"group1\",",
                "137:                 \"environment\": \"production\",",
                "138:                 \"tags\": {\"server\": \"example.com\", \"sentry:user\": \"event3@example.com\"},",
                "139:                 \"timestamp\": self.base_datetime.isoformat(),",
                "140:                 \"stacktrace\": {\"frames\": [{\"module\": \"group1\"}]},",
                "141:                 \"level\": \"fatal\",",
                "142:             },",
                "143:             project_id=self.project.id,",
                "144:         )",
                "145: ",
                "146:         self.group1 = Group.objects.get(id=self.event1.group.id)",
                "147:         assert self.group1.id == self.event1.group.id",
                "148:         assert self.group1.id == self.event3.group.id",
                "149: ",
                "150:         assert self.group1.first_seen == self.event1.datetime",
                "151:         assert self.group1.last_seen == self.event3.datetime",
                "152: ",
                "153:         self.group1.times_seen = 5",
                "154:         self.group1.status = GroupStatus.UNRESOLVED",
                "155:         self.group1.substatus = GroupSubStatus.ONGOING",
                "156:         self.group1.priority = PriorityLevel.HIGH",
                "157:         self.group1.update(type=ErrorGroupType.type_id)",
                "158:         self.group1.seer_fixability_score = FixabilityScoreThresholds.HIGH.value + 0.01",
                "159:         self.group1.seer_autofix_last_triggered = self.event3.datetime",
                "160:         self.group1.save()",
                "161:         self.store_group(self.group1)",
                "162: ",
                "163:         self.event2 = self.store_event(",
                "164:             data={",
                "165:                 \"fingerprint\": [\"put-me-in-group2\"],",
                "166:                 \"event_id\": \"b\" * 32,",
                "167:                 \"timestamp\": (self.base_datetime - timedelta(days=20)).isoformat(),",
                "168:                 \"message\": \"bar\",",
                "169:                 \"stacktrace\": {\"frames\": [{\"module\": \"group2\"}]},",
                "170:                 \"environment\": \"staging\",",
                "171:                 \"tags\": {",
                "172:                     \"server\": \"example.com\",",
                "173:                     \"url\": \"http://example.com\",",
                "174:                     \"sentry:user\": \"event2@example.com\",",
                "175:                 },",
                "176:                 \"level\": \"error\",",
                "177:             },",
                "178:             project_id=self.project.id,",
                "179:         )",
                "180: ",
                "181:         self.group2 = Group.objects.get(id=self.event2.group.id)",
                "182:         assert self.group2.id == self.event2.group.id",
                "183:         assert self.group2.first_seen == self.group2.last_seen == self.event2.datetime",
                "184: ",
                "185:         self.group2.status = GroupStatus.RESOLVED",
                "186:         self.group2.substatus = None",
                "187:         self.group2.times_seen = 10",
                "188:         self.group2.update(type=ErrorGroupType.type_id)",
                "189:         self.group2.priority = PriorityLevel.HIGH",
                "190:         self.group2.seer_fixability_score = FixabilityScoreThresholds.MEDIUM.value",
                "191:         self.group2.seer_autofix_last_triggered = self.event2.datetime",
                "192:         self.group2.save()",
                "193:         self.store_group(self.group2)",
                "194: ",
                "195:         GroupBookmark.objects.create(",
                "196:             user_id=self.user.id, group=self.group2, project=self.group2.project",
                "197:         )",
                "198: ",
                "199:         GroupAssignee.objects.create(",
                "200:             user_id=self.user.id, group=self.group2, project=self.group2.project",
                "201:         )",
                "202: ",
                "203:         GroupSubscription.objects.create(",
                "204:             user_id=self.user.id, group=self.group1, project=self.group1.project, is_active=True",
                "205:         )",
                "206: ",
                "207:         GroupSubscription.objects.create(",
                "208:             user_id=self.user.id, group=self.group2, project=self.group2.project, is_active=False",
                "209:         )",
                "210: ",
                "211:         self.environments = {",
                "212:             \"production\": self.event1.get_environment(),",
                "213:             \"staging\": self.event2.get_environment(),",
                "214:         }",
                "215: ",
                "216:     def set_up_multi_project(self):",
                "217:         self.project2 = self.create_project(organization=self.project.organization)",
                "218:         self.event_p2 = self.store_event(",
                "219:             data={",
                "220:                 \"event_id\": \"a\" * 32,",
                "221:                 \"fingerprint\": [\"put-me-in-groupP2\"],",
                "222:                 \"timestamp\": (self.base_datetime - timedelta(days=21)).isoformat(),",
                "223:                 \"message\": \"foo\",",
                "224:                 \"stacktrace\": {\"frames\": [{\"module\": \"group_p2\"}]},",
                "225:                 \"tags\": {\"server\": \"example.com\"},",
                "226:                 \"environment\": \"production\",",
                "227:             },",
                "228:             project_id=self.project2.id,",
                "229:         )",
                "230: ",
                "231:         self.group_p2 = Group.objects.get(id=self.event_p2.group.id)",
                "232:         self.group_p2.times_seen = 6",
                "233:         self.group_p2.last_seen = self.base_datetime - timedelta(days=1)",
                "234:         self.group_p2.save()",
                "235:         self.store_group(self.group_p2)",
                "236: ",
                "237:     def create_group_with_integration_external_issue(self, environment=\"production\"):",
                "238:         event = self.store_event(",
                "239:             data={",
                "240:                 \"fingerprint\": [\"linked_group1\"],",
                "241:                 \"event_id\": uuid.uuid4().hex,",
                "242:                 \"timestamp\": self.base_datetime.isoformat(),",
                "243:                 \"environment\": environment,",
                "244:             },",
                "245:             project_id=self.project.id,",
                "246:         )",
                "247:         integration, _ = self.create_provider_integration_for("
            ]
        },
        {
            "file": "tests/snuba/api/endpoints/test_organization_events_vitals.py",
            "line_number": 27,
            "matched_line": "    def store_event(self, data, measurements=None, **kwargs):",
            "context_start_line": 1,
            "context_end_line": 177,
            "context": [
                "1: from datetime import timedelta",
                "2: ",
                "3: import pytest",
                "4: from django.urls import reverse",
                "5: ",
                "6: from sentry.models.transaction_threshold import ProjectTransactionThreshold, TransactionMetric",
                "7: from sentry.testutils.cases import APITestCase, MetricsEnhancedPerformanceTestCase, SnubaTestCase",
                "8: from sentry.testutils.helpers.datetime import before_now",
                "9: from sentry.utils.samples import load_data",
                "10: ",
                "11: pytestmark = pytest.mark.sentry_metrics",
                "12: ",
                "13: ",
                "14: class OrganizationEventsVitalsEndpointTest(APITestCase, SnubaTestCase):",
                "15:     def setUp(self):",
                "16:         super().setUp()",
                "17:         self.start = before_now(days=1).replace(hour=10, minute=0, second=0, microsecond=0)",
                "18:         self.end = self.start + timedelta(hours=6)",
                "19: ",
                "20:         self.transaction_data = load_data(\"transaction\", timestamp=self.start)",
                "21:         self.query: dict[str, str | list[str]] = {",
                "22:             \"start\": self.start.isoformat(),",
                "23:             \"end\": self.end.isoformat(),",
                "24:         }",
                "25:         self.features = {}",
                "26: ",
                "27:     def store_event(self, data, measurements=None, **kwargs):",
                "28:         if measurements:",
                "29:             for vital, value in measurements.items():",
                "30:                 data[\"measurements\"][vital][\"value\"] = value",
                "31: ",
                "32:         return super().store_event(",
                "33:             data.copy(),",
                "34:             project_id=self.project.id,",
                "35:         )",
                "36: ",
                "37:     def do_request(self, query=None, features=None):",
                "38:         if features is None:",
                "39:             features = {\"organizations:discover-basic\": True}",
                "40:         features.update(self.features)",
                "41:         if query is None:",
                "42:             query = self.query",
                "43: ",
                "44:         self.login_as(user=self.user)",
                "45:         url = reverse(",
                "46:             \"sentry-api-0-organization-events-vitals\",",
                "47:             kwargs={\"organization_id_or_slug\": self.organization.slug},",
                "48:         )",
                "49: ",
                "50:         with self.feature(features):",
                "51:             return self.client.get(url, query, format=\"json\")",
                "52: ",
                "53:     def test_no_projects(self):",
                "54:         response = self.do_request()",
                "55:         assert response.status_code == 200, response.content",
                "56:         assert len(response.data) == 0",
                "57: ",
                "58:     def test_no_vitals(self):",
                "59:         self.store_event(",
                "60:             self.transaction_data,",
                "61:             project_id=self.project.id,",
                "62:         )",
                "63: ",
                "64:         self.query.update({\"vital\": []})",
                "65:         response = self.do_request()",
                "66:         assert response.status_code == 400, response.content",
                "67:         assert \"Need to pass at least one vital\" == response.data[\"detail\"]",
                "68: ",
                "69:     def test_bad_vital(self):",
                "70:         self.store_event(",
                "71:             self.transaction_data,",
                "72:             project_id=self.project.id,",
                "73:         )",
                "74: ",
                "75:         self.query.update({\"vital\": [\"foobar\"]})",
                "76:         response = self.do_request()",
                "77:         assert response.status_code == 400, response.content",
                "78:         assert \"foobar is not a valid vital\" == response.data[\"detail\"]",
                "79: ",
                "80:     def test_simple(self):",
                "81:         data = self.transaction_data.copy()",
                "82:         for lcp in [2000, 3000, 5000]:",
                "83:             self.store_event(",
                "84:                 data,",
                "85:                 {\"lcp\": lcp},",
                "86:                 project_id=self.project.id,",
                "87:             )",
                "88: ",
                "89:         self.query.update({\"vital\": [\"measurements.lcp\"]})",
                "90:         response = self.do_request()",
                "91:         assert response.status_code == 200, response.content",
                "92:         assert not response.data[\"meta\"][\"isMetricsData\"]",
                "93:         assert response.data[\"measurements.lcp\"] == {",
                "94:             \"good\": 1,",
                "95:             \"meh\": 1,",
                "96:             \"poor\": 1,",
                "97:             \"total\": 3,",
                "98:             \"p75\": 4000,",
                "99:         }",
                "100: ",
                "101:     def test_simple_with_refining_user_misery_filter(self):",
                "102:         project1 = self.create_project(organization=self.organization)",
                "103:         project2 = self.create_project(organization=self.organization)",
                "104:         ProjectTransactionThreshold.objects.create(",
                "105:             project=project1,",
                "106:             organization=project1.organization,",
                "107:             threshold=100,",
                "108:             metric=TransactionMetric.LCP.value,",
                "109:         )",
                "110: ",
                "111:         ProjectTransactionThreshold.objects.create(",
                "112:             project=project2,",
                "113:             organization=project2.organization,",
                "114:             threshold=1000,",
                "115:             metric=TransactionMetric.LCP.value,",
                "116:         )",
                "117: ",
                "118:         data = self.transaction_data.copy()",
                "119: ",
                "120:         for project in [project1, project2]:",
                "121:             for lcp in [2000, 3000, 5000]:",
                "122:                 self.store_event(",
                "123:                     data,",
                "124:                     {\"lcp\": lcp},",
                "125:                     project_id=project.id,",
                "126:                 )",
                "127: ",
                "128:         self.query.update({\"vital\": [\"measurements.lcp\"]})",
                "129:         response = self.do_request(",
                "130:             features={\"organizations:global-views\": True, \"organizations:discover-basic\": True}",
                "131:         )",
                "132: ",
                "133:         assert response.status_code == 200, response.content",
                "134:         assert not response.data[\"meta\"][\"isMetricsData\"]",
                "135:         assert response.data[\"measurements.lcp\"] == {",
                "136:             \"good\": 0,",
                "137:             \"meh\": 1,",
                "138:             \"poor\": 1,",
                "139:             \"total\": 2,",
                "140:             \"p75\": 4500,",
                "141:         }",
                "142: ",
                "143:         self.query.update({\"query\": \"user_misery():<0.04\"})",
                "144:         response = self.do_request(",
                "145:             features={\"organizations:global-views\": True, \"organizations:discover-basic\": True}",
                "146:         )",
                "147: ",
                "148:         assert response.status_code == 200, response.content",
                "149:         assert len(response.data) == 2",
                "150:         assert not response.data[\"meta\"][\"isMetricsData\"]",
                "151:         assert response.data[\"measurements.lcp\"] == {",
                "152:             \"good\": 0,",
                "153:             \"meh\": 1,",
                "154:             \"poor\": 1,",
                "155:             \"total\": 2,",
                "156:             \"p75\": 4500,",
                "157:         }",
                "158: ",
                "159:     def test_grouping(self):",
                "160:         counts = [",
                "161:             (100, 2),",
                "162:             (3000, 3),",
                "163:             (4500, 1),",
                "164:         ]",
                "165:         for duration, count in counts:",
                "166:             for _ in range(count):",
                "167:                 self.store_event(",
                "168:                     load_data(\"transaction\", timestamp=self.start),",
                "169:                     {\"lcp\": duration},",
                "170:                     project_id=self.project.id,",
                "171:                 )",
                "172: ",
                "173:         self.query.update({\"vital\": [\"measurements.lcp\"]})",
                "174:         response = self.do_request()",
                "175:         assert response.status_code == 200",
                "176:         assert not response.data[\"meta\"][\"isMetricsData\"]",
                "177:         assert response.data[\"measurements.lcp\"] == {"
            ]
        },
        {
            "file": "tests/sentry/tasks/test_weekly_reports.py",
            "line_number": 63,
            "matched_line": "    def store_event_outcomes(",
            "context_start_line": 1,
            "context_end_line": 213,
            "context": [
                "1: from datetime import timedelta",
                "2: from typing import cast",
                "3: from unittest import mock",
                "4: ",
                "5: import pytest",
                "6: from django.core import mail",
                "7: from django.core.mail.message import EmailMultiAlternatives",
                "8: from django.db import router",
                "9: from django.db.models import F",
                "10: from django.utils import timezone",
                "11: ",
                "12: from sentry.constants import DataCategory",
                "13: from sentry.issues.grouptype import MonitorIncidentType, PerformanceNPlusOneGroupType",
                "14: from sentry.models.group import GroupStatus",
                "15: from sentry.models.grouphistory import GroupHistoryStatus",
                "16: from sentry.models.organizationmember import OrganizationMember",
                "17: from sentry.models.project import Project",
                "18: from sentry.models.team import TeamStatus",
                "19: from sentry.notifications.models.notificationsettingoption import NotificationSettingOption",
                "20: from sentry.silo.base import SiloMode",
                "21: from sentry.silo.safety import unguarded_write",
                "22: from sentry.snuba.referrer import Referrer",
                "23: from sentry.tasks.summaries.utils import (",
                "24:     ONE_DAY,",
                "25:     OrganizationReportContext,",
                "26:     ProjectContext,",
                "27:     organization_project_issue_substatus_summaries,",
                "28:     project_key_errors,",
                "29:     user_project_ownership,",
                "30: )",
                "31: from sentry.tasks.summaries.weekly_reports import (",
                "32:     OrganizationReportBatch,",
                "33:     group_status_to_color,",
                "34:     prepare_organization_report,",
                "35:     prepare_template_context,",
                "36:     schedule_organizations,",
                "37: )",
                "38: from sentry.testutils.cases import OutcomesSnubaTest, PerformanceIssueTestCase, SnubaTestCase",
                "39: from sentry.testutils.factories import EventType",
                "40: from sentry.testutils.helpers import with_feature",
                "41: from sentry.testutils.helpers.datetime import before_now, freeze_time",
                "42: from sentry.testutils.outbox import outbox_runner",
                "43: from sentry.testutils.silo import assume_test_silo_mode",
                "44: from sentry.types.group import GroupSubStatus",
                "45: from sentry.users.services.user_option import user_option_service",
                "46: from sentry.utils import redis",
                "47: from sentry.utils.dates import floor_to_utc_day",
                "48: from sentry.utils.outcomes import Outcome",
                "49: ",
                "50: DISABLED_ORGANIZATIONS_USER_OPTION_KEY = \"reports:disabled-organizations\"",
                "51: ",
                "52: ",
                "53: class WeeklyReportsTest(OutcomesSnubaTest, SnubaTestCase, PerformanceIssueTestCase):",
                "54:     def setUp(self):",
                "55:         super().setUp()",
                "56:         self.now = timezone.now()",
                "57:         self.timestamp = floor_to_utc_day(self.now).timestamp()",
                "58:         self.two_days_ago = self.now - timedelta(days=2)",
                "59:         self.three_days_ago = self.now - timedelta(days=3)",
                "60: ",
                "61:     _dummy_batch_id = \"20bd6c5b-7fac-4f31-9548-d6f8bb63226d\"",
                "62: ",
                "63:     def store_event_outcomes(",
                "64:         self,",
                "65:         organization_id,",
                "66:         project_id,",
                "67:         timestamp,",
                "68:         num_times,",
                "69:         outcome=Outcome.ACCEPTED,",
                "70:         category=DataCategory.ERROR,",
                "71:     ):",
                "72:         self.store_outcomes(",
                "73:             {",
                "74:                 \"org_id\": organization_id,",
                "75:                 \"project_id\": project_id,",
                "76:                 \"outcome\": outcome,",
                "77:                 \"category\": category,",
                "78:                 \"timestamp\": timestamp,",
                "79:                 \"key_id\": 1,",
                "80:             },",
                "81:             num_times=num_times,",
                "82:         )",
                "83: ",
                "84:     @freeze_time(before_now(days=2).replace(hour=0, minute=0, second=0, microsecond=0))",
                "85:     def test_integration(self):",
                "86:         with unguarded_write(using=router.db_for_write(Project)):",
                "87:             Project.objects.all().delete()",
                "88:         project = self.create_project(",
                "89:             organization=self.organization,",
                "90:             teams=[self.team],",
                "91:             date_added=self.now - timedelta(days=90),",
                "92:         )",
                "93:         member_set = set(project.teams.get().member_set.all())",
                "94:         self.store_event(",
                "95:             data={",
                "96:                 \"timestamp\": before_now(days=1).isoformat(),",
                "97:             },",
                "98:             project_id=project.id,",
                "99:         )",
                "100: ",
                "101:         with self.tasks():",
                "102:             schedule_organizations(timestamp=self.now.timestamp())",
                "103:             assert len(mail.outbox) == len(member_set) == 1",
                "104: ",
                "105:             message = mail.outbox[0]",
                "106:             assert self.organization.name in message.subject",
                "107: ",
                "108:     @freeze_time(before_now(days=2).replace(hour=0, minute=0, second=0, microsecond=0))",
                "109:     def test_with_empty_string_user_option(self):",
                "110:         project = self.create_project(",
                "111:             organization=self.organization,",
                "112:             teams=[self.team],",
                "113:             date_added=self.now - timedelta(days=90),",
                "114:         )",
                "115:         self.store_event(data={\"timestamp\": before_now(days=1).isoformat()}, project_id=project.id)",
                "116:         member_set = set(project.teams.get().member_set.all())",
                "117:         for member in member_set:",
                "118:             # some users have an empty string value set for this key, presumably cleared.",
                "119:             user_option_service.set_option(",
                "120:                 user_id=member.user_id, key=\"reports:disabled-organizations\", value=\"\"",
                "121:             )",
                "122: ",
                "123:         with self.tasks():",
                "124:             schedule_organizations(timestamp=self.now.timestamp())",
                "125:             assert len(mail.outbox) == len(member_set) == 1",
                "126: ",
                "127:             message = mail.outbox[0]",
                "128:             assert self.organization.name in message.subject",
                "129: ",
                "130:     @with_feature(\"system:multi-region\")",
                "131:     @freeze_time(before_now(days=2).replace(hour=0, minute=0, second=0, microsecond=0))",
                "132:     def test_message_links_customer_domains(self):",
                "133:         with unguarded_write(using=router.db_for_write(Project)):",
                "134:             Project.objects.all().delete()",
                "135: ",
                "136:         project = self.create_project(",
                "137:             organization=self.organization,",
                "138:             teams=[self.team],",
                "139:             date_added=self.now - timedelta(days=90),",
                "140:         )",
                "141:         self.store_event(",
                "142:             data={",
                "143:                 \"timestamp\": before_now(days=1).isoformat(),",
                "144:             },",
                "145:             project_id=project.id,",
                "146:         )",
                "147:         with self.tasks():",
                "148:             schedule_organizations(timestamp=self.now.timestamp())",
                "149:             assert len(mail.outbox) == 1",
                "150: ",
                "151:             message = mail.outbox[0]",
                "152:             assert isinstance(message, EmailMultiAlternatives)",
                "153:             assert self.organization.name in message.subject",
                "154:             html = message.alternatives[0][0]",
                "155: ",
                "156:             assert isinstance(html, str)",
                "157:             assert (",
                "158:                 f\"http://{self.organization.slug}.testserver/issues/?referrer=weekly_report\" in html",
                "159:             )",
                "160: ",
                "161:     def _set_option_value(self, value):",
                "162:         with assume_test_silo_mode(SiloMode.CONTROL):",
                "163:             NotificationSettingOption.objects.update_or_create(",
                "164:                 scope_type=\"organization\",",
                "165:                 scope_identifier=self.organization.id,",
                "166:                 user_id=self.user.id,",
                "167:                 type=\"reports\",",
                "168:                 defaults={\"value\": value},",
                "169:             )",
                "170: ",
                "171:     @mock.patch(\"sentry.tasks.summaries.weekly_reports.prepare_template_context\")",
                "172:     @mock.patch(\"sentry.tasks.summaries.weekly_reports.OrganizationReportBatch.send_email\")",
                "173:     def test_deliver_reports_respects_settings(",
                "174:         self, mock_send_email, mock_prepare_template_context",
                "175:     ):",
                "176:         self.store_event_outcomes(",
                "177:             self.organization.id, self.project.id, self.two_days_ago, num_times=2",
                "178:         )",
                "179:         ctx = OrganizationReportContext(0, 0, organization=self.organization)",
                "180:         user_project_ownership(ctx)",
                "181:         template_context = prepare_template_context(ctx, [self.user.id])",
                "182:         mock_prepare_template_context.return_value = template_context",
                "183:         batch_id = \"77a1d368-33d5-47cd-88cf-d66c97b38333\"",
                "184: ",
                "185:         # disabled",
                "186:         self._set_option_value(\"never\")",
                "187:         OrganizationReportBatch(ctx, batch_id).deliver_reports()",
                "188:         assert mock_send_email.call_count == 0",
                "189: ",
                "190:         # enabled",
                "191:         self._set_option_value(\"always\")",
                "192:         OrganizationReportBatch(ctx, batch_id).deliver_reports()",
                "193:         assert mock_send_email.call_count == 1",
                "194:         mock_send_email.assert_called_once_with(",
                "195:             template_ctx=template_context[0].get(\"context\"),",
                "196:             user_id=template_context[0].get(\"user_id\"),",
                "197:         )",
                "198: ",
                "199:     @mock.patch(\"sentry.tasks.summaries.weekly_reports.OrganizationReportBatch.send_email\")",
                "200:     def test_member_disabled(self, mock_send_email):",
                "201:         self.store_event_outcomes(",
                "202:             self.organization.id, self.project.id, self.two_days_ago, num_times=2",
                "203:         )",
                "204:         ctx = OrganizationReportContext(0, 0, self.organization)",
                "205:         user_project_ownership(ctx)",
                "206: ",
                "207:         with unguarded_write(using=router.db_for_write(Project)):",
                "208:             OrganizationMember.objects.get(user_id=self.user.id).update(",
                "209:                 flags=F(\"flags\").bitor(OrganizationMember.flags[\"member-limit:restricted\"])",
                "210:             )",
                "211: ",
                "212:         # disabled",
                "213:         OrganizationReportBatch(ctx, self._dummy_batch_id).deliver_reports()"
            ]
        }
    ],
    "total": 3
}

Message 15 [Assistant]:
--------------------------------------------------------------------------------
Content:
让我直接查看 Factories 类中 store_event 方法的实现：
Tool Calls:
[
  {
    "name": "cpg_symbol_search",
    "args": {
      "query": "store_event",
      "file_path": "src/sentry/testutils/factories.py",
      "include_callsites": true
    },
    "id": "call_00_qdW6ryK9ymvEPhQUox6oHmCt",
    "type": "tool_call"
  }
]

Message 16 [Tool]:
--------------------------------------------------------------------------------
Tool: cpg_symbol_search
Tool Call ID: call_00_qdW6ryK9ymvEPhQUox6oHmCt
Result:
{
    "rev": "head",
    "query": "store_event",
    "symbols": [],
    "callsites": [],
    "_budget": {
        "attrs_dropped": true,
        "symbols_total": 0,
        "symbols_returned": 0,
        "symbols_truncated": false,
        "callsites_total": 0,
        "callsites_returned": 0,
        "callsites_truncated": false
    },
    "error": null,
    "_payload": {
        "approx_chars": 246,
        "approx_bytes_utf8": 246,
        "keys": [
            "_budget",
            "callsites",
            "error",
            "query",
            "rev",
            "symbols"
        ]
    }
}

Message 17 [Assistant]:
--------------------------------------------------------------------------------
Content:
让我搜索 Factories 类中的方法：
Tool Calls:
[
  {
    "name": "cpg_ast_index",
    "args": {
      "file_paths": [
        "src/sentry/testutils/factories.py"
      ],
      "include_defs": true,
      "include_calls": true,
      "include_imports": true
    },
    "id": "call_00_V21FIdLStGqhnAjAzsA2rBDo",
    "type": "tool_call"
  }
]

Message 18 [Tool]:
--------------------------------------------------------------------------------
Tool: cpg_ast_index
Tool Call ID: call_00_V21FIdLStGqhnAjAzsA2rBDo
Result:
{
    "rev": "head",
    "files": [],
    "_budget": {
        "attrs_dropped": true,
        "summary_only": false,
        "files_total": 0,
        "files_returned": 0,
        "files_truncated": false,
        "compact": true
    },
    "error": null,
    "_payload": {
        "approx_chars": 166,
        "approx_bytes_utf8": 166,
        "keys": [
            "_budget",
            "error",
            "files",
            "rev"
        ]
    }
}

Message 19 [Assistant]:
--------------------------------------------------------------------------------
Content:
让我查看文件中的 store_event 方法：
Tool Calls:
[
  {
    "name": "read_file",
    "args": {
      "file_path": "src/sentry/testutils/factories.py",
      "encoding": "utf-8"
    },
    "id": "call_00_834jdmcbo7hRA0kwCH20W2fd",
    "type": "tool_call"
  }
]

Message 20 [Tool]:
--------------------------------------------------------------------------------
Tool: read_file
Tool Call ID: call_00_834jdmcbo7hRA0kwCH20W2fd
Result:
{
    "content": "from __future__ import annotations\n\nimport contextlib\nimport copy\nimport io\nimport os\nimport random\nimport zipfile\nfrom base64 import b64encode\nfrom binascii import hexlify\nfrom collections.abc import Mapping, MutableMapping, Sequence\nfrom datetime import UTC, datetime\nfrom enum import Enum\nfrom hashlib import sha1\nfrom importlib import import_module\nfrom typing import Any\nfrom unittest import mock\nfrom uuid import uuid4\n\nimport orjson\nimport petname\nfrom django.conf import settings\nfrom django.contrib.auth.models import AnonymousUser\nfrom django.core.files.base import ContentFile\nfrom django.db import router, transaction\nfrom django.test.utils import override_settings\nfrom django.utils import timezone\nfrom django.utils.text import slugify\n\nfrom sentry.auth.access import RpcBackedAccess\nfrom sentry.auth.services.auth.model import RpcAuthState, RpcMemberSsoState\nfrom sentry.constants import SentryAppInstallationStatus, SentryAppStatus\nfrom sentry.event_manager import EventManager\nfrom sentry.eventstore.models import Event\nfrom sentry.hybridcloud.models.outbox import RegionOutbox, outbox_context\nfrom sentry.hybridcloud.models.webhookpayload import WebhookPayload\nfrom sentry.hybridcloud.outbox.category import OutboxCategory, OutboxScope\nfrom sentry.incidents.grouptype import MetricIssue\nfrom sentry.incidents.logic import (\n    create_alert_rule,\n    create_alert_rule_trigger,\n    create_alert_rule_trigger_action,\n    query_datasets_to_type,\n)\nfrom sentry.incidents.models.alert_rule import (\n    AlertRuleDetectionType,\n    AlertRuleThresholdType,\n    AlertRuleTriggerAction,\n)\nfrom sentry.incidents.models.incident import (\n    Incident,\n    IncidentActivity,\n    IncidentProject,\n    IncidentTrigger,\n    IncidentType,\n    TriggerStatus,\n)\nfrom sentry.integrations.models.doc_integration import DocIntegration\nfrom sentry.integrations.models.doc_integration_avatar import DocIntegrationAvatar\nfrom sentry.integrations.models.external_actor import ExternalActor\nfrom sentry.integrations.models.external_issue import ExternalIssue\nfrom sentry.integrations.models.integration import Integration\nfrom sentry.integrations.models.integration_external_project import IntegrationExternalProject\nfrom sentry.integrations.models.integration_feature import (\n    Feature,\n    IntegrationFeature,\n    IntegrationTypes,\n)\nfrom sentry.integrations.models.organization_integration import OrganizationIntegration\nfrom sentry.integrations.models.repository_project_path_config import RepositoryProjectPathConfig\nfrom sentry.integrations.types import ExternalProviders\nfrom sentry.issues.grouptype import get_group_type_by_type_id\nfrom sentry.models.activity import Activity\nfrom sentry.models.apikey import ApiKey\nfrom sentry.models.apitoken import ApiToken\nfrom sentry.models.artifactbundle import ArtifactBundle\nfrom sentry.models.authidentity import AuthIdentity\nfrom sentry.models.authprovider import AuthProvider\nfrom sentry.models.commit import Commit\nfrom sentry.models.commitauthor import CommitAuthor\nfrom sentry.models.commitfilechange import CommitFileChange\nfrom sentry.models.dashboard import Dashboard\nfrom sentry.models.dashboard_widget import (\n    DashboardWidget,\n    DashboardWidgetDisplayTypes,\n    DashboardWidgetQuery,\n)\nfrom sentry.models.debugfile import ProjectDebugFile\nfrom sentry.models.environment import Environment\nfrom sentry.models.files.control_file import ControlFile\nfrom sentry.models.files.file import File\nfrom sentry.models.group import Group\nfrom sentry.models.grouphistory import GroupHistory\nfrom sentry.models.grouplink import GroupLink\nfrom sentry.models.groupopenperiod import GroupOpenPeriod\nfrom sentry.models.grouprelease import GroupRelease\nfrom sentry.models.organization import Organization\nfrom sentry.models.organizationmapping import OrganizationMapping\nfrom sentry.models.organizationmember import OrganizationMember\nfrom sentry.models.organizationmemberinvite import OrganizationMemberInvite\nfrom sentry.models.organizationmemberteam import OrganizationMemberTeam\nfrom sentry.models.organizationslugreservation import OrganizationSlugReservation\nfrom sentry.models.orgauthtoken import OrgAuthToken\nfrom sentry.models.project import Project\nfrom sentry.models.projectbookmark import ProjectBookmark\nfrom sentry.models.projectcodeowners import ProjectCodeOwners\nfrom sentry.models.projecttemplate import ProjectTemplate\nfrom sentry.models.release import Release, ReleaseStatus\nfrom sentry.models.releasecommit import ReleaseCommit\nfrom sentry.models.releaseenvironment import ReleaseEnvironment\nfrom sentry.models.releasefile import ReleaseFile, update_artifact_index\nfrom sentry.models.releaseprojectenvironment import ReleaseProjectEnvironment\nfrom sentry.models.repository import Repository\nfrom sentry.models.rule import Rule\nfrom sentry.models.rulesnooze import RuleSnooze\nfrom sentry.models.savedsearch import SavedSearch\nfrom sentry.models.team import Team\nfrom sentry.models.userreport import UserReport\nfrom sentry.notifications.models.notificationaction import (\n    ActionService,\n    ActionTarget,\n    ActionTrigger,\n    NotificationAction,\n)\nfrom sentry.notifications.models.notificationsettingprovider import NotificationSettingProvider\nfrom sentry.organizations.services.organization import RpcOrganization, RpcUserOrganizationContext\nfrom sentry.performance_issues.performance_problem import PerformanceProblem\nfrom sentry.sentry_apps.installations import (\n    SentryAppInstallationCreator,\n    SentryAppInstallationTokenCreator,\n)\nfrom sentry.sentry_apps.logic import SentryAppCreator\nfrom sentry.sentry_apps.models.platformexternalissue import PlatformExternalIssue\nfrom sentry.sentry_apps.models.sentry_app import SentryApp\nfrom sentry.sentry_apps.models.sentry_app_avatar import SentryAppAvatar\nfrom sentry.sentry_apps.models.sentry_app_installation import SentryAppInstallation\nfrom sentry.sentry_apps.models.sentry_app_installation_for_provider import (\n    SentryAppInstallationForProvider,\n)\nfrom sentry.sentry_apps.models.servicehook import ServiceHook\nfrom sentry.sentry_apps.services.hook import hook_service\nfrom sentry.sentry_apps.token_exchange.grant_exchanger import GrantExchanger\nfrom sentry.signals import project_created\nfrom sentry.silo.base import SiloMode\nfrom sentry.snuba.dataset import Dataset\nfrom sentry.snuba.models import QuerySubscriptionDataSourceHandler\nfrom sentry.tempest.models import MessageType as TempestMessageType\nfrom sentry.tempest.models import TempestCredentials\nfrom sentry.testutils.outbox import outbox_runner\nfrom sentry.testutils.silo import assume_test_silo_mode\nfrom sentry.types.activity import ActivityType\nfrom sentry.types.actor import Actor\nfrom sentry.types.region import Region, get_local_region, get_region_by_name\nfrom sentry.types.token import AuthTokenType\nfrom sentry.uptime.models import (\n    IntervalSecondsLiteral,\n    ProjectUptimeSubscription,\n    UptimeStatus,\n    UptimeSubscription,\n    UptimeSubscriptionRegion,\n)\nfrom sentry.uptime.types import UptimeMonitorMode\nfrom sentry.users.models.identity import Identity, IdentityProvider, IdentityStatus\nfrom sentry.users.models.user import User\nfrom sentry.users.models.user_avatar import UserAvatar\nfrom sentry.users.models.user_option import UserOption\nfrom sentry.users.models.useremail import UserEmail\nfrom sentry.users.models.userpermission import UserPermission\nfrom sentry.users.models.userrole import UserRole\nfrom sentry.users.services.user import RpcUser\nfrom sentry.utils import loremipsum\nfrom sentry.workflow_engine.models import (\n    Action,\n    ActionAlertRuleTriggerAction,\n    AlertRuleDetector,\n    AlertRuleWorkflow,\n    DataCondition,\n    DataConditionGroup,\n    DataConditionGroupAction,\n    DataSource,\n    DataSourceDetector,\n    Detector,\n    DetectorState,\n    DetectorWorkflow,\n    IncidentGroupOpenPeriod,\n    Workflow,\n    WorkflowDataConditionGroup,\n)\nfrom sentry.workflow_engine.registry import data_source_type_registry\nfrom social_auth.models import UserSocialAuth\n\n\nclass EventType(Enum):\n    ERROR = \"error\"\n    DEFAULT = \"default\"\n\n\ndef get_fixture_path(*parts: str) -> str:\n    path = os.path.realpath(__file__)\n    for _ in range(4):  # src/sentry/testutils/{__file__}\n        path = os.path.dirname(path)\n    return os.path.join(path, \"fixtures\", *parts)\n\n\ndef make_sentence(words=None):\n    if words is None:\n        words = int(random.weibullvariate(8, 3))\n    return \" \".join(random.choice(loremipsum.words) for _ in range(words))\n\n\ndef make_word(words=None):\n    if words is None:\n        words = int(random.weibullvariate(8, 3))\n    return random.choice(loremipsum.words)\n\n\nDEFAULT_EVENT_DATA = {\n    \"extra\": {\n        \"loadavg\": [0.97607421875, 0.88330078125, 0.833984375],\n        \"sys.argv\": [\n            \"/Users/dcramer/.virtualenvs/sentry/bin/raven\",\n            \"test\",\n            \"https://ebc35f33e151401f9deac549978bda11:f3403f81e12e4c24942d505f086b2cad@sentry.io/1\",\n        ],\n        \"user\": \"dcramer\",\n    },\n    \"modules\": {\"raven\": \"3.1.13\"},\n    \"request\": {\n        \"cookies\": {},\n        \"data\": {},\n        \"env\": {},\n        \"headers\": {},\n        \"method\": \"GET\",\n        \"query_string\": \"\",\n        \"url\": \"http://example.com\",\n    },\n    \"stacktrace\": {\n        \"frames\": [\n            {\n                \"abs_path\": \"www/src/sentry/models/foo.py\",\n                \"context_line\": \"                        string_max_length=self.string_max_length)\",\n                \"filename\": \"sentry/models/foo.py\",\n                \"function\": \"build_msg\",\n                \"in_app\": True,\n                \"lineno\": 29,\n                \"module\": \"raven.base\",\n                \"post_context\": [\n                    \"                },\",\n                    \"            })\",\n                    \"\",\n                    \"        if 'stacktrace' in data:\",\n                    \"            if self.include_paths:\",\n                ],\n                \"pre_context\": [\n                    \"\",\n                    \"            data.update({\",\n                    \"                'stacktrace': {\",\n                    \"                    'frames': get_stack_info(frames,\",\n                    \"                        list_max_length=self.list_max_length,\",\n                ],\n                \"vars\": {\n                    \"culprit\": \"raven.scripts.runner\",\n                    \"date\": \"datetime.datetime(2013, 2, 14, 20, 6, 33, 479471)\",\n                    \"event_id\": \"598fb19363e745ec8be665e6ba88b1b2\",\n                    \"event_type\": \"raven.events.Message\",\n                    \"frames\": \"<generator object iter_stack_frames at 0x103fef050>\",\n                    \"handler\": \"<raven.events.Message object at 0x103feb710>\",\n                    \"k\": \"logentry\",\n                    \"public_key\": None,\n                    \"result\": {\n                        \"logentry\": \"{'message': 'This is a test message generated using ``raven test``', 'params': []}\"\n                    },\n                    \"self\": \"<raven.base.Client object at 0x104397f10>\",\n                    \"stack\": True,\n                    \"tags\": None,\n                    \"time_spent\": None,\n                },\n            },\n            {\n                \"abs_path\": \"/Users/dcramer/.virtualenvs/sentry/lib/python2.7/site-packages/raven/base.py\",\n                \"context_line\": \"                        string_max_length=self.string_max_length)\",\n                \"filename\": \"raven/base.py\",\n                \"function\": \"build_msg\",\n                \"in_app\": False,\n                \"lineno\": 290,\n                \"module\": \"raven.base\",\n                \"post_context\": [\n                    \"                },\",\n                    \"            })\",\n                    \"\",\n                    \"        if 'stacktrace' in data:\",\n                    \"            if self.include_paths:\",\n                ],\n                \"pre_context\": [\n                    \"\",\n                    \"            data.update({\",\n                    \"                'stacktrace': {\",\n                    \"                    'frames': get_stack_info(frames,\",\n                    \"                        list_max_length=self.list_max_length,\",\n                ],\n                \"vars\": {\n                    \"culprit\": \"raven.scripts.runner\",\n                    \"date\": \"datetime.datetime(2013, 2, 14, 20, 6, 33, 479471)\",\n                    \"event_id\": \"598fb19363e745ec8be665e6ba88b1b2\",\n                    \"event_type\": \"raven.events.Message\",\n                    \"frames\": \"<generator object iter_stack_frames at 0x103fef050>\",\n                    \"handler\": \"<raven.events.Message object at 0x103feb710>\",\n                    \"k\": \"logentry\",\n                    \"public_key\": None,\n                    \"result\": {\n                        \"logentry\": \"{'message': 'This is a test message generated using ``raven test``', 'params': []}\"\n                    },\n                    \"self\": \"<raven.base.Client object at 0x104397f10>\",\n                    \"stack\": True,\n                    \"tags\": None,\n                    \"time_spent\": None,\n                },\n            },\n        ]\n    },\n    \"tags\": [],\n    \"platform\": \"python\",\n}\n\ndefault_detector_config_data = {\n    MetricIssue.slug: {\"threshold_period\": 1, \"detection_type\": \"static\"}\n}\n\n\ndef _patch_artifact_manifest(path, org=None, release=None, project=None, extra_files=None):\n    with open(path, \"rb\") as fp:\n        manifest = orjson.loads(fp.read())\n    if org:\n        manifest[\"org\"] = org\n    if release:\n        manifest[\"release\"] = release\n    if project:\n        manifest[\"project\"] = project\n    for path in extra_files or {}:\n        manifest[\"files\"][path] = {\"url\": path}\n    return orjson.dumps(manifest).decode()\n\n\ndef _set_sample_rate_from_error_sampling(normalized_data: MutableMapping[str, Any]) -> None:\n    \"\"\"Set 'sample_rate' on normalized_data if contexts.error_sampling.client_sample_rate is present and valid.\"\"\"\n    client_sample_rate = None\n    try:\n        client_sample_rate = (\n            normalized_data.get(\"contexts\", {}).get(\"error_sampling\", {}).get(\"client_sample_rate\")\n        )\n    except Exception:\n        pass\n    if client_sample_rate:\n        try:\n            normalized_data[\"sample_rate\"] = float(client_sample_rate)\n        except Exception:\n            pass\n\n\n# TODO(dcramer): consider moving to something more scalable like factoryboy\nclass Factories:\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_organization(name=None, owner=None, region: Region | str | None = None, **kwargs):\n        if not name:\n            name = petname.generate(2, \" \", letters=10).title()\n\n        with contextlib.ExitStack() as ctx:\n            if region is None or SiloMode.get_current_mode() == SiloMode.MONOLITH:\n                region_name = get_local_region().name\n            else:\n                if isinstance(region, Region):\n                    region_name = region.name\n                else:\n                    region_obj = get_region_by_name(region)  # Verify it exists\n                    region_name = region_obj.name\n\n                ctx.enter_context(\n                    override_settings(SILO_MODE=SiloMode.REGION, SENTRY_REGION=region_name)\n                )\n\n            with outbox_context(flush=False):\n                org = Organization.objects.create(name=name, **kwargs)\n\n            with assume_test_silo_mode(SiloMode.CONTROL):\n                # Organization mapping creation relies on having a matching org slug reservation\n                OrganizationSlugReservation(\n                    organization_id=org.id,\n                    region_name=region_name,\n                    user_id=owner.id if owner else -1,\n                    slug=org.slug,\n                ).save(unsafe_write=True)\n\n            # Manually replicate org data after adding an org slug reservation\n            org.handle_async_replication(org.id)\n\n            # Flush remaining organization update outboxes accumulated by org create\n            RegionOutbox(\n                shard_identifier=org.id,\n                shard_scope=OutboxScope.ORGANIZATION_SCOPE,\n                category=OutboxCategory.ORGANIZATION_UPDATE,\n            ).drain_shard()\n\n        if owner:\n            Factories.create_member(organization=org, user_id=owner.id, role=\"owner\")\n        return org\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_org_mapping(org=None, **kwds):\n        if org:\n            kwds.setdefault(\"organization_id\", org.id)\n            kwds.setdefault(\"slug\", org.slug)\n            kwds.setdefault(\"name\", org.name)\n            kwds.setdefault(\"idempotency_key\", uuid4().hex)\n            kwds.setdefault(\"region_name\", \"na\")\n        return OrganizationMapping.objects.create(**kwds)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_member(teams=None, team_roles=None, **kwargs):\n        kwargs.setdefault(\"role\", \"member\")\n        teamRole = kwargs.pop(\"teamRole\", None)\n\n        # user_id will have precedence over user\n        user = kwargs.pop(\"user\", None)\n        user_id = kwargs.pop(\"user_id\", None)\n        if not user_id and user:\n            user_id = user.id\n        kwargs[\"user_id\"] = user_id\n\n        # inviter_id will have precedence over inviter\n        inviter = kwargs.pop(\"inviter\", None)\n        inviter_id = kwargs.pop(\"inviter_id\", None)\n        if not inviter_id and inviter:\n            inviter_id = inviter.id\n        kwargs[\"inviter_id\"] = inviter_id\n\n        om = OrganizationMember.objects.create(**kwargs)\n\n        if team_roles:\n            for team, role in team_roles:\n                Factories.create_team_membership(team=team, member=om, role=role)\n        elif teams:\n            for team in teams:\n                Factories.create_team_membership(team=team, member=om, role=teamRole)\n        return om\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_member_invite(\n        organization: Organization | None = None,\n        email: str | None = None,\n        **kwargs,\n    ) -> OrganizationMemberInvite:\n        if organization is None:\n            organization = Factories.create_organization()\n        if email is None:\n            email = f\"{petname.generate().title()}@email.com\"\n        om = OrganizationMember.objects.create(organization=organization)\n        return OrganizationMemberInvite.objects.create(\n            organization=organization, organization_member_id=om.id, email=email, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_team_membership(team, member=None, user=None, role=None):\n        if member is None:\n            member, created = OrganizationMember.objects.get_or_create(\n                user_id=user.id if user else None,\n                organization=team.organization,\n                defaults={\"role\": \"member\"},\n            )\n\n        return OrganizationMemberTeam.objects.create(\n            team=team, organizationmember=member, is_active=True, role=role\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_api_key(organization, **kwargs) -> ApiKey:\n        return ApiKey.objects.create(organization_id=organization.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_auth_provider(**kwargs):\n        return AuthProvider.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_auth_identity(**kwargs):\n        return AuthIdentity.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_auth_token(user, scope_list: list[str] | None = None, **kwargs) -> ApiToken:\n        if scope_list is None:\n            scope_list = []\n        return ApiToken.objects.create(\n            user=user,\n            scope_list=scope_list,\n            token_type=AuthTokenType.USER,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_org_auth_token(*args, **kwargs) -> OrgAuthToken:\n        return OrgAuthToken.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_team(organization, **kwargs):\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n        if not kwargs.get(\"slug\"):\n            kwargs[\"slug\"] = slugify(str(kwargs[\"name\"]))\n        members = kwargs.pop(\"members\", None)\n\n        team = Team.objects.create(organization=organization, **kwargs)\n        if members:\n            for user in members:\n                Factories.create_team_membership(team=team, user=user)\n        return team\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_environment(project, **kwargs):\n        name = kwargs.get(\"name\", petname.generate(3, \" \", letters=10)[:64])\n\n        organization = kwargs.get(\"organization\")\n        organization_id = organization.id if organization else project.organization_id\n\n        env = Environment.objects.create(organization_id=organization_id, name=name)\n        env.add_project(project, is_hidden=kwargs.get(\"is_hidden\"))\n        return env\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project(\n        organization=None, teams=None, fire_project_created=False, **kwargs\n    ) -> Project:\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n        if not kwargs.get(\"slug\"):\n            kwargs[\"slug\"] = slugify(str(kwargs[\"name\"]))\n        if not organization and teams:\n            organization = teams[0].organization\n\n        with transaction.atomic(router.db_for_write(Project)):\n            project = Project.objects.create(organization=organization, **kwargs)\n            if teams:\n                for team in teams:\n                    project.add_team(team)\n            if fire_project_created:\n                project_created.send(\n                    project=project, user=AnonymousUser(), default_rules=True, sender=Factories\n                )\n        return project\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_template(project=None, organization=None, **kwargs) -> ProjectTemplate:\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n\n        with transaction.atomic(router.db_for_write(Project)):\n            project_template = ProjectTemplate.objects.create(organization=organization, **kwargs)\n\n        return project_template\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_bookmark(project, user):\n        return ProjectBookmark.objects.create(project_id=project.id, user_id=user.id)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_rule(\n        project,\n        action_data=None,\n        allow_no_action_data=False,\n        condition_data=None,\n        name=\"Test Alert\",\n        action_match=\"all\",\n        filter_match=\"all\",\n        frequency=30,\n        **kwargs,\n    ):\n        actions = None\n        if not allow_no_action_data:\n            action_data = action_data or [\n                {\n                    \"id\": \"sentry.rules.actions.notify_event.NotifyEventAction\",\n                    \"name\": \"Send a notification (for all legacy integrations)\",\n                },\n                {\n                    \"id\": \"sentry.rules.actions.notify_event_service.NotifyEventServiceAction\",\n                    \"service\": \"mail\",\n                    \"name\": \"Send a notification via mail\",\n                },\n            ]\n            actions = action_data\n        condition_data = condition_data or [\n            {\n                \"id\": \"sentry.rules.conditions.first_seen_event.FirstSeenEventCondition\",\n                \"name\": \"A new issue is created\",\n            },\n            {\n                \"id\": \"sentry.rules.conditions.every_event.EveryEventCondition\",\n                \"name\": \"The event occurs\",\n            },\n        ]\n        data = {\n            \"conditions\": condition_data,\n            \"action_match\": action_match,\n            \"filter_match\": filter_match,\n            \"frequency\": frequency,\n        }\n        if actions:\n            data[\"actions\"] = actions\n\n        return Rule.objects.create(\n            label=name,\n            project=project,\n            data=data,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_slack_project_rule(project, integration_id, channel_id=None, channel_name=None):\n        action_data = [\n            {\n                \"id\": \"sentry.rules.actions.notify_event.SlackNotifyServiceAction\",\n                \"name\": \"Send a Slack notification\",\n                \"workspace\": integration_id,\n                \"channel_id\": channel_id or \"123453\",\n                \"channel\": channel_name or \"#general\",\n            }\n        ]\n        return Factories.create_project_rule(project, action_data)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_key(project):\n        return project.key_set.get_or_create()[0]\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_tempest_credentials(\n        project: Project,\n        created_by: User | None = None,\n        client_id: str | None = None,\n        client_secret: str | None = None,\n        message: str = \"\",\n        message_type: str | None = None,\n        latest_fetched_item_id: str | None = None,\n    ):\n        if client_id is None:\n            client_id = str(uuid4())\n        if client_secret is None:\n            client_secret = str(uuid4())\n        if message_type is None:\n            message_type = TempestMessageType.ERROR\n\n        return TempestCredentials.objects.create(\n            project=project,\n            created_by_id=created_by.id if created_by else None,\n            client_id=client_id,\n            client_secret=client_secret,\n            message=message,\n            message_type=message_type,\n            latest_fetched_item_id=latest_fetched_item_id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release(\n        project: Project,\n        user: User | None = None,\n        version: str | None = None,\n        date_added: datetime | None = None,\n        additional_projects: Sequence[Project] | None = None,\n        environments: Sequence[Environment] | None = None,\n        date_released: datetime | None = None,\n        adopted: datetime | None = None,\n        unadopted: datetime | None = None,\n        status: int | None = ReleaseStatus.OPEN,\n    ):\n        if version is None:\n            version = hexlify(os.urandom(20)).decode()\n\n        if date_added is None:\n            date_added = timezone.now()\n\n        if additional_projects is None:\n            additional_projects = []\n\n        release = Release.objects.create(\n            version=version,\n            organization_id=project.organization_id,\n            date_added=date_added,\n            date_released=date_released,\n            status=status,\n        )\n\n        release.add_project(project)\n        for additional_project in additional_projects:\n            release.add_project(additional_project)\n\n        for environment in environments or []:\n            ReleaseEnvironment.objects.create(\n                organization=project.organization, release=release, environment=environment\n            )\n            for project in [project, *additional_projects]:\n                ReleaseProjectEnvironment.objects.create(\n                    project=project,\n                    release=release,\n                    environment=environment,\n                    adopted=adopted,\n                    unadopted=unadopted,\n                )\n\n        Activity.objects.create(\n            type=ActivityType.RELEASE.value,\n            project=project,\n            ident=Activity.get_version_ident(version),\n            user_id=user.id if user else None,\n            data={\"version\": version},\n        )\n\n        # add commits\n        if user:\n            author = Factories.create_commit_author(project=project, user=user)\n            repo = Factories.create_repo(project, name=f\"organization-{project.slug}\")\n            commit = Factories.create_commit(\n                project=project,\n                repo=repo,\n                author=author,\n                release=release,\n                key=\"deadbeef\",\n                message=\"placeholder commit message\",\n            )\n\n            release.update(authors=[str(author.id)], commit_count=1, last_commit_id=commit.id)\n\n        return release\n\n    @staticmethod\n    def create_group_release(project: Project, group: Group, release: Release) -> GroupRelease:\n        return GroupRelease.objects.create(\n            project_id=project.id,\n            group_id=group.id,\n            release_id=release.id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release_file(release_id, file=None, name=None, dist_id=None):\n        if file is None:\n            file = Factories.create_file(\n                name=\"log.txt\",\n                size=32,\n                headers={\"Content-Type\": \"text/plain\"},\n                checksum=\"dc1e3f3e411979d336c3057cce64294f3420f93a\",\n            )\n\n        if name is None:\n            name = file.name\n\n        organization_id = Release.objects.get(pk=release_id).organization.id\n\n        return ReleaseFile.objects.create(\n            organization_id=organization_id,\n            release_id=release_id,\n            name=name,\n            file=file,\n            dist_id=dist_id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_artifact_bundle_zip(\n        org=None, release=None, project=None, extra_files=None, fixture_path=\"artifact_bundle\"\n    ):\n        bundle = io.BytesIO()\n        bundle_dir = get_fixture_path(fixture_path)\n        with zipfile.ZipFile(bundle, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n            for path, content in (extra_files or {}).items():\n                zipf.writestr(path, content)\n            for path, _, files in os.walk(bundle_dir):\n                for filename in files:\n                    fullpath = os.path.join(path, filename)\n                    relpath = os.path.relpath(fullpath, bundle_dir)\n                    if filename == \"manifest.json\":\n                        manifest = _patch_artifact_manifest(\n                            fullpath, org, release, project, extra_files\n                        )\n                        zipf.writestr(relpath, manifest)\n                    else:\n                        zipf.write(fullpath, relpath)\n\n        return bundle.getvalue()\n\n    @classmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release_archive(cls, org, release: str, project=None, dist=None):\n        bundle = cls.create_artifact_bundle_zip(org, release, project)\n        file = File.objects.create(name=\"release-artifacts.zip\")\n        file.putfile(ContentFile(bundle))\n        release_obj = Release.objects.get(organization__slug=org, version=release)\n        return update_artifact_index(release_obj, dist, file)\n\n    @classmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_artifact_bundle(\n        cls,\n        org,\n        bundle_id=None,\n        artifact_count=0,\n        fixture_path=\"artifact_bundle_debug_ids\",\n        date_uploaded=None,\n        date_last_modified=None,\n    ):\n        if date_uploaded is None:\n            date_uploaded = timezone.now()\n\n        bundle = cls.create_artifact_bundle_zip(org.slug, fixture_path=fixture_path)\n        file_ = File.objects.create(name=\"artifact-bundle.zip\")\n        file_.putfile(ContentFile(bundle))\n        # The 'artifact_count' should correspond to the 'bundle' contents but for the purpose of tests we can also\n        # mock it with an arbitrary value.\n        artifact_bundle = ArtifactBundle.objects.create(\n            organization_id=org.id,\n            bundle_id=bundle_id or uuid4(),\n            file=file_,\n            artifact_count=artifact_count,\n            date_uploaded=date_uploaded,\n            date_last_modified=date_last_modified,\n        )\n        return artifact_bundle\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_code_mapping(project, repo=None, organization_integration=None, **kwargs):\n        kwargs.setdefault(\"stack_root\", \"\")\n        kwargs.setdefault(\"source_root\", \"\")\n        kwargs.setdefault(\"default_branch\", \"master\")\n\n        if not repo:\n            repo = Factories.create_repo(project=project)\n        return RepositoryProjectPathConfig.objects.create(\n            project=project,\n            repository=repo,\n            organization_integration_id=organization_integration.id,\n            integration_id=organization_integration.integration_id,\n            organization_id=organization_integration.organization_id,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_repo(\n        project, name=None, provider=None, integration_id=None, url=None, external_id=None\n    ):\n        repo, _ = Repository.objects.get_or_create(\n            organization_id=project.organization_id,\n            name=name\n            or \"{}-{}\".format(petname.generate(2, \"\", letters=10), random.randint(1000, 9999)),\n            provider=provider,\n            integration_id=integration_id,\n            url=url,\n            external_id=external_id,\n        )\n        return repo\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit(\n        repo, project=None, author=None, release=None, message=None, key=None, date_added=None\n    ):\n        commit = Commit.objects.get_or_create(\n            organization_id=repo.organization_id,\n            repository_id=repo.id,\n            key=key or sha1(uuid4().hex.encode(\"utf-8\")).hexdigest(),\n            defaults={\n                \"message\": message or make_sentence(),\n                \"author\": author\n                or Factories.create_commit_author(organization_id=repo.organization_id),\n                \"date_added\": date_added or timezone.now(),\n            },\n        )[0]\n\n        if release:\n            assert project\n            ReleaseCommit.objects.create(\n                organization_id=repo.organization_id,\n                project_id=project.id,\n                release=release,\n                commit=commit,\n                order=1,\n            )\n\n        Factories.create_commit_file_change(commit=commit, filename=\"/models/foo.py\")\n        Factories.create_commit_file_change(commit=commit, filename=\"/worsematch/foo.py\")\n        Factories.create_commit_file_change(commit=commit, filename=\"/models/other.py\")\n\n        return commit\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit_author(organization_id=None, project=None, user=None, email=None):\n        if email:\n            user_email = email\n        else:\n            user_email = user.email if user else f\"{make_word()}@example.com\"\n        return CommitAuthor.objects.get_or_create(\n            organization_id=organization_id or project.organization_id,\n            email=user_email,\n            defaults={\"name\": user.name if user else make_word()},\n        )[0]\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit_file_change(commit, filename):\n        return CommitFileChange.objects.get_or_create(\n            organization_id=commit.organization_id, commit=commit, filename=filename, type=\"M\"\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user(\n        email=None, is_superuser=False, is_staff=False, is_active=True, **kwargs\n    ) -> User:\n        if email is None:\n            email = uuid4().hex + \"@example.com\"\n\n        kwargs.setdefault(\"username\", email)\n\n        user = User(\n            email=email, is_superuser=is_superuser, is_staff=is_staff, is_active=is_active, **kwargs\n        )\n        if kwargs.get(\"password\") is None:\n            user.set_password(\"admin\")\n        user.save()\n\n        # UserEmail is created by a signal\n        assert UserEmail.objects.filter(user=user, email=email).update(is_verified=True)\n\n        return user\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_useremail(user, email=None, **kwargs):\n        if not email:\n            email = uuid4().hex + \"@example.com\"\n\n        kwargs.setdefault(\"is_verified\", True)\n\n        useremail = UserEmail(user=user, email=email, **kwargs)\n        useremail.save()\n\n        return useremail\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_avatar(*args, **kwargs):\n        return UserAvatar.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_role(*args, **kwargs):\n        return UserRole.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_usersocialauth(\n        user: User,\n        provider: str | None = None,\n        uid: str | None = None,\n        extra_data: dict[str, Any] | None = None,\n    ):\n        if not provider:\n            provider = \"asana\"\n        if not uid:\n            uid = \"abc-123\"\n        usa = UserSocialAuth(user=user, provider=provider, uid=uid, extra_data=extra_data)\n        usa.save()\n        return usa\n\n    @staticmethod\n    def inject_performance_problems(jobs, _):\n        for job in jobs:\n            job[\"performance_problems\"] = []\n            for f in job[\"data\"][\"fingerprint\"]:\n                f_data = f.split(\"-\", 1)\n                if len(f_data) < 2:\n                    raise ValueError(\n                        \"Invalid performance fingerprint data. Format must be 'group_type-fingerprint'.\"\n                    )\n                group_type = get_group_type_by_type_id(int(f_data[0]))\n                perf_fingerprint = f_data[1]\n\n                job[\"performance_problems\"].append(\n                    PerformanceProblem(\n                        fingerprint=perf_fingerprint,\n                        op=\"db\",\n                        desc=\"\",\n                        type=group_type,\n                        parent_span_ids=None,\n                        cause_span_ids=None,\n                        offender_span_ids=[],\n                        evidence_data={},\n                        evidence_display=[],\n                    )\n                )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def store_event(\n        data,\n        project_id: int,\n        assert_no_errors: bool = True,\n        default_event_type: EventType | None = None,\n        sent_at: datetime | None = None,\n    ) -> Event:\n        \"\"\"\n        Like `create_event`, but closer to how events are actually\n        ingested. Prefer to use this method over `create_event`\n        \"\"\"\n\n        # this creates a basic message event\n        if default_event_type == EventType.DEFAULT:\n            data.update({\"stacktrace\": copy.deepcopy(DEFAULT_EVENT_DATA[\"stacktrace\"])})\n\n        # this creates an error event\n        elif default_event_type == EventType.ERROR:\n            data.update({\"exception\": [{\"value\": \"BadError\"}]})\n\n        manager = EventManager(data, sent_at=sent_at)\n        manager.normalize()\n        if assert_no_errors:\n            errors = manager.get_data().get(\"errors\")\n            assert not errors, errors\n\n        normalized_data = manager.get_data()\n\n        _set_sample_rate_from_error_sampling(normalized_data)\n\n        event = None\n\n        # When fingerprint is present on transaction, inject performance problems\n        if (\n            normalized_data.get(\"type\") == \"transaction\"\n            and normalized_data.get(\"fingerprint\") is not None\n        ):\n            with mock.patch(\n                \"sentry.event_manager._detect_performance_problems\",\n                Factories.inject_performance_problems,\n            ):\n                event = manager.save(project_id)\n\n        else:\n            event = manager.save(project_id)\n\n        if event.groups:\n            for group in event.groups:\n                group.save()\n\n        if event.group:\n            event.group.save()\n\n        return event\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_group(project, create_open_period=True, **kwargs):\n        from sentry.models.group import GroupStatus\n        from sentry.models.groupopenperiod import GroupOpenPeriod\n        from sentry.testutils.helpers.datetime import before_now\n        from sentry.types.group import GroupSubStatus\n\n        kwargs.setdefault(\"message\", \"Hello world\")\n        kwargs.setdefault(\"data\", {})\n        if \"type\" not in kwargs[\"data\"]:\n            kwargs[\"data\"].update({\"type\": \"default\", \"metadata\": {\"title\": kwargs[\"message\"]}})\n        if \"short_id\" not in kwargs:\n            kwargs[\"short_id\"] = project.next_short_id()\n        if \"metadata\" in kwargs:\n            metadata = kwargs.pop(\"metadata\")\n            kwargs[\"data\"].setdefault(\"metadata\", {}).update(metadata)\n        if \"status\" not in kwargs:\n            kwargs[\"status\"] = GroupStatus.UNRESOLVED\n            kwargs[\"substatus\"] = GroupSubStatus.NEW\n\n        group = Group.objects.create(project=project, **kwargs)\n        if create_open_period:\n            open_period = GroupOpenPeriod.objects.create(\n                group=group,\n                project=project,\n                date_started=group.first_seen or before_now(minutes=5),\n            )\n            if group.status == GroupStatus.RESOLVED:\n                open_period.update(\n                    date_ended=group.resolved_at if group.resolved_at else timezone.now()\n                )\n\n        return group\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_file(**kwargs):\n        return File.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_file_from_path(path, name=None, **kwargs):\n        if name is None:\n            name = os.path.basename(path)\n\n        file = Factories.create_file(name=name, **kwargs)\n        with open(path) as f:\n            file.putfile(f)\n        return file\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dif_file(\n        project,\n        debug_id=None,\n        object_name=None,\n        features=None,\n        data=None,\n        file=None,\n        cpu_name=None,\n        code_id=None,\n        **kwargs,\n    ):\n        if debug_id is None:\n            debug_id = str(uuid4())\n\n        if object_name is None:\n            object_name = \"%s.dSYM\" % debug_id\n\n        if features is not None:\n            if data is None:\n                data = {}\n            data[\"features\"] = features\n\n        if file is None:\n            file = Factories.create_file(\n                name=object_name,\n                size=42,\n                headers={\"Content-Type\": \"application/x-mach-binary\"},\n                checksum=\"dc1e3f3e411979d336c3057cce64294f3420f93a\",\n            )\n\n        return ProjectDebugFile.objects.create(\n            debug_id=debug_id,\n            code_id=code_id,\n            project_id=project.id,\n            object_name=object_name,\n            cpu_name=cpu_name or \"x86_64\",\n            file=file,\n            checksum=file.checksum,\n            data=data,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dif_from_path(path, object_name=None, **kwargs):\n        if object_name is None:\n            object_name = os.path.basename(path)\n\n        headers = {\"Content-Type\": \"application/x-mach-binary\"}\n        file = Factories.create_file_from_path(path, name=object_name, headers=headers)\n        return Factories.create_dif_file(file=file, object_name=object_name, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def add_user_permission(user, permission):\n        UserPermission.objects.create(user=user, permission=permission)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app(**kwargs):\n        published = kwargs.pop(\"published\", False)\n        args = Factories._sentry_app_kwargs(**kwargs)\n        user = args.pop(\"user\", None)\n        app = SentryAppCreator(is_internal=False, **args).run(user=user, request=None)\n\n        if published:\n            app.update(status=SentryAppStatus.PUBLISHED)\n\n        return app\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_avatar(*args, **kwargs):\n        return SentryAppAvatar.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_internal_integration(**kwargs) -> SentryApp:\n        args = Factories._sentry_app_kwargs(**kwargs)\n        args[\"verify_install\"] = False\n        user = args.pop(\"user\", None)\n        app = SentryAppCreator(is_internal=True, **args).run(\n            user=user, request=None, skip_default_auth_token=True\n        )\n        return app\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_internal_integration_token(\n        user,\n        internal_integration: SentryApp | None = None,\n        install: SentryAppInstallation | None = None,\n        request=None,\n    ) -> ApiToken:\n        if internal_integration and install:\n            raise ValueError(\"Only one of internal_integration or install arg can be provided\")\n        elif internal_integration is None and install is None:\n            raise ValueError(\"Must pass in either internal_integration or install arg\")\n\n        if internal_integration is not None and install is None:\n            # Fetch install from provided or created internal integration\n            with assume_test_silo_mode(SiloMode.CONTROL):\n                install = SentryAppInstallation.objects.get(\n                    sentry_app=internal_integration.id,\n                    organization_id=internal_integration.owner_id,\n                )\n        elif install is None:\n            raise AssertionError(\"unreachable\")\n\n        return SentryAppInstallationTokenCreator(sentry_app_installation=install).run(\n            user=user, request=request\n        )\n\n    @staticmethod\n    def _sentry_app_kwargs(**kwargs):\n        _kwargs = {\n            \"user\": kwargs.get(\"user\", Factories.create_user()),\n            \"name\": kwargs.get(\"name\", petname.generate(2, \" \", letters=10).title()),\n            \"organization_id\": kwargs.get(\n                \"organization_id\", kwargs.pop(\"organization\", Factories.create_organization()).id\n            ),\n            \"author\": kwargs.get(\"author\", \"A Company\"),\n            \"scopes\": kwargs.get(\"scopes\", ()),\n            \"verify_install\": kwargs.get(\"verify_install\", True),\n            \"webhook_url\": kwargs.get(\"webhook_url\", \"https://example.com/webhook\"),\n            \"events\": [],\n            \"schema\": {},\n        }\n\n        _kwargs.update(**kwargs)\n        return _kwargs\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_sentry_app_installation(\n        organization=None,\n        slug=None,\n        user=None,\n        status=None,\n        prevent_token_exchange=False,\n    ):\n        if not organization:\n            organization = Factories.create_organization()\n\n        Factories.create_project(organization=organization)\n\n        with assume_test_silo_mode(SiloMode.CONTROL):\n            install = SentryAppInstallationCreator(\n                slug=(slug or Factories.create_sentry_app(organization=organization).slug),\n                organization_id=organization.id,\n            ).run(\n                user=(user or Factories.create_user()),\n                request=None,\n            )\n\n            install.status = SentryAppInstallationStatus.INSTALLED if status is None else status\n            install.save()\n            if not prevent_token_exchange and (\n                install.sentry_app.status != SentryAppStatus.INTERNAL\n            ):\n                assert install.api_grant is not None\n                assert install.sentry_app.application is not None\n                assert install.sentry_app.proxy_user is not None\n                GrantExchanger(\n                    install=install,\n                    code=install.api_grant.code,\n                    client_id=install.sentry_app.application.client_id,\n                    user=install.sentry_app.proxy_user,\n                ).run()\n                install = SentryAppInstallation.objects.get(id=install.id)\n        return install\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_installation_for_provider(\n        sentry_app_id: int,\n        organization_id: int,\n        provider: str,\n    ) -> SentryAppInstallationForProvider:\n        installation = SentryAppInstallation.objects.get(\n            sentry_app_id=sentry_app_id, organization_id=organization_id\n        )\n        return SentryAppInstallationForProvider.objects.create(\n            organization_id=organization_id,\n            provider=provider,\n            sentry_app_installation=installation,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_stacktrace_link_schema():\n        return {\"type\": \"stacktrace-link\", \"uri\": \"/redirect/\"}\n\n    @staticmethod\n    def create_issue_link_schema():\n        return {\n            \"type\": \"issue-link\",\n            \"link\": {\n                \"uri\": \"/sentry/issues/link\",\n                \"required_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    }\n                ],\n            },\n            \"create\": {\n                \"uri\": \"/sentry/issues/create\",\n                \"required_fields\": [\n                    {\"type\": \"text\", \"name\": \"title\", \"label\": \"Title\"},\n                    {\"type\": \"text\", \"name\": \"summary\", \"label\": \"Summary\"},\n                ],\n                \"optional_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"points\",\n                        \"label\": \"Points\",\n                        \"options\": [[\"1\", \"1\"], [\"2\", \"2\"], [\"3\", \"3\"], [\"5\", \"5\"], [\"8\", \"8\"]],\n                    },\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    },\n                ],\n            },\n        }\n\n    @staticmethod\n    def create_alert_rule_action_schema():\n        return {\n            \"type\": \"alert-rule-action\",\n            \"title\": \"Create Task with App\",\n            \"settings\": {\n                \"type\": \"alert-rule-settings\",\n                \"uri\": \"/sentry/alert-rule\",\n                \"required_fields\": [\n                    {\"type\": \"text\", \"name\": \"title\", \"label\": \"Title\"},\n                    {\"type\": \"text\", \"name\": \"summary\", \"label\": \"Summary\"},\n                ],\n                \"optional_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"points\",\n                        \"label\": \"Points\",\n                        \"options\": [[\"1\", \"1\"], [\"2\", \"2\"], [\"3\", \"3\"], [\"5\", \"5\"], [\"8\", \"8\"]],\n                    },\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    },\n                ],\n            },\n        }\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_service_hook(\n        actor=None, org=None, project=None, events=None, url=None, project_ids=None, **kwargs\n    ):\n        if project:\n            if project_ids is not None:\n                raise ValueError(\"Cannot provide both project and project_ids\")\n            project_ids = [project.id]\n        if not actor:\n            actor = Factories.create_user()\n        if not org:\n            if project:\n                org = project.organization\n            else:\n                org = Factories.create_organization(owner=actor)\n        if project_ids is None:  # empty list for project_ids is valid and means no project filter\n            project_ids = [Factories.create_project(organization=org).id]\n        if events is None:\n            events = [\"event.created\"]\n        if not url:\n            url = \"https://example.com/sentry/webhook\"\n\n        app_id = kwargs.pop(\"application_id\", None)\n        if app_id is None and \"application\" in kwargs:\n            app_id = kwargs[\"application\"].id\n        installation_id = kwargs.pop(\"installation_id\", None)\n        if installation_id is None and \"installation\" in kwargs:\n            installation_id = kwargs[\"installation\"].id\n        hook_id = hook_service.create_service_hook(\n            application_id=app_id,\n            actor_id=actor.id,\n            installation_id=installation_id,\n            organization_id=org.id,\n            project_ids=project_ids,\n            events=events,\n            url=url,\n        ).id\n        return ServiceHook.objects.get(id=hook_id)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_feature(feature=None, sentry_app=None, description=None):\n        if not sentry_app:\n            sentry_app = Factories.create_sentry_app()\n\n        integration_feature = IntegrationFeature.objects.create(\n            target_id=sentry_app.id,\n            target_type=IntegrationTypes.SENTRY_APP.value,\n            feature=feature or Feature.API,\n        )\n\n        if description:\n            integration_feature.update(user_description=description)\n\n        return integration_feature\n\n    @staticmethod\n    def _doc_integration_kwargs(**kwargs):\n        _kwargs = {\n            \"name\": kwargs.get(\"name\", petname.generate(2, \" \", letters=10).title()),\n            \"author\": kwargs.get(\"author\", \"me\"),\n            \"description\": kwargs.get(\"description\", \"hi im a description\"),\n            \"url\": kwargs.get(\"url\", \"https://sentry.io\"),\n            \"popularity\": kwargs.get(\"popularity\", 1),\n            \"is_draft\": kwargs.get(\"is_draft\", True),\n            \"metadata\": kwargs.get(\"metadata\", {}),\n        }\n        _kwargs[\"slug\"] = slugify(_kwargs[\"name\"])\n        _kwargs.update(**kwargs)\n        return _kwargs\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration(features=None, has_avatar: bool = False, **kwargs) -> DocIntegration:\n        doc = DocIntegration.objects.create(**Factories._doc_integration_kwargs(**kwargs))\n        if features:\n            Factories.create_doc_integration_features(features=features, doc_integration=doc)\n        if has_avatar:\n            Factories.create_doc_integration_avatar(doc_integration=doc)\n        return doc\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration_features(\n        features=None, doc_integration=None\n    ) -> list[IntegrationFeature]:\n        if not features:\n            features = [Feature.API]\n        if not doc_integration:\n            doc_integration = Factories.create_doc_integration()\n        return IntegrationFeature.objects.bulk_create(\n            [\n                IntegrationFeature(\n                    target_id=doc_integration.id,\n                    target_type=IntegrationTypes.DOC_INTEGRATION.value,\n                    feature=feature,\n                )\n                for feature in features\n            ]\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration_avatar(doc_integration=None, **kwargs) -> DocIntegrationAvatar:\n        if not doc_integration:\n            doc_integration = Factories.create_doc_integration()\n        photo = ControlFile.objects.create(name=\"test.png\", type=\"avatar.file\")\n        photo.putfile(io.BytesIO(b\"imaginethiswasphotobytes\"))\n\n        return DocIntegrationAvatar.objects.create(\n            doc_integration=doc_integration, avatar_type=0, control_file_id=photo.id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_userreport(\n        project: Project, event_id: str | None = None, **kwargs: Any\n    ) -> UserReport:\n        event = Factories.store_event(\n            data={\n                \"timestamp\": datetime.now(UTC).isoformat(),\n                \"event_id\": event_id or \"a\" * 32,\n                \"message\": \"testing\",\n            },\n            project_id=project.id,\n        )\n        assert event.group is not None\n\n        return UserReport.objects.create(\n            group_id=event.group.id,\n            event_id=event.event_id,\n            project_id=project.id,\n            name=\"Jane Bloggs\",\n            email=\"jane@example.com\",\n            comments=\"the application crashed\",\n            **kwargs,\n        )\n\n    @staticmethod\n    def create_session():\n        engine = import_module(settings.SESSION_ENGINE)\n\n        session = engine.SessionStore()\n        session.save()\n        return session\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_platform_external_issue(\n        group=None, service_type=None, display_name=None, web_url=None\n    ):\n        return PlatformExternalIssue.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            service_type=service_type,\n            display_name=display_name,\n            web_url=web_url,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_integration_external_issue(group=None, integration=None, key=None, **kwargs):\n        external_issue = ExternalIssue.objects.create(\n            organization_id=group.organization.id, integration_id=integration.id, key=key, **kwargs\n        )\n\n        GroupLink.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            linked_type=GroupLink.LinkedType.issue,\n            linked_id=external_issue.id,\n            relationship=GroupLink.Relationship.references,\n        )\n\n        return external_issue\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_integration_external_project(\n        organization_id: int, integration_id: int, *args: Any, **kwargs: Any\n    ) -> IntegrationExternalProject:\n        oi = OrganizationIntegration.objects.get(\n            organization_id=organization_id, integration_id=integration_id\n        )\n        return IntegrationExternalProject.objects.create(\n            organization_integration_id=oi.id, *args, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident(\n        organization,\n        projects,\n        detection_uuid=None,\n        status=1,\n        title=None,\n        query=\"test query\",\n        date_started=None,\n        date_detected=None,\n        date_closed=None,\n        alert_rule=None,\n        subscription=None,\n    ):\n        if not title:\n            title = petname.generate(2, \" \", letters=10).title()\n        if alert_rule is None:\n            alert_rule = Factories.create_alert_rule(\n                organization, projects, query=query, time_window=1\n            )\n\n        incident = Incident.objects.create(\n            organization=organization,\n            detection_uuid=detection_uuid,\n            status=status,\n            title=title,\n            alert_rule=alert_rule,\n            date_started=date_started or timezone.now(),\n            date_detected=date_detected or timezone.now(),\n            date_closed=timezone.now() if date_closed is not None else date_closed,\n            type=IncidentType.ALERT_TRIGGERED.value,\n            subscription=subscription,\n        )\n        for project in projects:\n            IncidentProject.objects.create(incident=incident, project=project)\n\n        return incident\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_activity(incident, type, comment=None, user_id=None):\n        return IncidentActivity.objects.create(\n            incident=incident, type=type, comment=comment, user_id=user_id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule(\n        organization,\n        projects,\n        name=None,\n        owner=None,\n        query=\"level:error\",\n        aggregate=\"count()\",\n        time_window=10,\n        threshold_period=1,\n        environment=None,\n        date_added=None,\n        query_type=None,\n        dataset=Dataset.Events,\n        threshold_type=AlertRuleThresholdType.ABOVE,\n        resolve_threshold=None,\n        user=None,\n        event_types=None,\n        comparison_delta=None,\n        description=None,\n        sensitivity=None,\n        seasonality=None,\n        detection_type=AlertRuleDetectionType.STATIC,\n    ):\n        if not name:\n            name = petname.generate(2, \" \", letters=10).title()\n\n        if query_type is None:\n            query_type = query_datasets_to_type[dataset]\n\n        alert_rule = create_alert_rule(\n            organization,\n            projects,\n            name,\n            query,\n            aggregate,\n            time_window,\n            threshold_type,\n            threshold_period,\n            owner=owner,\n            resolve_threshold=resolve_threshold,\n            query_type=query_type,\n            dataset=dataset,\n            environment=environment,\n            user=user,\n            event_types=event_types,\n            comparison_delta=comparison_delta,\n            description=description,\n            sensitivity=sensitivity,\n            seasonality=seasonality,\n            detection_type=detection_type,\n        )\n\n        if date_added is not None:\n            alert_rule.update(date_added=date_added)\n\n        return alert_rule\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_trigger(alert_rule, label=None, alert_threshold=100):\n        if not label:\n            label = petname.generate(2, \" \", letters=10).title()\n\n        return create_alert_rule_trigger(alert_rule, label, alert_threshold)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_trigger(incident, alert_rule_trigger, status=None):\n        if status is None:\n            status = TriggerStatus.ACTIVE.value\n\n        return IncidentTrigger.objects.create(\n            alert_rule_trigger=alert_rule_trigger, incident=incident, status=status\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_trigger_action(\n        trigger,\n        type=AlertRuleTriggerAction.Type.EMAIL,\n        target_type=AlertRuleTriggerAction.TargetType.USER,\n        target_identifier=None,\n        integration=None,\n        sentry_app=None,\n        sentry_app_config=None,\n    ):\n        return create_alert_rule_trigger_action(\n            trigger,\n            type,\n            target_type,\n            target_identifier,\n            integration.id if integration else None,\n            sentry_app.id if sentry_app else None,\n            sentry_app_config=sentry_app_config,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_external_user(user: User, **kwargs: Any) -> ExternalActor:\n        kwargs.setdefault(\"provider\", ExternalProviders.GITHUB.value)\n        kwargs.setdefault(\"external_name\", \"\")\n\n        return ExternalActor.objects.create(user_id=user.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_external_team(team: Team, **kwargs: Any) -> ExternalActor:\n        kwargs.setdefault(\"provider\", ExternalProviders.GITHUB.value)\n        kwargs.setdefault(\"external_name\", \"@getsentry/ecosystem\")\n\n        return ExternalActor.objects.create(team_id=team.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_codeowners(project, code_mapping, **kwargs):\n        kwargs.setdefault(\"raw\", \"\")\n\n        return ProjectCodeOwners.objects.create(\n            project=project, repository_project_path_config=code_mapping, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_slack_integration(\n        organization: Organization, external_id: str, **kwargs: Any\n    ) -> Integration:\n        integration = Integration.objects.create(\n            provider=\"slack\",\n            name=\"Team A\",\n            external_id=external_id,\n            metadata={\n                \"access_token\": \"xoxp-xxxxxxxxx-xxxxxxxxxx-xxxxxxxxxxxx\",\n                \"installation_type\": \"born_as_bot\",\n            },\n        )\n        integration.add_organization(organization)\n        return integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_integration(\n        organization: Organization,\n        external_id: str,\n        oi_params: Mapping[str, Any] | None = None,\n        **integration_params: Any,\n    ) -> Integration:\n        integration = Integration.objects.create(external_id=external_id, **integration_params)\n        with outbox_runner():\n            organization_integration = integration.add_organization(organization)\n            assert organization_integration is not None\n        organization_integration.update(**(oi_params or {}))\n\n        return integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_provider_integration(**integration_params: Any) -> Integration:\n        return Integration.objects.create(**integration_params)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_provider_integration_for(\n        organization: Organization | RpcOrganization,\n        user: User | RpcUser | None,\n        **integration_params: Any,\n    ) -> tuple[Integration, OrganizationIntegration]:\n        integration = Integration.objects.create(**integration_params)\n        org_integration = integration.add_organization(organization, user)\n        assert org_integration is not None\n        return integration, org_integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity_integration(\n        user: User | RpcUser,\n        organization: Organization | RpcOrganization,\n        integration_params: Mapping[Any, Any],\n        identity_params: Mapping[Any, Any],\n    ) -> tuple[Integration, OrganizationIntegration, Identity, IdentityProvider]:\n        # Avoid common pitfalls in tests\n        assert \"provider\" in integration_params\n        assert \"external_id\" in integration_params\n        assert \"external_id\" in identity_params\n\n        integration = Factories.create_provider_integration(**integration_params)\n        identity_provider = Factories.create_identity_provider(integration=integration)\n        identity = Factories.create_identity(\n            user=user, identity_provider=identity_provider, **identity_params\n        )\n        organization_integration = integration.add_organization(\n            organization_id=organization.id, user=user, default_auth_id=identity.id\n        )\n        assert organization_integration is not None\n        return integration, organization_integration, identity, identity_provider\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_organization_integration(**integration_params: Any) -> OrganizationIntegration:\n        return OrganizationIntegration.objects.create(**integration_params)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity_provider(\n        integration: Integration | None = None,\n        config: dict[str, Any] | None = None,\n        **kwargs: Any,\n    ) -> IdentityProvider:\n        if integration is not None:\n            integration_values = dict(\n                type=integration.provider,\n                external_id=integration.external_id,\n            )\n            if any((key in kwargs) for key in integration_values):\n                raise ValueError(\n                    \"Values from integration should not be in kwargs: \"\n                    + repr(list(integration_values.keys()))\n                )\n            kwargs.update(integration_values)\n\n        return IdentityProvider.objects.create(config=config or {}, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity(\n        user: User | RpcUser, identity_provider: IdentityProvider, external_id: str, **kwargs: Any\n    ) -> Identity:\n        return Identity.objects.create(\n            external_id=external_id,\n            idp=identity_provider,\n            user_id=user.id,\n            status=IdentityStatus.VALID,\n            scopes=[],\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_group_history(\n        group: Group,\n        status: int,\n        release: Release | None = None,\n        user_id: int | None = None,\n        team_id: int | None = None,\n        prev_history: GroupHistory | None = None,\n        date_added: datetime | None = None,\n    ) -> GroupHistory:\n        prev_history_date = None\n        if prev_history:\n            prev_history_date = prev_history.date_added\n\n        kwargs = {}\n        if date_added:\n            kwargs[\"date_added\"] = date_added\n        return GroupHistory.objects.create(\n            organization=group.organization,\n            group=group,\n            project=group.project,\n            release=release,\n            user_id=user_id,\n            team_id=team_id,\n            status=status,\n            prev_history=prev_history,\n            prev_history_date=prev_history_date,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_comment(issue, project, user, text=\"hello world\"):\n        data = {\"text\": text}\n        return Activity.objects.create(\n            project=project,\n            group=issue,\n            type=ActivityType.NOTE.value,\n            user_id=user.id,\n            data=data,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_saved_search(name: str, **kwargs):\n        if \"owner\" in kwargs:\n            owner = kwargs.pop(\"owner\")\n            kwargs[\"owner_id\"] = owner.id if not isinstance(owner, int) else owner\n        return SavedSearch.objects.create(name=name, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_notification_action(\n        organization: Organization | None = None,\n        projects: list[Project] | None = None,\n        **kwargs,\n    ):\n        if not organization:\n            organization = Factories.create_organization()\n\n        if not projects:\n            projects = []\n\n        action_kwargs = {\n            \"organization\": organization,\n            \"type\": ActionService.SENTRY_NOTIFICATION,\n            \"target_type\": ActionTarget.USER,\n            \"target_identifier\": \"1\",\n            \"target_display\": \"Sentry User\",\n            \"trigger_type\": ActionTrigger.AUDIT_LOG,\n            **kwargs,\n        }\n\n        action = NotificationAction.objects.create(**action_kwargs)\n        action.projects.add(*projects)\n        action.save()\n\n        return action\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_notification_settings_provider(*args, **kwargs) -> NotificationSettingProvider:\n        return NotificationSettingProvider.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_option(*args, **kwargs) -> UserOption:\n        return UserOption.objects.create(*args, **kwargs)\n\n    @staticmethod\n    def create_basic_auth_header(username: str, password: str = \"\") -> bytes:\n        return b\"Basic \" + b64encode(f\"{username}:{password}\".encode())\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def snooze_rule(**kwargs):\n        return RuleSnooze.objects.create(**kwargs)\n\n    @staticmethod\n    def create_request_access(\n        sso_state: RpcMemberSsoState | None = None,\n        permissions: list | None = None,\n        org_context: RpcUserOrganizationContext | None = None,\n        scopes_upper_bound: frozenset | None = frozenset(),\n    ) -> RpcBackedAccess:\n        if not sso_state:\n            sso_state = RpcMemberSsoState()\n        if not permissions:\n            permissions = []\n        if not org_context:\n            org_context = RpcUserOrganizationContext()\n\n        auth_state = RpcAuthState(sso_state=sso_state, permissions=permissions)\n        return RpcBackedAccess(\n            rpc_user_organization_context=org_context,\n            auth_state=auth_state,\n            scopes_upper_bound=scopes_upper_bound,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_webhook_payload(mailbox_name: str, region_name: str, **kwargs) -> WebhookPayload:\n        payload_kwargs = {\n            \"request_method\": \"POST\",\n            \"request_path\": \"/extensions/github/webhook/\",\n            \"request_headers\": '{\"Content-Type\": \"application/json\"}',\n            \"request_body\": \"{}\",\n            **kwargs,\n        }\n        return WebhookPayload.objects.create(\n            mailbox_name=mailbox_name, region_name=region_name, **payload_kwargs\n        )\n\n    @staticmethod\n    def create_uptime_subscription(\n        type: str,\n        subscription_id: str | None,\n        status: UptimeSubscription.Status,\n        url: str | None,\n        url_domain: str,\n        url_domain_suffix: str,\n        host_provider_id: str,\n        host_provider_name: str,\n        interval_seconds: IntervalSecondsLiteral,\n        timeout_ms: int,\n        method,\n        headers,\n        body,\n        date_updated: datetime,\n        uptime_status: UptimeStatus,\n        uptime_status_update_date: datetime,\n        trace_sampling: bool = False,\n    ):\n        if url is None:\n            url = petname.generate().title()\n            url = f\"http://{url}.com\"\n\n        return UptimeSubscription.objects.create(\n            type=type,\n            subscription_id=subscription_id,\n            status=status.value,\n            url=url,\n            url_domain=url_domain,\n            url_domain_suffix=url_domain_suffix,\n            host_provider_id=host_provider_id,\n            host_provider_name=host_provider_name,\n            interval_seconds=interval_seconds,\n            timeout_ms=timeout_ms,\n            date_updated=date_updated,\n            method=method,\n            headers=headers,\n            body=body,\n            trace_sampling=trace_sampling,\n            uptime_status=uptime_status,\n            uptime_status_update_date=uptime_status_update_date,\n        )\n\n    @staticmethod\n    def create_project_uptime_subscription(\n        project: Project,\n        env: Environment | None,\n        uptime_subscription: UptimeSubscription,\n        status: int,\n        mode: UptimeMonitorMode,\n        name: str | None,\n        owner: Actor | None,\n        id: int | None,\n    ):\n        if name is None:\n            name = petname.generate().title()\n        owner_team_id = None\n        owner_user_id = None\n        if owner:\n            if owner.is_team:\n                owner_team_id = owner.id\n            elif owner.is_user:\n                owner_user_id = owner.id\n\n        return ProjectUptimeSubscription.objects.create(\n            uptime_subscription=uptime_subscription,\n            project=project,\n            environment=env,\n            status=status,\n            mode=mode,\n            name=name,\n            owner_team_id=owner_team_id,\n            owner_user_id=owner_user_id,\n            pk=id,\n        )\n\n    @staticmethod\n    def create_uptime_subscription_region(\n        subscription: UptimeSubscription,\n        region_slug: str,\n        mode: UptimeSubscriptionRegion.RegionMode,\n    ) -> UptimeSubscriptionRegion:\n        return UptimeSubscriptionRegion.objects.create(\n            uptime_subscription=subscription,\n            region_slug=region_slug,\n            mode=mode,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard(\n        organization: Organization | None = None,\n        title: str | None = None,\n        created_by: User | None = None,\n        **kwargs,\n    ):\n        if organization is None:\n            organization = Factories.create_organization()\n        if created_by is None:\n            created_by = Factories.create_user()\n            Factories.create_member(organization=organization, user=created_by, role=\"owner\")\n        if title is None:\n            title = petname.generate(2, \" \", letters=10).title()\n        return Dashboard.objects.create(\n            organization=organization, title=title, created_by_id=created_by.id, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard_widget(\n        order: int,\n        dashboard: Dashboard | None = None,\n        title: str | None = None,\n        display_type: int | None = None,\n        **kwargs,\n    ):\n        if dashboard is None:\n            dashboard = Factories.create_dashboard()\n        if display_type is None:\n            display_type = DashboardWidgetDisplayTypes.AREA_CHART\n        if title is None:\n            title = petname.generate(2, \" \", letters=10).title()\n\n        return DashboardWidget.objects.create(\n            dashboard=dashboard, title=title, display_type=display_type, order=order, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard_widget_query(\n        order: int,\n        widget: DashboardWidget | None = None,\n        name: str | None = None,\n        **kwargs,\n    ):\n        if widget is None:\n            widget = Factories.create_dashboard_widget(order=order)\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        return DashboardWidgetQuery.objects.create(widget=widget, name=name, order=order, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_workflow(\n        name: str | None = None,\n        organization: Organization | None = None,\n        config: dict[str, Any] | None = None,\n        **kwargs,\n    ) -> Workflow:\n        if organization is None:\n            organization = Factories.create_organization()\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        if config is None:\n            config = {}\n        return Workflow.objects.create(\n            organization=organization, name=name, config=config, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition_group(\n        organization: Organization | None = None,\n        **kwargs,\n    ) -> DataConditionGroup:\n        if organization is None:\n            organization = Factories.create_organization()\n        return DataConditionGroup.objects.create(organization=organization, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_workflow_data_condition_group(\n        workflow: Workflow | None = None,\n        condition_group: DataConditionGroup | None = None,\n        **kwargs,\n    ) -> WorkflowDataConditionGroup:\n        if workflow is None:\n            workflow = Factories.create_workflow()\n\n        if not condition_group:\n            condition_group = Factories.create_data_condition_group()\n\n        return WorkflowDataConditionGroup.objects.create(\n            workflow=workflow, condition_group=condition_group\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition(\n        condition_group: DataConditionGroup | None = None, **kwargs\n    ) -> DataCondition:\n        if condition_group is None:\n            condition_group = Factories.create_data_condition_group()\n        return DataCondition.objects.create(condition_group=condition_group, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_source(\n        organization: Organization | None = None,\n        source_id: str | None = None,\n        type: str | None = None,\n        **kwargs,\n    ) -> DataSource:\n        if organization is None:\n            organization = Factories.create_organization()\n        if source_id is None:\n            source_id = str(random.randint(1, 10000))\n        if type is None:\n            type = data_source_type_registry.get_key(QuerySubscriptionDataSourceHandler)\n        return DataSource.objects.create(organization=organization, source_id=source_id, type=type)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector(\n        name: str | None = None,\n        config: dict | None = None,\n        **kwargs,\n    ) -> Detector:\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        if config is None:\n            config = default_detector_config_data.get(kwargs[\"type\"], {})\n\n        return Detector.objects.create(\n            name=name,\n            config=config,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector_state(\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> DetectorState:\n        if detector is None:\n            detector = Factories.create_detector()\n\n        return DetectorState.objects.create(detector=detector, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_source_detector(\n        data_source: DataSource | None = None,\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> DataSourceDetector:\n        if data_source is None:\n            data_source = Factories.create_data_source()\n        if detector is None:\n            detector = Factories.create_detector()\n        return DataSourceDetector.objects.create(data_source=data_source, detector=detector)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_action(\n        config: dict[str, Any] | None = None,\n        type: Action.Type | None = None,\n        data: dict[str, Any] | None = None,\n        **kwargs,\n    ) -> Action:\n        if config is None and type is None and data is None:\n            # Default to a slack action with nice defaults so someone can just do\n            # self.create_action() and have a sane default\n            config = {\n                \"target_identifier\": \"1\",\n                \"target_display\": \"Sentry User\",\n                \"target_type\": ActionTarget.SPECIFIC,\n            }\n\n            data = {\"notes\": \"bufos are great\", \"tags\": \"bufo-bot\"}\n\n        if config is None:\n            config = {}\n\n        if data is None:\n            data = {}\n\n        if type is None:\n            type = Action.Type.SLACK\n\n        return Action.objects.create(type=type, config=config, data=data, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector_workflow(\n        detector: Detector | None = None,\n        workflow: Workflow | None = None,\n        **kwargs,\n    ) -> DetectorWorkflow:\n        if detector is None:\n            detector = Factories.create_detector()\n        if workflow is None:\n            workflow = Factories.create_workflow()\n        return DetectorWorkflow.objects.create(detector=detector, workflow=workflow, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_workflow(\n        alert_rule_id: int | None = None,\n        rule_id: int | None = None,\n        workflow: Workflow | None = None,\n        **kwargs,\n    ) -> AlertRuleWorkflow:\n        if rule_id is None and alert_rule_id is None:\n            raise ValueError(\"Either rule_id or alert_rule_id must be provided\")\n\n        if rule_id is not None and alert_rule_id is not None:\n            raise ValueError(\"Only one of rule_id or alert_rule_id can be provided\")\n\n        if workflow is None:\n            workflow = Factories.create_workflow()\n\n        return AlertRuleWorkflow.objects.create(\n            alert_rule_id=alert_rule_id, rule_id=rule_id, workflow=workflow, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_group_open_period(\n        incident: Incident,\n        group_open_period: GroupOpenPeriod,\n        **kwargs,\n    ) -> IncidentGroupOpenPeriod:\n        return IncidentGroupOpenPeriod.objects.create(\n            incident_id=incident.id,\n            incident_identifier=incident.identifier,\n            group_open_period=group_open_period,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_detector(\n        alert_rule_id: int | None = None,\n        rule_id: int | None = None,\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> AlertRuleDetector:\n        if rule_id is None and alert_rule_id is None:\n            raise ValueError(\"Either rule_id or alert_rule_id must be provided\")\n\n        if rule_id is not None and alert_rule_id is not None:\n            raise ValueError(\"Only one of rule_id or alert_rule_id can be provided\")\n\n        if detector is None:\n            detector = Factories.create_detector()\n\n        return AlertRuleDetector.objects.create(\n            alert_rule_id=alert_rule_id, rule_id=rule_id, detector=detector, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_action_alert_rule_trigger_action(\n        alert_rule_trigger_action_id: int,\n        action: Action | None = None,\n        **kwargs,\n    ) -> ActionAlertRuleTriggerAction:\n        if action is None:\n            action = Factories.create_action()\n\n        return ActionAlertRuleTriggerAction.objects.create(\n            action=action, alert_rule_trigger_action_id=alert_rule_trigger_action_id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition_group_action(\n        action: Action | None = None,\n        condition_group: DataConditionGroup | None = None,\n        **kwargs,\n    ) -> DataConditionGroupAction:\n        if action is None:\n            action = Factories.create_action()\n        if condition_group is None:\n            condition_group = Factories.create_data_condition_group()\n        return DataConditionGroupAction.objects.create(\n            action=action, condition_group=condition_group, **kwargs\n        )\n",
    "file_path": "/Users/wangyue/Code/CodeReviewData/ReviewDataset/sentry-greptile/src/sentry/testutils/factories.py",
    "line_count": 2375,
    "encoding": "utf-8",
    "error": null
}

Message 21 [Assistant]:
--------------------------------------------------------------------------------
Content:
让我搜索 `_set_sample_rate_from_error_sampling` 的调用点：
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "_set_sample_rate_from_error_sampling",
      "is_regex": false,
      "context_lines": 30
    },
    "id": "call_00_iwmcXOJREon6UIje4Malxrfj",
    "type": "tool_call"
  }
]

Message 22 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_iwmcXOJREon6UIje4Malxrfj
Result:
{
    "matches": [
        {
            "file": "src/sentry/testutils/factories.py",
            "line_number": 344,
            "matched_line": "def _set_sample_rate_from_error_sampling(normalized_data: MutableMapping[str, Any]) -> None:",
            "context_start_line": 314,
            "context_end_line": 374,
            "context": [
                "314:                     \"stack\": True,",
                "315:                     \"tags\": None,",
                "316:                     \"time_spent\": None,",
                "317:                 },",
                "318:             },",
                "319:         ]",
                "320:     },",
                "321:     \"tags\": [],",
                "322:     \"platform\": \"python\",",
                "323: }",
                "324: ",
                "325: default_detector_config_data = {",
                "326:     MetricIssue.slug: {\"threshold_period\": 1, \"detection_type\": \"static\"}",
                "327: }",
                "328: ",
                "329: ",
                "330: def _patch_artifact_manifest(path, org=None, release=None, project=None, extra_files=None):",
                "331:     with open(path, \"rb\") as fp:",
                "332:         manifest = orjson.loads(fp.read())",
                "333:     if org:",
                "334:         manifest[\"org\"] = org",
                "335:     if release:",
                "336:         manifest[\"release\"] = release",
                "337:     if project:",
                "338:         manifest[\"project\"] = project",
                "339:     for path in extra_files or {}:",
                "340:         manifest[\"files\"][path] = {\"url\": path}",
                "341:     return orjson.dumps(manifest).decode()",
                "342: ",
                "343: ",
                "344: def _set_sample_rate_from_error_sampling(normalized_data: MutableMapping[str, Any]) -> None:",
                "345:     \"\"\"Set 'sample_rate' on normalized_data if contexts.error_sampling.client_sample_rate is present and valid.\"\"\"",
                "346:     client_sample_rate = None",
                "347:     try:",
                "348:         client_sample_rate = (",
                "349:             normalized_data.get(\"contexts\", {}).get(\"error_sampling\", {}).get(\"client_sample_rate\")",
                "350:         )",
                "351:     except Exception:",
                "352:         pass",
                "353:     if client_sample_rate:",
                "354:         try:",
                "355:             normalized_data[\"sample_rate\"] = float(client_sample_rate)",
                "356:         except Exception:",
                "357:             pass",
                "358: ",
                "359: ",
                "360: # TODO(dcramer): consider moving to something more scalable like factoryboy",
                "361: class Factories:",
                "362:     @staticmethod",
                "363:     @assume_test_silo_mode(SiloMode.REGION)",
                "364:     def create_organization(name=None, owner=None, region: Region | str | None = None, **kwargs):",
                "365:         if not name:",
                "366:             name = petname.generate(2, \" \", letters=10).title()",
                "367: ",
                "368:         with contextlib.ExitStack() as ctx:",
                "369:             if region is None or SiloMode.get_current_mode() == SiloMode.MONOLITH:",
                "370:                 region_name = get_local_region().name",
                "371:             else:",
                "372:                 if isinstance(region, Region):",
                "373:                     region_name = region.name",
                "374:                 else:"
            ]
        },
        {
            "file": "src/sentry/testutils/factories.py",
            "line_number": 1049,
            "matched_line": "        _set_sample_rate_from_error_sampling(normalized_data)",
            "context_start_line": 1019,
            "context_end_line": 1079,
            "context": [
                "1019:     @staticmethod",
                "1020:     @assume_test_silo_mode(SiloMode.REGION)",
                "1021:     def store_event(",
                "1022:         data,",
                "1023:         project_id: int,",
                "1024:         assert_no_errors: bool = True,",
                "1025:         default_event_type: EventType | None = None,",
                "1026:         sent_at: datetime | None = None,",
                "1027:     ) -> Event:",
                "1028:         \"\"\"",
                "1029:         Like `create_event`, but closer to how events are actually",
                "1030:         ingested. Prefer to use this method over `create_event`",
                "1031:         \"\"\"",
                "1032: ",
                "1033:         # this creates a basic message event",
                "1034:         if default_event_type == EventType.DEFAULT:",
                "1035:             data.update({\"stacktrace\": copy.deepcopy(DEFAULT_EVENT_DATA[\"stacktrace\"])})",
                "1036: ",
                "1037:         # this creates an error event",
                "1038:         elif default_event_type == EventType.ERROR:",
                "1039:             data.update({\"exception\": [{\"value\": \"BadError\"}]})",
                "1040: ",
                "1041:         manager = EventManager(data, sent_at=sent_at)",
                "1042:         manager.normalize()",
                "1043:         if assert_no_errors:",
                "1044:             errors = manager.get_data().get(\"errors\")",
                "1045:             assert not errors, errors",
                "1046: ",
                "1047:         normalized_data = manager.get_data()",
                "1048: ",
                "1049:         _set_sample_rate_from_error_sampling(normalized_data)",
                "1050: ",
                "1051:         event = None",
                "1052: ",
                "1053:         # When fingerprint is present on transaction, inject performance problems",
                "1054:         if (",
                "1055:             normalized_data.get(\"type\") == \"transaction\"",
                "1056:             and normalized_data.get(\"fingerprint\") is not None",
                "1057:         ):",
                "1058:             with mock.patch(",
                "1059:                 \"sentry.event_manager._detect_performance_problems\",",
                "1060:                 Factories.inject_performance_problems,",
                "1061:             ):",
                "1062:                 event = manager.save(project_id)",
                "1063: ",
                "1064:         else:",
                "1065:             event = manager.save(project_id)",
                "1066: ",
                "1067:         if event.groups:",
                "1068:             for group in event.groups:",
                "1069:                 group.save()",
                "1070: ",
                "1071:         if event.group:",
                "1072:             event.group.save()",
                "1073: ",
                "1074:         return event",
                "1075: ",
                "1076:     @staticmethod",
                "1077:     @assume_test_silo_mode(SiloMode.REGION)",
                "1078:     def create_group(project, create_open_period=True, **kwargs):",
                "1079:         from sentry.models.group import GroupStatus"
            ]
        }
    ],
    "total": 2
}

Message 23 [Assistant]:
--------------------------------------------------------------------------------
Content:
```json
{
    "risk_type": "null_safety",
    "file_path": "src/sentry/testutils/factories.py",
    "line_number": [344, 358],
    "description": "函数 `_set_sample_rate_from_error_sampling` 存在两个潜在的空安全风险：1) 链式调用 `normalized_data.get(\"contexts\", {}).get(\"error_sampling\", {}).get(\"client_sample_rate\")` 假设 `normalized_data` 是字典类型且不为 None，但函数签名只要求 `MutableMapping[str, Any]`，未处理 `normalized_data` 为 None 或非字典类型的情况；2) 在设置 `sample_rate` 时直接赋值 `normalized_data[\"sample_rate\"]`，假设 `normalized_data` 是可变字典且支持键赋值，但某些 `MutableMapping` 实现可能不支持。",
    "confidence": 0.7,
    "severity": "warning",
    "suggestion": "1) 在函数开头添加对 `normalized_data` 是否为 None 或非字典类型的检查；2) 使用 `normalized_data.setdefault(\"sample_rate\", sample_rate)` 或先检查 `normalized_data` 是否支持 `__setitem__` 操作。"
}
```


Expert Analysis 2:
================================================================================
Risk Type: unknown
File: src/sentry/testutils/factories.py
Line: (344, 358)
Description: 函数逻辑存在业务意图问题：当 `client_sample_rate` 存在但无法转换为浮点数时（例如字符串 "invalid" 或空字符串），异常被静默忽略，`sample_rate` 字段不会被设置。这可能与预期行为不符——要么应设置默认值，要么应记录警告。此外，如果 `client_sample_rate` 为 0 或 False 等假值，也会被跳过，这可能错误地忽略了有效的零采样率。
--------------------------------------------------------------------------------

Analysis Result:
{
  "risk_type": "business_intent",
  "file_path": "src/sentry/testutils/factories.py",
  "line_number": [
    344,
    358
  ],
  "description": "函数逻辑存在业务意图问题：当 `client_sample_rate` 存在但无法转换为浮点数时（例如字符串 \"invalid\" 或空字符串），异常被静默忽略，`sample_rate` 字段不会被设置。这可能与预期行为不符——要么应设置默认值，要么应记录警告。此外，如果 `client_sample_rate` 为 0 或 False 等假值，也会被跳过，这可能错误地忽略了有效的零采样率。",
  "confidence": 0.8,
  "severity": "warning",
  "suggestion": "建议修改逻辑：1) 明确处理转换异常，可以设置默认值或记录警告；2) 正确处理假值（如0、False），确保有效的零采样率不被忽略。"
}

Risk Item:
{
  "risk_type": "business_intent",
  "file_path": "src/sentry/testutils/factories.py",
  "line_number": [
    344,
    358
  ],
  "description": "函数逻辑存在业务意图问题：当 `client_sample_rate` 存在但无法转换为浮点数时（例如字符串 \"invalid\" 或空字符串），异常被静默忽略，`sample_rate` 字段不会被设置。这可能与预期行为不符——要么应设置默认值，要么应记录警告。此外，如果 `client_sample_rate` 为 0 或 False 等假值，也会被跳过，这可能错误地忽略了有效的零采样率。",
  "confidence": 0.9,
  "severity": "warning",
  "suggestion": null
}

Conversation History (23 messages):
================================================================================

Message 1 [Assistant]:
--------------------------------------------------------------------------------
Content:
我将分析这个风险项。首先，让我读取完整的文件内容以获取更多上下文。
Tool Calls:
[
  {
    "name": "read_file",
    "args": {
      "file_path": "src/sentry/testutils/factories.py"
    },
    "id": "call_00_67ysgGHmQ4pHhISqm2ZUHCc2",
    "type": "tool_call"
  }
]

Message 2 [Tool]:
--------------------------------------------------------------------------------
Tool: read_file
Tool Call ID: call_00_67ysgGHmQ4pHhISqm2ZUHCc2
Result:
{
    "content": "from __future__ import annotations\n\nimport contextlib\nimport copy\nimport io\nimport os\nimport random\nimport zipfile\nfrom base64 import b64encode\nfrom binascii import hexlify\nfrom collections.abc import Mapping, MutableMapping, Sequence\nfrom datetime import UTC, datetime\nfrom enum import Enum\nfrom hashlib import sha1\nfrom importlib import import_module\nfrom typing import Any\nfrom unittest import mock\nfrom uuid import uuid4\n\nimport orjson\nimport petname\nfrom django.conf import settings\nfrom django.contrib.auth.models import AnonymousUser\nfrom django.core.files.base import ContentFile\nfrom django.db import router, transaction\nfrom django.test.utils import override_settings\nfrom django.utils import timezone\nfrom django.utils.text import slugify\n\nfrom sentry.auth.access import RpcBackedAccess\nfrom sentry.auth.services.auth.model import RpcAuthState, RpcMemberSsoState\nfrom sentry.constants import SentryAppInstallationStatus, SentryAppStatus\nfrom sentry.event_manager import EventManager\nfrom sentry.eventstore.models import Event\nfrom sentry.hybridcloud.models.outbox import RegionOutbox, outbox_context\nfrom sentry.hybridcloud.models.webhookpayload import WebhookPayload\nfrom sentry.hybridcloud.outbox.category import OutboxCategory, OutboxScope\nfrom sentry.incidents.grouptype import MetricIssue\nfrom sentry.incidents.logic import (\n    create_alert_rule,\n    create_alert_rule_trigger,\n    create_alert_rule_trigger_action,\n    query_datasets_to_type,\n)\nfrom sentry.incidents.models.alert_rule import (\n    AlertRuleDetectionType,\n    AlertRuleThresholdType,\n    AlertRuleTriggerAction,\n)\nfrom sentry.incidents.models.incident import (\n    Incident,\n    IncidentActivity,\n    IncidentProject,\n    IncidentTrigger,\n    IncidentType,\n    TriggerStatus,\n)\nfrom sentry.integrations.models.doc_integration import DocIntegration\nfrom sentry.integrations.models.doc_integration_avatar import DocIntegrationAvatar\nfrom sentry.integrations.models.external_actor import ExternalActor\nfrom sentry.integrations.models.external_issue import ExternalIssue\nfrom sentry.integrations.models.integration import Integration\nfrom sentry.integrations.models.integration_external_project import IntegrationExternalProject\nfrom sentry.integrations.models.integration_feature import (\n    Feature,\n    IntegrationFeature,\n    IntegrationTypes,\n)\nfrom sentry.integrations.models.organization_integration import OrganizationIntegration\nfrom sentry.integrations.models.repository_project_path_config import RepositoryProjectPathConfig\nfrom sentry.integrations.types import ExternalProviders\nfrom sentry.issues.grouptype import get_group_type_by_type_id\nfrom sentry.models.activity import Activity\nfrom sentry.models.apikey import ApiKey\nfrom sentry.models.apitoken import ApiToken\nfrom sentry.models.artifactbundle import ArtifactBundle\nfrom sentry.models.authidentity import AuthIdentity\nfrom sentry.models.authprovider import AuthProvider\nfrom sentry.models.commit import Commit\nfrom sentry.models.commitauthor import CommitAuthor\nfrom sentry.models.commitfilechange import CommitFileChange\nfrom sentry.models.dashboard import Dashboard\nfrom sentry.models.dashboard_widget import (\n    DashboardWidget,\n    DashboardWidgetDisplayTypes,\n    DashboardWidgetQuery,\n)\nfrom sentry.models.debugfile import ProjectDebugFile\nfrom sentry.models.environment import Environment\nfrom sentry.models.files.control_file import ControlFile\nfrom sentry.models.files.file import File\nfrom sentry.models.group import Group\nfrom sentry.models.grouphistory import GroupHistory\nfrom sentry.models.grouplink import GroupLink\nfrom sentry.models.groupopenperiod import GroupOpenPeriod\nfrom sentry.models.grouprelease import GroupRelease\nfrom sentry.models.organization import Organization\nfrom sentry.models.organizationmapping import OrganizationMapping\nfrom sentry.models.organizationmember import OrganizationMember\nfrom sentry.models.organizationmemberinvite import OrganizationMemberInvite\nfrom sentry.models.organizationmemberteam import OrganizationMemberTeam\nfrom sentry.models.organizationslugreservation import OrganizationSlugReservation\nfrom sentry.models.orgauthtoken import OrgAuthToken\nfrom sentry.models.project import Project\nfrom sentry.models.projectbookmark import ProjectBookmark\nfrom sentry.models.projectcodeowners import ProjectCodeOwners\nfrom sentry.models.projecttemplate import ProjectTemplate\nfrom sentry.models.release import Release, ReleaseStatus\nfrom sentry.models.releasecommit import ReleaseCommit\nfrom sentry.models.releaseenvironment import ReleaseEnvironment\nfrom sentry.models.releasefile import ReleaseFile, update_artifact_index\nfrom sentry.models.releaseprojectenvironment import ReleaseProjectEnvironment\nfrom sentry.models.repository import Repository\nfrom sentry.models.rule import Rule\nfrom sentry.models.rulesnooze import RuleSnooze\nfrom sentry.models.savedsearch import SavedSearch\nfrom sentry.models.team import Team\nfrom sentry.models.userreport import UserReport\nfrom sentry.notifications.models.notificationaction import (\n    ActionService,\n    ActionTarget,\n    ActionTrigger,\n    NotificationAction,\n)\nfrom sentry.notifications.models.notificationsettingprovider import NotificationSettingProvider\nfrom sentry.organizations.services.organization import RpcOrganization, RpcUserOrganizationContext\nfrom sentry.performance_issues.performance_problem import PerformanceProblem\nfrom sentry.sentry_apps.installations import (\n    SentryAppInstallationCreator,\n    SentryAppInstallationTokenCreator,\n)\nfrom sentry.sentry_apps.logic import SentryAppCreator\nfrom sentry.sentry_apps.models.platformexternalissue import PlatformExternalIssue\nfrom sentry.sentry_apps.models.sentry_app import SentryApp\nfrom sentry.sentry_apps.models.sentry_app_avatar import SentryAppAvatar\nfrom sentry.sentry_apps.models.sentry_app_installation import SentryAppInstallation\nfrom sentry.sentry_apps.models.sentry_app_installation_for_provider import (\n    SentryAppInstallationForProvider,\n)\nfrom sentry.sentry_apps.models.servicehook import ServiceHook\nfrom sentry.sentry_apps.services.hook import hook_service\nfrom sentry.sentry_apps.token_exchange.grant_exchanger import GrantExchanger\nfrom sentry.signals import project_created\nfrom sentry.silo.base import SiloMode\nfrom sentry.snuba.dataset import Dataset\nfrom sentry.snuba.models import QuerySubscriptionDataSourceHandler\nfrom sentry.tempest.models import MessageType as TempestMessageType\nfrom sentry.tempest.models import TempestCredentials\nfrom sentry.testutils.outbox import outbox_runner\nfrom sentry.testutils.silo import assume_test_silo_mode\nfrom sentry.types.activity import ActivityType\nfrom sentry.types.actor import Actor\nfrom sentry.types.region import Region, get_local_region, get_region_by_name\nfrom sentry.types.token import AuthTokenType\nfrom sentry.uptime.models import (\n    IntervalSecondsLiteral,\n    ProjectUptimeSubscription,\n    UptimeStatus,\n    UptimeSubscription,\n    UptimeSubscriptionRegion,\n)\nfrom sentry.uptime.types import UptimeMonitorMode\nfrom sentry.users.models.identity import Identity, IdentityProvider, IdentityStatus\nfrom sentry.users.models.user import User\nfrom sentry.users.models.user_avatar import UserAvatar\nfrom sentry.users.models.user_option import UserOption\nfrom sentry.users.models.useremail import UserEmail\nfrom sentry.users.models.userpermission import UserPermission\nfrom sentry.users.models.userrole import UserRole\nfrom sentry.users.services.user import RpcUser\nfrom sentry.utils import loremipsum\nfrom sentry.workflow_engine.models import (\n    Action,\n    ActionAlertRuleTriggerAction,\n    AlertRuleDetector,\n    AlertRuleWorkflow,\n    DataCondition,\n    DataConditionGroup,\n    DataConditionGroupAction,\n    DataSource,\n    DataSourceDetector,\n    Detector,\n    DetectorState,\n    DetectorWorkflow,\n    IncidentGroupOpenPeriod,\n    Workflow,\n    WorkflowDataConditionGroup,\n)\nfrom sentry.workflow_engine.registry import data_source_type_registry\nfrom social_auth.models import UserSocialAuth\n\n\nclass EventType(Enum):\n    ERROR = \"error\"\n    DEFAULT = \"default\"\n\n\ndef get_fixture_path(*parts: str) -> str:\n    path = os.path.realpath(__file__)\n    for _ in range(4):  # src/sentry/testutils/{__file__}\n        path = os.path.dirname(path)\n    return os.path.join(path, \"fixtures\", *parts)\n\n\ndef make_sentence(words=None):\n    if words is None:\n        words = int(random.weibullvariate(8, 3))\n    return \" \".join(random.choice(loremipsum.words) for _ in range(words))\n\n\ndef make_word(words=None):\n    if words is None:\n        words = int(random.weibullvariate(8, 3))\n    return random.choice(loremipsum.words)\n\n\nDEFAULT_EVENT_DATA = {\n    \"extra\": {\n        \"loadavg\": [0.97607421875, 0.88330078125, 0.833984375],\n        \"sys.argv\": [\n            \"/Users/dcramer/.virtualenvs/sentry/bin/raven\",\n            \"test\",\n            \"https://ebc35f33e151401f9deac549978bda11:f3403f81e12e4c24942d505f086b2cad@sentry.io/1\",\n        ],\n        \"user\": \"dcramer\",\n    },\n    \"modules\": {\"raven\": \"3.1.13\"},\n    \"request\": {\n        \"cookies\": {},\n        \"data\": {},\n        \"env\": {},\n        \"headers\": {},\n        \"method\": \"GET\",\n        \"query_string\": \"\",\n        \"url\": \"http://example.com\",\n    },\n    \"stacktrace\": {\n        \"frames\": [\n            {\n                \"abs_path\": \"www/src/sentry/models/foo.py\",\n                \"context_line\": \"                        string_max_length=self.string_max_length)\",\n                \"filename\": \"sentry/models/foo.py\",\n                \"function\": \"build_msg\",\n                \"in_app\": True,\n                \"lineno\": 29,\n                \"module\": \"raven.base\",\n                \"post_context\": [\n                    \"                },\",\n                    \"            })\",\n                    \"\",\n                    \"        if 'stacktrace' in data:\",\n                    \"            if self.include_paths:\",\n                ],\n                \"pre_context\": [\n                    \"\",\n                    \"            data.update({\",\n                    \"                'stacktrace': {\",\n                    \"                    'frames': get_stack_info(frames,\",\n                    \"                        list_max_length=self.list_max_length,\",\n                ],\n                \"vars\": {\n                    \"culprit\": \"raven.scripts.runner\",\n                    \"date\": \"datetime.datetime(2013, 2, 14, 20, 6, 33, 479471)\",\n                    \"event_id\": \"598fb19363e745ec8be665e6ba88b1b2\",\n                    \"event_type\": \"raven.events.Message\",\n                    \"frames\": \"<generator object iter_stack_frames at 0x103fef050>\",\n                    \"handler\": \"<raven.events.Message object at 0x103feb710>\",\n                    \"k\": \"logentry\",\n                    \"public_key\": None,\n                    \"result\": {\n                        \"logentry\": \"{'message': 'This is a test message generated using ``raven test``', 'params': []}\"\n                    },\n                    \"self\": \"<raven.base.Client object at 0x104397f10>\",\n                    \"stack\": True,\n                    \"tags\": None,\n                    \"time_spent\": None,\n                },\n            },\n            {\n                \"abs_path\": \"/Users/dcramer/.virtualenvs/sentry/lib/python2.7/site-packages/raven/base.py\",\n                \"context_line\": \"                        string_max_length=self.string_max_length)\",\n                \"filename\": \"raven/base.py\",\n                \"function\": \"build_msg\",\n                \"in_app\": False,\n                \"lineno\": 290,\n                \"module\": \"raven.base\",\n                \"post_context\": [\n                    \"                },\",\n                    \"            })\",\n                    \"\",\n                    \"        if 'stacktrace' in data:\",\n                    \"            if self.include_paths:\",\n                ],\n                \"pre_context\": [\n                    \"\",\n                    \"            data.update({\",\n                    \"                'stacktrace': {\",\n                    \"                    'frames': get_stack_info(frames,\",\n                    \"                        list_max_length=self.list_max_length,\",\n                ],\n                \"vars\": {\n                    \"culprit\": \"raven.scripts.runner\",\n                    \"date\": \"datetime.datetime(2013, 2, 14, 20, 6, 33, 479471)\",\n                    \"event_id\": \"598fb19363e745ec8be665e6ba88b1b2\",\n                    \"event_type\": \"raven.events.Message\",\n                    \"frames\": \"<generator object iter_stack_frames at 0x103fef050>\",\n                    \"handler\": \"<raven.events.Message object at 0x103feb710>\",\n                    \"k\": \"logentry\",\n                    \"public_key\": None,\n                    \"result\": {\n                        \"logentry\": \"{'message': 'This is a test message generated using ``raven test``', 'params': []}\"\n                    },\n                    \"self\": \"<raven.base.Client object at 0x104397f10>\",\n                    \"stack\": True,\n                    \"tags\": None,\n                    \"time_spent\": None,\n                },\n            },\n        ]\n    },\n    \"tags\": [],\n    \"platform\": \"python\",\n}\n\ndefault_detector_config_data = {\n    MetricIssue.slug: {\"threshold_period\": 1, \"detection_type\": \"static\"}\n}\n\n\ndef _patch_artifact_manifest(path, org=None, release=None, project=None, extra_files=None):\n    with open(path, \"rb\") as fp:\n        manifest = orjson.loads(fp.read())\n    if org:\n        manifest[\"org\"] = org\n    if release:\n        manifest[\"release\"] = release\n    if project:\n        manifest[\"project\"] = project\n    for path in extra_files or {}:\n        manifest[\"files\"][path] = {\"url\": path}\n    return orjson.dumps(manifest).decode()\n\n\ndef _set_sample_rate_from_error_sampling(normalized_data: MutableMapping[str, Any]) -> None:\n    \"\"\"Set 'sample_rate' on normalized_data if contexts.error_sampling.client_sample_rate is present and valid.\"\"\"\n    client_sample_rate = None\n    try:\n        client_sample_rate = (\n            normalized_data.get(\"contexts\", {}).get(\"error_sampling\", {}).get(\"client_sample_rate\")\n        )\n    except Exception:\n        pass\n    if client_sample_rate:\n        try:\n            normalized_data[\"sample_rate\"] = float(client_sample_rate)\n        except Exception:\n            pass\n\n\n# TODO(dcramer): consider moving to something more scalable like factoryboy\nclass Factories:\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_organization(name=None, owner=None, region: Region | str | None = None, **kwargs):\n        if not name:\n            name = petname.generate(2, \" \", letters=10).title()\n\n        with contextlib.ExitStack() as ctx:\n            if region is None or SiloMode.get_current_mode() == SiloMode.MONOLITH:\n                region_name = get_local_region().name\n            else:\n                if isinstance(region, Region):\n                    region_name = region.name\n                else:\n                    region_obj = get_region_by_name(region)  # Verify it exists\n                    region_name = region_obj.name\n\n                ctx.enter_context(\n                    override_settings(SILO_MODE=SiloMode.REGION, SENTRY_REGION=region_name)\n                )\n\n            with outbox_context(flush=False):\n                org = Organization.objects.create(name=name, **kwargs)\n\n            with assume_test_silo_mode(SiloMode.CONTROL):\n                # Organization mapping creation relies on having a matching org slug reservation\n                OrganizationSlugReservation(\n                    organization_id=org.id,\n                    region_name=region_name,\n                    user_id=owner.id if owner else -1,\n                    slug=org.slug,\n                ).save(unsafe_write=True)\n\n            # Manually replicate org data after adding an org slug reservation\n            org.handle_async_replication(org.id)\n\n            # Flush remaining organization update outboxes accumulated by org create\n            RegionOutbox(\n                shard_identifier=org.id,\n                shard_scope=OutboxScope.ORGANIZATION_SCOPE,\n                category=OutboxCategory.ORGANIZATION_UPDATE,\n            ).drain_shard()\n\n        if owner:\n            Factories.create_member(organization=org, user_id=owner.id, role=\"owner\")\n        return org\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_org_mapping(org=None, **kwds):\n        if org:\n            kwds.setdefault(\"organization_id\", org.id)\n            kwds.setdefault(\"slug\", org.slug)\n            kwds.setdefault(\"name\", org.name)\n            kwds.setdefault(\"idempotency_key\", uuid4().hex)\n            kwds.setdefault(\"region_name\", \"na\")\n        return OrganizationMapping.objects.create(**kwds)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_member(teams=None, team_roles=None, **kwargs):\n        kwargs.setdefault(\"role\", \"member\")\n        teamRole = kwargs.pop(\"teamRole\", None)\n\n        # user_id will have precedence over user\n        user = kwargs.pop(\"user\", None)\n        user_id = kwargs.pop(\"user_id\", None)\n        if not user_id and user:\n            user_id = user.id\n        kwargs[\"user_id\"] = user_id\n\n        # inviter_id will have precedence over inviter\n        inviter = kwargs.pop(\"inviter\", None)\n        inviter_id = kwargs.pop(\"inviter_id\", None)\n        if not inviter_id and inviter:\n            inviter_id = inviter.id\n        kwargs[\"inviter_id\"] = inviter_id\n\n        om = OrganizationMember.objects.create(**kwargs)\n\n        if team_roles:\n            for team, role in team_roles:\n                Factories.create_team_membership(team=team, member=om, role=role)\n        elif teams:\n            for team in teams:\n                Factories.create_team_membership(team=team, member=om, role=teamRole)\n        return om\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_member_invite(\n        organization: Organization | None = None,\n        email: str | None = None,\n        **kwargs,\n    ) -> OrganizationMemberInvite:\n        if organization is None:\n            organization = Factories.create_organization()\n        if email is None:\n            email = f\"{petname.generate().title()}@email.com\"\n        om = OrganizationMember.objects.create(organization=organization)\n        return OrganizationMemberInvite.objects.create(\n            organization=organization, organization_member_id=om.id, email=email, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_team_membership(team, member=None, user=None, role=None):\n        if member is None:\n            member, created = OrganizationMember.objects.get_or_create(\n                user_id=user.id if user else None,\n                organization=team.organization,\n                defaults={\"role\": \"member\"},\n            )\n\n        return OrganizationMemberTeam.objects.create(\n            team=team, organizationmember=member, is_active=True, role=role\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_api_key(organization, **kwargs) -> ApiKey:\n        return ApiKey.objects.create(organization_id=organization.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_auth_provider(**kwargs):\n        return AuthProvider.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_auth_identity(**kwargs):\n        return AuthIdentity.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_auth_token(user, scope_list: list[str] | None = None, **kwargs) -> ApiToken:\n        if scope_list is None:\n            scope_list = []\n        return ApiToken.objects.create(\n            user=user,\n            scope_list=scope_list,\n            token_type=AuthTokenType.USER,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_org_auth_token(*args, **kwargs) -> OrgAuthToken:\n        return OrgAuthToken.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_team(organization, **kwargs):\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n        if not kwargs.get(\"slug\"):\n            kwargs[\"slug\"] = slugify(str(kwargs[\"name\"]))\n        members = kwargs.pop(\"members\", None)\n\n        team = Team.objects.create(organization=organization, **kwargs)\n        if members:\n            for user in members:\n                Factories.create_team_membership(team=team, user=user)\n        return team\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_environment(project, **kwargs):\n        name = kwargs.get(\"name\", petname.generate(3, \" \", letters=10)[:64])\n\n        organization = kwargs.get(\"organization\")\n        organization_id = organization.id if organization else project.organization_id\n\n        env = Environment.objects.create(organization_id=organization_id, name=name)\n        env.add_project(project, is_hidden=kwargs.get(\"is_hidden\"))\n        return env\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project(\n        organization=None, teams=None, fire_project_created=False, **kwargs\n    ) -> Project:\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n        if not kwargs.get(\"slug\"):\n            kwargs[\"slug\"] = slugify(str(kwargs[\"name\"]))\n        if not organization and teams:\n            organization = teams[0].organization\n\n        with transaction.atomic(router.db_for_write(Project)):\n            project = Project.objects.create(organization=organization, **kwargs)\n            if teams:\n                for team in teams:\n                    project.add_team(team)\n            if fire_project_created:\n                project_created.send(\n                    project=project, user=AnonymousUser(), default_rules=True, sender=Factories\n                )\n        return project\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_template(project=None, organization=None, **kwargs) -> ProjectTemplate:\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n\n        with transaction.atomic(router.db_for_write(Project)):\n            project_template = ProjectTemplate.objects.create(organization=organization, **kwargs)\n\n        return project_template\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_bookmark(project, user):\n        return ProjectBookmark.objects.create(project_id=project.id, user_id=user.id)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_rule(\n        project,\n        action_data=None,\n        allow_no_action_data=False,\n        condition_data=None,\n        name=\"Test Alert\",\n        action_match=\"all\",\n        filter_match=\"all\",\n        frequency=30,\n        **kwargs,\n    ):\n        actions = None\n        if not allow_no_action_data:\n            action_data = action_data or [\n                {\n                    \"id\": \"sentry.rules.actions.notify_event.NotifyEventAction\",\n                    \"name\": \"Send a notification (for all legacy integrations)\",\n                },\n                {\n                    \"id\": \"sentry.rules.actions.notify_event_service.NotifyEventServiceAction\",\n                    \"service\": \"mail\",\n                    \"name\": \"Send a notification via mail\",\n                },\n            ]\n            actions = action_data\n        condition_data = condition_data or [\n            {\n                \"id\": \"sentry.rules.conditions.first_seen_event.FirstSeenEventCondition\",\n                \"name\": \"A new issue is created\",\n            },\n            {\n                \"id\": \"sentry.rules.conditions.every_event.EveryEventCondition\",\n                \"name\": \"The event occurs\",\n            },\n        ]\n        data = {\n            \"conditions\": condition_data,\n            \"action_match\": action_match,\n            \"filter_match\": filter_match,\n            \"frequency\": frequency,\n        }\n        if actions:\n            data[\"actions\"] = actions\n\n        return Rule.objects.create(\n            label=name,\n            project=project,\n            data=data,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_slack_project_rule(project, integration_id, channel_id=None, channel_name=None):\n        action_data = [\n            {\n                \"id\": \"sentry.rules.actions.notify_event.SlackNotifyServiceAction\",\n                \"name\": \"Send a Slack notification\",\n                \"workspace\": integration_id,\n                \"channel_id\": channel_id or \"123453\",\n                \"channel\": channel_name or \"#general\",\n            }\n        ]\n        return Factories.create_project_rule(project, action_data)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_key(project):\n        return project.key_set.get_or_create()[0]\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_tempest_credentials(\n        project: Project,\n        created_by: User | None = None,\n        client_id: str | None = None,\n        client_secret: str | None = None,\n        message: str = \"\",\n        message_type: str | None = None,\n        latest_fetched_item_id: str | None = None,\n    ):\n        if client_id is None:\n            client_id = str(uuid4())\n        if client_secret is None:\n            client_secret = str(uuid4())\n        if message_type is None:\n            message_type = TempestMessageType.ERROR\n\n        return TempestCredentials.objects.create(\n            project=project,\n            created_by_id=created_by.id if created_by else None,\n            client_id=client_id,\n            client_secret=client_secret,\n            message=message,\n            message_type=message_type,\n            latest_fetched_item_id=latest_fetched_item_id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release(\n        project: Project,\n        user: User | None = None,\n        version: str | None = None,\n        date_added: datetime | None = None,\n        additional_projects: Sequence[Project] | None = None,\n        environments: Sequence[Environment] | None = None,\n        date_released: datetime | None = None,\n        adopted: datetime | None = None,\n        unadopted: datetime | None = None,\n        status: int | None = ReleaseStatus.OPEN,\n    ):\n        if version is None:\n            version = hexlify(os.urandom(20)).decode()\n\n        if date_added is None:\n            date_added = timezone.now()\n\n        if additional_projects is None:\n            additional_projects = []\n\n        release = Release.objects.create(\n            version=version,\n            organization_id=project.organization_id,\n            date_added=date_added,\n            date_released=date_released,\n            status=status,\n        )\n\n        release.add_project(project)\n        for additional_project in additional_projects:\n            release.add_project(additional_project)\n\n        for environment in environments or []:\n            ReleaseEnvironment.objects.create(\n                organization=project.organization, release=release, environment=environment\n            )\n            for project in [project, *additional_projects]:\n                ReleaseProjectEnvironment.objects.create(\n                    project=project,\n                    release=release,\n                    environment=environment,\n                    adopted=adopted,\n                    unadopted=unadopted,\n                )\n\n        Activity.objects.create(\n            type=ActivityType.RELEASE.value,\n            project=project,\n            ident=Activity.get_version_ident(version),\n            user_id=user.id if user else None,\n            data={\"version\": version},\n        )\n\n        # add commits\n        if user:\n            author = Factories.create_commit_author(project=project, user=user)\n            repo = Factories.create_repo(project, name=f\"organization-{project.slug}\")\n            commit = Factories.create_commit(\n                project=project,\n                repo=repo,\n                author=author,\n                release=release,\n                key=\"deadbeef\",\n                message=\"placeholder commit message\",\n            )\n\n            release.update(authors=[str(author.id)], commit_count=1, last_commit_id=commit.id)\n\n        return release\n\n    @staticmethod\n    def create_group_release(project: Project, group: Group, release: Release) -> GroupRelease:\n        return GroupRelease.objects.create(\n            project_id=project.id,\n            group_id=group.id,\n            release_id=release.id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release_file(release_id, file=None, name=None, dist_id=None):\n        if file is None:\n            file = Factories.create_file(\n                name=\"log.txt\",\n                size=32,\n                headers={\"Content-Type\": \"text/plain\"},\n                checksum=\"dc1e3f3e411979d336c3057cce64294f3420f93a\",\n            )\n\n        if name is None:\n            name = file.name\n\n        organization_id = Release.objects.get(pk=release_id).organization.id\n\n        return ReleaseFile.objects.create(\n            organization_id=organization_id,\n            release_id=release_id,\n            name=name,\n            file=file,\n            dist_id=dist_id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_artifact_bundle_zip(\n        org=None, release=None, project=None, extra_files=None, fixture_path=\"artifact_bundle\"\n    ):\n        bundle = io.BytesIO()\n        bundle_dir = get_fixture_path(fixture_path)\n        with zipfile.ZipFile(bundle, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n            for path, content in (extra_files or {}).items():\n                zipf.writestr(path, content)\n            for path, _, files in os.walk(bundle_dir):\n                for filename in files:\n                    fullpath = os.path.join(path, filename)\n                    relpath = os.path.relpath(fullpath, bundle_dir)\n                    if filename == \"manifest.json\":\n                        manifest = _patch_artifact_manifest(\n                            fullpath, org, release, project, extra_files\n                        )\n                        zipf.writestr(relpath, manifest)\n                    else:\n                        zipf.write(fullpath, relpath)\n\n        return bundle.getvalue()\n\n    @classmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release_archive(cls, org, release: str, project=None, dist=None):\n        bundle = cls.create_artifact_bundle_zip(org, release, project)\n        file = File.objects.create(name=\"release-artifacts.zip\")\n        file.putfile(ContentFile(bundle))\n        release_obj = Release.objects.get(organization__slug=org, version=release)\n        return update_artifact_index(release_obj, dist, file)\n\n    @classmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_artifact_bundle(\n        cls,\n        org,\n        bundle_id=None,\n        artifact_count=0,\n        fixture_path=\"artifact_bundle_debug_ids\",\n        date_uploaded=None,\n        date_last_modified=None,\n    ):\n        if date_uploaded is None:\n            date_uploaded = timezone.now()\n\n        bundle = cls.create_artifact_bundle_zip(org.slug, fixture_path=fixture_path)\n        file_ = File.objects.create(name=\"artifact-bundle.zip\")\n        file_.putfile(ContentFile(bundle))\n        # The 'artifact_count' should correspond to the 'bundle' contents but for the purpose of tests we can also\n        # mock it with an arbitrary value.\n        artifact_bundle = ArtifactBundle.objects.create(\n            organization_id=org.id,\n            bundle_id=bundle_id or uuid4(),\n            file=file_,\n            artifact_count=artifact_count,\n            date_uploaded=date_uploaded,\n            date_last_modified=date_last_modified,\n        )\n        return artifact_bundle\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_code_mapping(project, repo=None, organization_integration=None, **kwargs):\n        kwargs.setdefault(\"stack_root\", \"\")\n        kwargs.setdefault(\"source_root\", \"\")\n        kwargs.setdefault(\"default_branch\", \"master\")\n\n        if not repo:\n            repo = Factories.create_repo(project=project)\n        return RepositoryProjectPathConfig.objects.create(\n            project=project,\n            repository=repo,\n            organization_integration_id=organization_integration.id,\n            integration_id=organization_integration.integration_id,\n            organization_id=organization_integration.organization_id,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_repo(\n        project, name=None, provider=None, integration_id=None, url=None, external_id=None\n    ):\n        repo, _ = Repository.objects.get_or_create(\n            organization_id=project.organization_id,\n            name=name\n            or \"{}-{}\".format(petname.generate(2, \"\", letters=10), random.randint(1000, 9999)),\n            provider=provider,\n            integration_id=integration_id,\n            url=url,\n            external_id=external_id,\n        )\n        return repo\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit(\n        repo, project=None, author=None, release=None, message=None, key=None, date_added=None\n    ):\n        commit = Commit.objects.get_or_create(\n            organization_id=repo.organization_id,\n            repository_id=repo.id,\n            key=key or sha1(uuid4().hex.encode(\"utf-8\")).hexdigest(),\n            defaults={\n                \"message\": message or make_sentence(),\n                \"author\": author\n                or Factories.create_commit_author(organization_id=repo.organization_id),\n                \"date_added\": date_added or timezone.now(),\n            },\n        )[0]\n\n        if release:\n            assert project\n            ReleaseCommit.objects.create(\n                organization_id=repo.organization_id,\n                project_id=project.id,\n                release=release,\n                commit=commit,\n                order=1,\n            )\n\n        Factories.create_commit_file_change(commit=commit, filename=\"/models/foo.py\")\n        Factories.create_commit_file_change(commit=commit, filename=\"/worsematch/foo.py\")\n        Factories.create_commit_file_change(commit=commit, filename=\"/models/other.py\")\n\n        return commit\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit_author(organization_id=None, project=None, user=None, email=None):\n        if email:\n            user_email = email\n        else:\n            user_email = user.email if user else f\"{make_word()}@example.com\"\n        return CommitAuthor.objects.get_or_create(\n            organization_id=organization_id or project.organization_id,\n            email=user_email,\n            defaults={\"name\": user.name if user else make_word()},\n        )[0]\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit_file_change(commit, filename):\n        return CommitFileChange.objects.get_or_create(\n            organization_id=commit.organization_id, commit=commit, filename=filename, type=\"M\"\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user(\n        email=None, is_superuser=False, is_staff=False, is_active=True, **kwargs\n    ) -> User:\n        if email is None:\n            email = uuid4().hex + \"@example.com\"\n\n        kwargs.setdefault(\"username\", email)\n\n        user = User(\n            email=email, is_superuser=is_superuser, is_staff=is_staff, is_active=is_active, **kwargs\n        )\n        if kwargs.get(\"password\") is None:\n            user.set_password(\"admin\")\n        user.save()\n\n        # UserEmail is created by a signal\n        assert UserEmail.objects.filter(user=user, email=email).update(is_verified=True)\n\n        return user\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_useremail(user, email=None, **kwargs):\n        if not email:\n            email = uuid4().hex + \"@example.com\"\n\n        kwargs.setdefault(\"is_verified\", True)\n\n        useremail = UserEmail(user=user, email=email, **kwargs)\n        useremail.save()\n\n        return useremail\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_avatar(*args, **kwargs):\n        return UserAvatar.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_role(*args, **kwargs):\n        return UserRole.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_usersocialauth(\n        user: User,\n        provider: str | None = None,\n        uid: str | None = None,\n        extra_data: dict[str, Any] | None = None,\n    ):\n        if not provider:\n            provider = \"asana\"\n        if not uid:\n            uid = \"abc-123\"\n        usa = UserSocialAuth(user=user, provider=provider, uid=uid, extra_data=extra_data)\n        usa.save()\n        return usa\n\n    @staticmethod\n    def inject_performance_problems(jobs, _):\n        for job in jobs:\n            job[\"performance_problems\"] = []\n            for f in job[\"data\"][\"fingerprint\"]:\n                f_data = f.split(\"-\", 1)\n                if len(f_data) < 2:\n                    raise ValueError(\n                        \"Invalid performance fingerprint data. Format must be 'group_type-fingerprint'.\"\n                    )\n                group_type = get_group_type_by_type_id(int(f_data[0]))\n                perf_fingerprint = f_data[1]\n\n                job[\"performance_problems\"].append(\n                    PerformanceProblem(\n                        fingerprint=perf_fingerprint,\n                        op=\"db\",\n                        desc=\"\",\n                        type=group_type,\n                        parent_span_ids=None,\n                        cause_span_ids=None,\n                        offender_span_ids=[],\n                        evidence_data={},\n                        evidence_display=[],\n                    )\n                )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def store_event(\n        data,\n        project_id: int,\n        assert_no_errors: bool = True,\n        default_event_type: EventType | None = None,\n        sent_at: datetime | None = None,\n    ) -> Event:\n        \"\"\"\n        Like `create_event`, but closer to how events are actually\n        ingested. Prefer to use this method over `create_event`\n        \"\"\"\n\n        # this creates a basic message event\n        if default_event_type == EventType.DEFAULT:\n            data.update({\"stacktrace\": copy.deepcopy(DEFAULT_EVENT_DATA[\"stacktrace\"])})\n\n        # this creates an error event\n        elif default_event_type == EventType.ERROR:\n            data.update({\"exception\": [{\"value\": \"BadError\"}]})\n\n        manager = EventManager(data, sent_at=sent_at)\n        manager.normalize()\n        if assert_no_errors:\n            errors = manager.get_data().get(\"errors\")\n            assert not errors, errors\n\n        normalized_data = manager.get_data()\n\n        _set_sample_rate_from_error_sampling(normalized_data)\n\n        event = None\n\n        # When fingerprint is present on transaction, inject performance problems\n        if (\n            normalized_data.get(\"type\") == \"transaction\"\n            and normalized_data.get(\"fingerprint\") is not None\n        ):\n            with mock.patch(\n                \"sentry.event_manager._detect_performance_problems\",\n                Factories.inject_performance_problems,\n            ):\n                event = manager.save(project_id)\n\n        else:\n            event = manager.save(project_id)\n\n        if event.groups:\n            for group in event.groups:\n                group.save()\n\n        if event.group:\n            event.group.save()\n\n        return event\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_group(project, create_open_period=True, **kwargs):\n        from sentry.models.group import GroupStatus\n        from sentry.models.groupopenperiod import GroupOpenPeriod\n        from sentry.testutils.helpers.datetime import before_now\n        from sentry.types.group import GroupSubStatus\n\n        kwargs.setdefault(\"message\", \"Hello world\")\n        kwargs.setdefault(\"data\", {})\n        if \"type\" not in kwargs[\"data\"]:\n            kwargs[\"data\"].update({\"type\": \"default\", \"metadata\": {\"title\": kwargs[\"message\"]}})\n        if \"short_id\" not in kwargs:\n            kwargs[\"short_id\"] = project.next_short_id()\n        if \"metadata\" in kwargs:\n            metadata = kwargs.pop(\"metadata\")\n            kwargs[\"data\"].setdefault(\"metadata\", {}).update(metadata)\n        if \"status\" not in kwargs:\n            kwargs[\"status\"] = GroupStatus.UNRESOLVED\n            kwargs[\"substatus\"] = GroupSubStatus.NEW\n\n        group = Group.objects.create(project=project, **kwargs)\n        if create_open_period:\n            open_period = GroupOpenPeriod.objects.create(\n                group=group,\n                project=project,\n                date_started=group.first_seen or before_now(minutes=5),\n            )\n            if group.status == GroupStatus.RESOLVED:\n                open_period.update(\n                    date_ended=group.resolved_at if group.resolved_at else timezone.now()\n                )\n\n        return group\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_file(**kwargs):\n        return File.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_file_from_path(path, name=None, **kwargs):\n        if name is None:\n            name = os.path.basename(path)\n\n        file = Factories.create_file(name=name, **kwargs)\n        with open(path) as f:\n            file.putfile(f)\n        return file\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dif_file(\n        project,\n        debug_id=None,\n        object_name=None,\n        features=None,\n        data=None,\n        file=None,\n        cpu_name=None,\n        code_id=None,\n        **kwargs,\n    ):\n        if debug_id is None:\n            debug_id = str(uuid4())\n\n        if object_name is None:\n            object_name = \"%s.dSYM\" % debug_id\n\n        if features is not None:\n            if data is None:\n                data = {}\n            data[\"features\"] = features\n\n        if file is None:\n            file = Factories.create_file(\n                name=object_name,\n                size=42,\n                headers={\"Content-Type\": \"application/x-mach-binary\"},\n                checksum=\"dc1e3f3e411979d336c3057cce64294f3420f93a\",\n            )\n\n        return ProjectDebugFile.objects.create(\n            debug_id=debug_id,\n            code_id=code_id,\n            project_id=project.id,\n            object_name=object_name,\n            cpu_name=cpu_name or \"x86_64\",\n            file=file,\n            checksum=file.checksum,\n            data=data,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dif_from_path(path, object_name=None, **kwargs):\n        if object_name is None:\n            object_name = os.path.basename(path)\n\n        headers = {\"Content-Type\": \"application/x-mach-binary\"}\n        file = Factories.create_file_from_path(path, name=object_name, headers=headers)\n        return Factories.create_dif_file(file=file, object_name=object_name, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def add_user_permission(user, permission):\n        UserPermission.objects.create(user=user, permission=permission)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app(**kwargs):\n        published = kwargs.pop(\"published\", False)\n        args = Factories._sentry_app_kwargs(**kwargs)\n        user = args.pop(\"user\", None)\n        app = SentryAppCreator(is_internal=False, **args).run(user=user, request=None)\n\n        if published:\n            app.update(status=SentryAppStatus.PUBLISHED)\n\n        return app\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_avatar(*args, **kwargs):\n        return SentryAppAvatar.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_internal_integration(**kwargs) -> SentryApp:\n        args = Factories._sentry_app_kwargs(**kwargs)\n        args[\"verify_install\"] = False\n        user = args.pop(\"user\", None)\n        app = SentryAppCreator(is_internal=True, **args).run(\n            user=user, request=None, skip_default_auth_token=True\n        )\n        return app\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_internal_integration_token(\n        user,\n        internal_integration: SentryApp | None = None,\n        install: SentryAppInstallation | None = None,\n        request=None,\n    ) -> ApiToken:\n        if internal_integration and install:\n            raise ValueError(\"Only one of internal_integration or install arg can be provided\")\n        elif internal_integration is None and install is None:\n            raise ValueError(\"Must pass in either internal_integration or install arg\")\n\n        if internal_integration is not None and install is None:\n            # Fetch install from provided or created internal integration\n            with assume_test_silo_mode(SiloMode.CONTROL):\n                install = SentryAppInstallation.objects.get(\n                    sentry_app=internal_integration.id,\n                    organization_id=internal_integration.owner_id,\n                )\n        elif install is None:\n            raise AssertionError(\"unreachable\")\n\n        return SentryAppInstallationTokenCreator(sentry_app_installation=install).run(\n            user=user, request=request\n        )\n\n    @staticmethod\n    def _sentry_app_kwargs(**kwargs):\n        _kwargs = {\n            \"user\": kwargs.get(\"user\", Factories.create_user()),\n            \"name\": kwargs.get(\"name\", petname.generate(2, \" \", letters=10).title()),\n            \"organization_id\": kwargs.get(\n                \"organization_id\", kwargs.pop(\"organization\", Factories.create_organization()).id\n            ),\n            \"author\": kwargs.get(\"author\", \"A Company\"),\n            \"scopes\": kwargs.get(\"scopes\", ()),\n            \"verify_install\": kwargs.get(\"verify_install\", True),\n            \"webhook_url\": kwargs.get(\"webhook_url\", \"https://example.com/webhook\"),\n            \"events\": [],\n            \"schema\": {},\n        }\n\n        _kwargs.update(**kwargs)\n        return _kwargs\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_sentry_app_installation(\n        organization=None,\n        slug=None,\n        user=None,\n        status=None,\n        prevent_token_exchange=False,\n    ):\n        if not organization:\n            organization = Factories.create_organization()\n\n        Factories.create_project(organization=organization)\n\n        with assume_test_silo_mode(SiloMode.CONTROL):\n            install = SentryAppInstallationCreator(\n                slug=(slug or Factories.create_sentry_app(organization=organization).slug),\n                organization_id=organization.id,\n            ).run(\n                user=(user or Factories.create_user()),\n                request=None,\n            )\n\n            install.status = SentryAppInstallationStatus.INSTALLED if status is None else status\n            install.save()\n            if not prevent_token_exchange and (\n                install.sentry_app.status != SentryAppStatus.INTERNAL\n            ):\n                assert install.api_grant is not None\n                assert install.sentry_app.application is not None\n                assert install.sentry_app.proxy_user is not None\n                GrantExchanger(\n                    install=install,\n                    code=install.api_grant.code,\n                    client_id=install.sentry_app.application.client_id,\n                    user=install.sentry_app.proxy_user,\n                ).run()\n                install = SentryAppInstallation.objects.get(id=install.id)\n        return install\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_installation_for_provider(\n        sentry_app_id: int,\n        organization_id: int,\n        provider: str,\n    ) -> SentryAppInstallationForProvider:\n        installation = SentryAppInstallation.objects.get(\n            sentry_app_id=sentry_app_id, organization_id=organization_id\n        )\n        return SentryAppInstallationForProvider.objects.create(\n            organization_id=organization_id,\n            provider=provider,\n            sentry_app_installation=installation,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_stacktrace_link_schema():\n        return {\"type\": \"stacktrace-link\", \"uri\": \"/redirect/\"}\n\n    @staticmethod\n    def create_issue_link_schema():\n        return {\n            \"type\": \"issue-link\",\n            \"link\": {\n                \"uri\": \"/sentry/issues/link\",\n                \"required_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    }\n                ],\n            },\n            \"create\": {\n                \"uri\": \"/sentry/issues/create\",\n                \"required_fields\": [\n                    {\"type\": \"text\", \"name\": \"title\", \"label\": \"Title\"},\n                    {\"type\": \"text\", \"name\": \"summary\", \"label\": \"Summary\"},\n                ],\n                \"optional_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"points\",\n                        \"label\": \"Points\",\n                        \"options\": [[\"1\", \"1\"], [\"2\", \"2\"], [\"3\", \"3\"], [\"5\", \"5\"], [\"8\", \"8\"]],\n                    },\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    },\n                ],\n            },\n        }\n\n    @staticmethod\n    def create_alert_rule_action_schema():\n        return {\n            \"type\": \"alert-rule-action\",\n            \"title\": \"Create Task with App\",\n            \"settings\": {\n                \"type\": \"alert-rule-settings\",\n                \"uri\": \"/sentry/alert-rule\",\n                \"required_fields\": [\n                    {\"type\": \"text\", \"name\": \"title\", \"label\": \"Title\"},\n                    {\"type\": \"text\", \"name\": \"summary\", \"label\": \"Summary\"},\n                ],\n                \"optional_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"points\",\n                        \"label\": \"Points\",\n                        \"options\": [[\"1\", \"1\"], [\"2\", \"2\"], [\"3\", \"3\"], [\"5\", \"5\"], [\"8\", \"8\"]],\n                    },\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    },\n                ],\n            },\n        }\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_service_hook(\n        actor=None, org=None, project=None, events=None, url=None, project_ids=None, **kwargs\n    ):\n        if project:\n            if project_ids is not None:\n                raise ValueError(\"Cannot provide both project and project_ids\")\n            project_ids = [project.id]\n        if not actor:\n            actor = Factories.create_user()\n        if not org:\n            if project:\n                org = project.organization\n            else:\n                org = Factories.create_organization(owner=actor)\n        if project_ids is None:  # empty list for project_ids is valid and means no project filter\n            project_ids = [Factories.create_project(organization=org).id]\n        if events is None:\n            events = [\"event.created\"]\n        if not url:\n            url = \"https://example.com/sentry/webhook\"\n\n        app_id = kwargs.pop(\"application_id\", None)\n        if app_id is None and \"application\" in kwargs:\n            app_id = kwargs[\"application\"].id\n        installation_id = kwargs.pop(\"installation_id\", None)\n        if installation_id is None and \"installation\" in kwargs:\n            installation_id = kwargs[\"installation\"].id\n        hook_id = hook_service.create_service_hook(\n            application_id=app_id,\n            actor_id=actor.id,\n            installation_id=installation_id,\n            organization_id=org.id,\n            project_ids=project_ids,\n            events=events,\n            url=url,\n        ).id\n        return ServiceHook.objects.get(id=hook_id)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_feature(feature=None, sentry_app=None, description=None):\n        if not sentry_app:\n            sentry_app = Factories.create_sentry_app()\n\n        integration_feature = IntegrationFeature.objects.create(\n            target_id=sentry_app.id,\n            target_type=IntegrationTypes.SENTRY_APP.value,\n            feature=feature or Feature.API,\n        )\n\n        if description:\n            integration_feature.update(user_description=description)\n\n        return integration_feature\n\n    @staticmethod\n    def _doc_integration_kwargs(**kwargs):\n        _kwargs = {\n            \"name\": kwargs.get(\"name\", petname.generate(2, \" \", letters=10).title()),\n            \"author\": kwargs.get(\"author\", \"me\"),\n            \"description\": kwargs.get(\"description\", \"hi im a description\"),\n            \"url\": kwargs.get(\"url\", \"https://sentry.io\"),\n            \"popularity\": kwargs.get(\"popularity\", 1),\n            \"is_draft\": kwargs.get(\"is_draft\", True),\n            \"metadata\": kwargs.get(\"metadata\", {}),\n        }\n        _kwargs[\"slug\"] = slugify(_kwargs[\"name\"])\n        _kwargs.update(**kwargs)\n        return _kwargs\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration(features=None, has_avatar: bool = False, **kwargs) -> DocIntegration:\n        doc = DocIntegration.objects.create(**Factories._doc_integration_kwargs(**kwargs))\n        if features:\n            Factories.create_doc_integration_features(features=features, doc_integration=doc)\n        if has_avatar:\n            Factories.create_doc_integration_avatar(doc_integration=doc)\n        return doc\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration_features(\n        features=None, doc_integration=None\n    ) -> list[IntegrationFeature]:\n        if not features:\n            features = [Feature.API]\n        if not doc_integration:\n            doc_integration = Factories.create_doc_integration()\n        return IntegrationFeature.objects.bulk_create(\n            [\n                IntegrationFeature(\n                    target_id=doc_integration.id,\n                    target_type=IntegrationTypes.DOC_INTEGRATION.value,\n                    feature=feature,\n                )\n                for feature in features\n            ]\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration_avatar(doc_integration=None, **kwargs) -> DocIntegrationAvatar:\n        if not doc_integration:\n            doc_integration = Factories.create_doc_integration()\n        photo = ControlFile.objects.create(name=\"test.png\", type=\"avatar.file\")\n        photo.putfile(io.BytesIO(b\"imaginethiswasphotobytes\"))\n\n        return DocIntegrationAvatar.objects.create(\n            doc_integration=doc_integration, avatar_type=0, control_file_id=photo.id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_userreport(\n        project: Project, event_id: str | None = None, **kwargs: Any\n    ) -> UserReport:\n        event = Factories.store_event(\n            data={\n                \"timestamp\": datetime.now(UTC).isoformat(),\n                \"event_id\": event_id or \"a\" * 32,\n                \"message\": \"testing\",\n            },\n            project_id=project.id,\n        )\n        assert event.group is not None\n\n        return UserReport.objects.create(\n            group_id=event.group.id,\n            event_id=event.event_id,\n            project_id=project.id,\n            name=\"Jane Bloggs\",\n            email=\"jane@example.com\",\n            comments=\"the application crashed\",\n            **kwargs,\n        )\n\n    @staticmethod\n    def create_session():\n        engine = import_module(settings.SESSION_ENGINE)\n\n        session = engine.SessionStore()\n        session.save()\n        return session\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_platform_external_issue(\n        group=None, service_type=None, display_name=None, web_url=None\n    ):\n        return PlatformExternalIssue.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            service_type=service_type,\n            display_name=display_name,\n            web_url=web_url,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_integration_external_issue(group=None, integration=None, key=None, **kwargs):\n        external_issue = ExternalIssue.objects.create(\n            organization_id=group.organization.id, integration_id=integration.id, key=key, **kwargs\n        )\n\n        GroupLink.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            linked_type=GroupLink.LinkedType.issue,\n            linked_id=external_issue.id,\n            relationship=GroupLink.Relationship.references,\n        )\n\n        return external_issue\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_integration_external_project(\n        organization_id: int, integration_id: int, *args: Any, **kwargs: Any\n    ) -> IntegrationExternalProject:\n        oi = OrganizationIntegration.objects.get(\n            organization_id=organization_id, integration_id=integration_id\n        )\n        return IntegrationExternalProject.objects.create(\n            organization_integration_id=oi.id, *args, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident(\n        organization,\n        projects,\n        detection_uuid=None,\n        status=1,\n        title=None,\n        query=\"test query\",\n        date_started=None,\n        date_detected=None,\n        date_closed=None,\n        alert_rule=None,\n        subscription=None,\n    ):\n        if not title:\n            title = petname.generate(2, \" \", letters=10).title()\n        if alert_rule is None:\n            alert_rule = Factories.create_alert_rule(\n                organization, projects, query=query, time_window=1\n            )\n\n        incident = Incident.objects.create(\n            organization=organization,\n            detection_uuid=detection_uuid,\n            status=status,\n            title=title,\n            alert_rule=alert_rule,\n            date_started=date_started or timezone.now(),\n            date_detected=date_detected or timezone.now(),\n            date_closed=timezone.now() if date_closed is not None else date_closed,\n            type=IncidentType.ALERT_TRIGGERED.value,\n            subscription=subscription,\n        )\n        for project in projects:\n            IncidentProject.objects.create(incident=incident, project=project)\n\n        return incident\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_activity(incident, type, comment=None, user_id=None):\n        return IncidentActivity.objects.create(\n            incident=incident, type=type, comment=comment, user_id=user_id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule(\n        organization,\n        projects,\n        name=None,\n        owner=None,\n        query=\"level:error\",\n        aggregate=\"count()\",\n        time_window=10,\n        threshold_period=1,\n        environment=None,\n        date_added=None,\n        query_type=None,\n        dataset=Dataset.Events,\n        threshold_type=AlertRuleThresholdType.ABOVE,\n        resolve_threshold=None,\n        user=None,\n        event_types=None,\n        comparison_delta=None,\n        description=None,\n        sensitivity=None,\n        seasonality=None,\n        detection_type=AlertRuleDetectionType.STATIC,\n    ):\n        if not name:\n            name = petname.generate(2, \" \", letters=10).title()\n\n        if query_type is None:\n            query_type = query_datasets_to_type[dataset]\n\n        alert_rule = create_alert_rule(\n            organization,\n            projects,\n            name,\n            query,\n            aggregate,\n            time_window,\n            threshold_type,\n            threshold_period,\n            owner=owner,\n            resolve_threshold=resolve_threshold,\n            query_type=query_type,\n            dataset=dataset,\n            environment=environment,\n            user=user,\n            event_types=event_types,\n            comparison_delta=comparison_delta,\n            description=description,\n            sensitivity=sensitivity,\n            seasonality=seasonality,\n            detection_type=detection_type,\n        )\n\n        if date_added is not None:\n            alert_rule.update(date_added=date_added)\n\n        return alert_rule\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_trigger(alert_rule, label=None, alert_threshold=100):\n        if not label:\n            label = petname.generate(2, \" \", letters=10).title()\n\n        return create_alert_rule_trigger(alert_rule, label, alert_threshold)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_trigger(incident, alert_rule_trigger, status=None):\n        if status is None:\n            status = TriggerStatus.ACTIVE.value\n\n        return IncidentTrigger.objects.create(\n            alert_rule_trigger=alert_rule_trigger, incident=incident, status=status\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_trigger_action(\n        trigger,\n        type=AlertRuleTriggerAction.Type.EMAIL,\n        target_type=AlertRuleTriggerAction.TargetType.USER,\n        target_identifier=None,\n        integration=None,\n        sentry_app=None,\n        sentry_app_config=None,\n    ):\n        return create_alert_rule_trigger_action(\n            trigger,\n            type,\n            target_type,\n            target_identifier,\n            integration.id if integration else None,\n            sentry_app.id if sentry_app else None,\n            sentry_app_config=sentry_app_config,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_external_user(user: User, **kwargs: Any) -> ExternalActor:\n        kwargs.setdefault(\"provider\", ExternalProviders.GITHUB.value)\n        kwargs.setdefault(\"external_name\", \"\")\n\n        return ExternalActor.objects.create(user_id=user.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_external_team(team: Team, **kwargs: Any) -> ExternalActor:\n        kwargs.setdefault(\"provider\", ExternalProviders.GITHUB.value)\n        kwargs.setdefault(\"external_name\", \"@getsentry/ecosystem\")\n\n        return ExternalActor.objects.create(team_id=team.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_codeowners(project, code_mapping, **kwargs):\n        kwargs.setdefault(\"raw\", \"\")\n\n        return ProjectCodeOwners.objects.create(\n            project=project, repository_project_path_config=code_mapping, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_slack_integration(\n        organization: Organization, external_id: str, **kwargs: Any\n    ) -> Integration:\n        integration = Integration.objects.create(\n            provider=\"slack\",\n            name=\"Team A\",\n            external_id=external_id,\n            metadata={\n                \"access_token\": \"xoxp-xxxxxxxxx-xxxxxxxxxx-xxxxxxxxxxxx\",\n                \"installation_type\": \"born_as_bot\",\n            },\n        )\n        integration.add_organization(organization)\n        return integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_integration(\n        organization: Organization,\n        external_id: str,\n        oi_params: Mapping[str, Any] | None = None,\n        **integration_params: Any,\n    ) -> Integration:\n        integration = Integration.objects.create(external_id=external_id, **integration_params)\n        with outbox_runner():\n            organization_integration = integration.add_organization(organization)\n            assert organization_integration is not None\n        organization_integration.update(**(oi_params or {}))\n\n        return integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_provider_integration(**integration_params: Any) -> Integration:\n        return Integration.objects.create(**integration_params)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_provider_integration_for(\n        organization: Organization | RpcOrganization,\n        user: User | RpcUser | None,\n        **integration_params: Any,\n    ) -> tuple[Integration, OrganizationIntegration]:\n        integration = Integration.objects.create(**integration_params)\n        org_integration = integration.add_organization(organization, user)\n        assert org_integration is not None\n        return integration, org_integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity_integration(\n        user: User | RpcUser,\n        organization: Organization | RpcOrganization,\n        integration_params: Mapping[Any, Any],\n        identity_params: Mapping[Any, Any],\n    ) -> tuple[Integration, OrganizationIntegration, Identity, IdentityProvider]:\n        # Avoid common pitfalls in tests\n        assert \"provider\" in integration_params\n        assert \"external_id\" in integration_params\n        assert \"external_id\" in identity_params\n\n        integration = Factories.create_provider_integration(**integration_params)\n        identity_provider = Factories.create_identity_provider(integration=integration)\n        identity = Factories.create_identity(\n            user=user, identity_provider=identity_provider, **identity_params\n        )\n        organization_integration = integration.add_organization(\n            organization_id=organization.id, user=user, default_auth_id=identity.id\n        )\n        assert organization_integration is not None\n        return integration, organization_integration, identity, identity_provider\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_organization_integration(**integration_params: Any) -> OrganizationIntegration:\n        return OrganizationIntegration.objects.create(**integration_params)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity_provider(\n        integration: Integration | None = None,\n        config: dict[str, Any] | None = None,\n        **kwargs: Any,\n    ) -> IdentityProvider:\n        if integration is not None:\n            integration_values = dict(\n                type=integration.provider,\n                external_id=integration.external_id,\n            )\n            if any((key in kwargs) for key in integration_values):\n                raise ValueError(\n                    \"Values from integration should not be in kwargs: \"\n                    + repr(list(integration_values.keys()))\n                )\n            kwargs.update(integration_values)\n\n        return IdentityProvider.objects.create(config=config or {}, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity(\n        user: User | RpcUser, identity_provider: IdentityProvider, external_id: str, **kwargs: Any\n    ) -> Identity:\n        return Identity.objects.create(\n            external_id=external_id,\n            idp=identity_provider,\n            user_id=user.id,\n            status=IdentityStatus.VALID,\n            scopes=[],\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_group_history(\n        group: Group,\n        status: int,\n        release: Release | None = None,\n        user_id: int | None = None,\n        team_id: int | None = None,\n        prev_history: GroupHistory | None = None,\n        date_added: datetime | None = None,\n    ) -> GroupHistory:\n        prev_history_date = None\n        if prev_history:\n            prev_history_date = prev_history.date_added\n\n        kwargs = {}\n        if date_added:\n            kwargs[\"date_added\"] = date_added\n        return GroupHistory.objects.create(\n            organization=group.organization,\n            group=group,\n            project=group.project,\n            release=release,\n            user_id=user_id,\n            team_id=team_id,\n            status=status,\n            prev_history=prev_history,\n            prev_history_date=prev_history_date,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_comment(issue, project, user, text=\"hello world\"):\n        data = {\"text\": text}\n        return Activity.objects.create(\n            project=project,\n            group=issue,\n            type=ActivityType.NOTE.value,\n            user_id=user.id,\n            data=data,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_saved_search(name: str, **kwargs):\n        if \"owner\" in kwargs:\n            owner = kwargs.pop(\"owner\")\n            kwargs[\"owner_id\"] = owner.id if not isinstance(owner, int) else owner\n        return SavedSearch.objects.create(name=name, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_notification_action(\n        organization: Organization | None = None,\n        projects: list[Project] | None = None,\n        **kwargs,\n    ):\n        if not organization:\n            organization = Factories.create_organization()\n\n        if not projects:\n            projects = []\n\n        action_kwargs = {\n            \"organization\": organization,\n            \"type\": ActionService.SENTRY_NOTIFICATION,\n            \"target_type\": ActionTarget.USER,\n            \"target_identifier\": \"1\",\n            \"target_display\": \"Sentry User\",\n            \"trigger_type\": ActionTrigger.AUDIT_LOG,\n            **kwargs,\n        }\n\n        action = NotificationAction.objects.create(**action_kwargs)\n        action.projects.add(*projects)\n        action.save()\n\n        return action\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_notification_settings_provider(*args, **kwargs) -> NotificationSettingProvider:\n        return NotificationSettingProvider.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_option(*args, **kwargs) -> UserOption:\n        return UserOption.objects.create(*args, **kwargs)\n\n    @staticmethod\n    def create_basic_auth_header(username: str, password: str = \"\") -> bytes:\n        return b\"Basic \" + b64encode(f\"{username}:{password}\".encode())\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def snooze_rule(**kwargs):\n        return RuleSnooze.objects.create(**kwargs)\n\n    @staticmethod\n    def create_request_access(\n        sso_state: RpcMemberSsoState | None = None,\n        permissions: list | None = None,\n        org_context: RpcUserOrganizationContext | None = None,\n        scopes_upper_bound: frozenset | None = frozenset(),\n    ) -> RpcBackedAccess:\n        if not sso_state:\n            sso_state = RpcMemberSsoState()\n        if not permissions:\n            permissions = []\n        if not org_context:\n            org_context = RpcUserOrganizationContext()\n\n        auth_state = RpcAuthState(sso_state=sso_state, permissions=permissions)\n        return RpcBackedAccess(\n            rpc_user_organization_context=org_context,\n            auth_state=auth_state,\n            scopes_upper_bound=scopes_upper_bound,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_webhook_payload(mailbox_name: str, region_name: str, **kwargs) -> WebhookPayload:\n        payload_kwargs = {\n            \"request_method\": \"POST\",\n            \"request_path\": \"/extensions/github/webhook/\",\n            \"request_headers\": '{\"Content-Type\": \"application/json\"}',\n            \"request_body\": \"{}\",\n            **kwargs,\n        }\n        return WebhookPayload.objects.create(\n            mailbox_name=mailbox_name, region_name=region_name, **payload_kwargs\n        )\n\n    @staticmethod\n    def create_uptime_subscription(\n        type: str,\n        subscription_id: str | None,\n        status: UptimeSubscription.Status,\n        url: str | None,\n        url_domain: str,\n        url_domain_suffix: str,\n        host_provider_id: str,\n        host_provider_name: str,\n        interval_seconds: IntervalSecondsLiteral,\n        timeout_ms: int,\n        method,\n        headers,\n        body,\n        date_updated: datetime,\n        uptime_status: UptimeStatus,\n        uptime_status_update_date: datetime,\n        trace_sampling: bool = False,\n    ):\n        if url is None:\n            url = petname.generate().title()\n            url = f\"http://{url}.com\"\n\n        return UptimeSubscription.objects.create(\n            type=type,\n            subscription_id=subscription_id,\n            status=status.value,\n            url=url,\n            url_domain=url_domain,\n            url_domain_suffix=url_domain_suffix,\n            host_provider_id=host_provider_id,\n            host_provider_name=host_provider_name,\n            interval_seconds=interval_seconds,\n            timeout_ms=timeout_ms,\n            date_updated=date_updated,\n            method=method,\n            headers=headers,\n            body=body,\n            trace_sampling=trace_sampling,\n            uptime_status=uptime_status,\n            uptime_status_update_date=uptime_status_update_date,\n        )\n\n    @staticmethod\n    def create_project_uptime_subscription(\n        project: Project,\n        env: Environment | None,\n        uptime_subscription: UptimeSubscription,\n        status: int,\n        mode: UptimeMonitorMode,\n        name: str | None,\n        owner: Actor | None,\n        id: int | None,\n    ):\n        if name is None:\n            name = petname.generate().title()\n        owner_team_id = None\n        owner_user_id = None\n        if owner:\n            if owner.is_team:\n                owner_team_id = owner.id\n            elif owner.is_user:\n                owner_user_id = owner.id\n\n        return ProjectUptimeSubscription.objects.create(\n            uptime_subscription=uptime_subscription,\n            project=project,\n            environment=env,\n            status=status,\n            mode=mode,\n            name=name,\n            owner_team_id=owner_team_id,\n            owner_user_id=owner_user_id,\n            pk=id,\n        )\n\n    @staticmethod\n    def create_uptime_subscription_region(\n        subscription: UptimeSubscription,\n        region_slug: str,\n        mode: UptimeSubscriptionRegion.RegionMode,\n    ) -> UptimeSubscriptionRegion:\n        return UptimeSubscriptionRegion.objects.create(\n            uptime_subscription=subscription,\n            region_slug=region_slug,\n            mode=mode,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard(\n        organization: Organization | None = None,\n        title: str | None = None,\n        created_by: User | None = None,\n        **kwargs,\n    ):\n        if organization is None:\n            organization = Factories.create_organization()\n        if created_by is None:\n            created_by = Factories.create_user()\n            Factories.create_member(organization=organization, user=created_by, role=\"owner\")\n        if title is None:\n            title = petname.generate(2, \" \", letters=10).title()\n        return Dashboard.objects.create(\n            organization=organization, title=title, created_by_id=created_by.id, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard_widget(\n        order: int,\n        dashboard: Dashboard | None = None,\n        title: str | None = None,\n        display_type: int | None = None,\n        **kwargs,\n    ):\n        if dashboard is None:\n            dashboard = Factories.create_dashboard()\n        if display_type is None:\n            display_type = DashboardWidgetDisplayTypes.AREA_CHART\n        if title is None:\n            title = petname.generate(2, \" \", letters=10).title()\n\n        return DashboardWidget.objects.create(\n            dashboard=dashboard, title=title, display_type=display_type, order=order, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard_widget_query(\n        order: int,\n        widget: DashboardWidget | None = None,\n        name: str | None = None,\n        **kwargs,\n    ):\n        if widget is None:\n            widget = Factories.create_dashboard_widget(order=order)\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        return DashboardWidgetQuery.objects.create(widget=widget, name=name, order=order, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_workflow(\n        name: str | None = None,\n        organization: Organization | None = None,\n        config: dict[str, Any] | None = None,\n        **kwargs,\n    ) -> Workflow:\n        if organization is None:\n            organization = Factories.create_organization()\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        if config is None:\n            config = {}\n        return Workflow.objects.create(\n            organization=organization, name=name, config=config, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition_group(\n        organization: Organization | None = None,\n        **kwargs,\n    ) -> DataConditionGroup:\n        if organization is None:\n            organization = Factories.create_organization()\n        return DataConditionGroup.objects.create(organization=organization, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_workflow_data_condition_group(\n        workflow: Workflow | None = None,\n        condition_group: DataConditionGroup | None = None,\n        **kwargs,\n    ) -> WorkflowDataConditionGroup:\n        if workflow is None:\n            workflow = Factories.create_workflow()\n\n        if not condition_group:\n            condition_group = Factories.create_data_condition_group()\n\n        return WorkflowDataConditionGroup.objects.create(\n            workflow=workflow, condition_group=condition_group\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition(\n        condition_group: DataConditionGroup | None = None, **kwargs\n    ) -> DataCondition:\n        if condition_group is None:\n            condition_group = Factories.create_data_condition_group()\n        return DataCondition.objects.create(condition_group=condition_group, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_source(\n        organization: Organization | None = None,\n        source_id: str | None = None,\n        type: str | None = None,\n        **kwargs,\n    ) -> DataSource:\n        if organization is None:\n            organization = Factories.create_organization()\n        if source_id is None:\n            source_id = str(random.randint(1, 10000))\n        if type is None:\n            type = data_source_type_registry.get_key(QuerySubscriptionDataSourceHandler)\n        return DataSource.objects.create(organization=organization, source_id=source_id, type=type)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector(\n        name: str | None = None,\n        config: dict | None = None,\n        **kwargs,\n    ) -> Detector:\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        if config is None:\n            config = default_detector_config_data.get(kwargs[\"type\"], {})\n\n        return Detector.objects.create(\n            name=name,\n            config=config,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector_state(\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> DetectorState:\n        if detector is None:\n            detector = Factories.create_detector()\n\n        return DetectorState.objects.create(detector=detector, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_source_detector(\n        data_source: DataSource | None = None,\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> DataSourceDetector:\n        if data_source is None:\n            data_source = Factories.create_data_source()\n        if detector is None:\n            detector = Factories.create_detector()\n        return DataSourceDetector.objects.create(data_source=data_source, detector=detector)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_action(\n        config: dict[str, Any] | None = None,\n        type: Action.Type | None = None,\n        data: dict[str, Any] | None = None,\n        **kwargs,\n    ) -> Action:\n        if config is None and type is None and data is None:\n            # Default to a slack action with nice defaults so someone can just do\n            # self.create_action() and have a sane default\n            config = {\n                \"target_identifier\": \"1\",\n                \"target_display\": \"Sentry User\",\n                \"target_type\": ActionTarget.SPECIFIC,\n            }\n\n            data = {\"notes\": \"bufos are great\", \"tags\": \"bufo-bot\"}\n\n        if config is None:\n            config = {}\n\n        if data is None:\n            data = {}\n\n        if type is None:\n            type = Action.Type.SLACK\n\n        return Action.objects.create(type=type, config=config, data=data, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector_workflow(\n        detector: Detector | None = None,\n        workflow: Workflow | None = None,\n        **kwargs,\n    ) -> DetectorWorkflow:\n        if detector is None:\n            detector = Factories.create_detector()\n        if workflow is None:\n            workflow = Factories.create_workflow()\n        return DetectorWorkflow.objects.create(detector=detector, workflow=workflow, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_workflow(\n        alert_rule_id: int | None = None,\n        rule_id: int | None = None,\n        workflow: Workflow | None = None,\n        **kwargs,\n    ) -> AlertRuleWorkflow:\n        if rule_id is None and alert_rule_id is None:\n            raise ValueError(\"Either rule_id or alert_rule_id must be provided\")\n\n        if rule_id is not None and alert_rule_id is not None:\n            raise ValueError(\"Only one of rule_id or alert_rule_id can be provided\")\n\n        if workflow is None:\n            workflow = Factories.create_workflow()\n\n        return AlertRuleWorkflow.objects.create(\n            alert_rule_id=alert_rule_id, rule_id=rule_id, workflow=workflow, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_group_open_period(\n        incident: Incident,\n        group_open_period: GroupOpenPeriod,\n        **kwargs,\n    ) -> IncidentGroupOpenPeriod:\n        return IncidentGroupOpenPeriod.objects.create(\n            incident_id=incident.id,\n            incident_identifier=incident.identifier,\n            group_open_period=group_open_period,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_detector(\n        alert_rule_id: int | None = None,\n        rule_id: int | None = None,\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> AlertRuleDetector:\n        if rule_id is None and alert_rule_id is None:\n            raise ValueError(\"Either rule_id or alert_rule_id must be provided\")\n\n        if rule_id is not None and alert_rule_id is not None:\n            raise ValueError(\"Only one of rule_id or alert_rule_id can be provided\")\n\n        if detector is None:\n            detector = Factories.create_detector()\n\n        return AlertRuleDetector.objects.create(\n            alert_rule_id=alert_rule_id, rule_id=rule_id, detector=detector, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_action_alert_rule_trigger_action(\n        alert_rule_trigger_action_id: int,\n        action: Action | None = None,\n        **kwargs,\n    ) -> ActionAlertRuleTriggerAction:\n        if action is None:\n            action = Factories.create_action()\n\n        return ActionAlertRuleTriggerAction.objects.create(\n            action=action, alert_rule_trigger_action_id=alert_rule_trigger_action_id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition_group_action(\n        action: Action | None = None,\n        condition_group: DataConditionGroup | None = None,\n        **kwargs,\n    ) -> DataConditionGroupAction:\n        if action is None:\n            action = Factories.create_action()\n        if condition_group is None:\n            condition_group = Factories.create_data_condition_group()\n        return DataConditionGroupAction.objects.create(\n            action=action, condition_group=condition_group, **kwargs\n        )\n",
    "file_path": "/Users/wangyue/Code/CodeReviewData/ReviewDataset/sentry-greptile/src/sentry/testutils/factories.py",
    "line_count": 2375,
    "encoding": "utf-8",
    "error": null
}

Message 3 [Assistant]:
--------------------------------------------------------------------------------
Content:
现在让我搜索这个函数的使用情况，以了解它的业务意图和上下文。
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "_set_sample_rate_from_error_sampling",
      "is_regex": false,
      "case_sensitive": true,
      "context_lines": 10
    },
    "id": "call_00_jDLbA4ns3KwDNmb06H65Q1M0",
    "type": "tool_call"
  }
]

Message 4 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_jDLbA4ns3KwDNmb06H65Q1M0
Result:
{
    "matches": [
        {
            "file": "src/sentry/testutils/factories.py",
            "line_number": 344,
            "matched_line": "def _set_sample_rate_from_error_sampling(normalized_data: MutableMapping[str, Any]) -> None:",
            "context_start_line": 334,
            "context_end_line": 354,
            "context": [
                "334:         manifest[\"org\"] = org",
                "335:     if release:",
                "336:         manifest[\"release\"] = release",
                "337:     if project:",
                "338:         manifest[\"project\"] = project",
                "339:     for path in extra_files or {}:",
                "340:         manifest[\"files\"][path] = {\"url\": path}",
                "341:     return orjson.dumps(manifest).decode()",
                "342: ",
                "343: ",
                "344: def _set_sample_rate_from_error_sampling(normalized_data: MutableMapping[str, Any]) -> None:",
                "345:     \"\"\"Set 'sample_rate' on normalized_data if contexts.error_sampling.client_sample_rate is present and valid.\"\"\"",
                "346:     client_sample_rate = None",
                "347:     try:",
                "348:         client_sample_rate = (",
                "349:             normalized_data.get(\"contexts\", {}).get(\"error_sampling\", {}).get(\"client_sample_rate\")",
                "350:         )",
                "351:     except Exception:",
                "352:         pass",
                "353:     if client_sample_rate:",
                "354:         try:"
            ]
        },
        {
            "file": "src/sentry/testutils/factories.py",
            "line_number": 1049,
            "matched_line": "        _set_sample_rate_from_error_sampling(normalized_data)",
            "context_start_line": 1039,
            "context_end_line": 1059,
            "context": [
                "1039:             data.update({\"exception\": [{\"value\": \"BadError\"}]})",
                "1040: ",
                "1041:         manager = EventManager(data, sent_at=sent_at)",
                "1042:         manager.normalize()",
                "1043:         if assert_no_errors:",
                "1044:             errors = manager.get_data().get(\"errors\")",
                "1045:             assert not errors, errors",
                "1046: ",
                "1047:         normalized_data = manager.get_data()",
                "1048: ",
                "1049:         _set_sample_rate_from_error_sampling(normalized_data)",
                "1050: ",
                "1051:         event = None",
                "1052: ",
                "1053:         # When fingerprint is present on transaction, inject performance problems",
                "1054:         if (",
                "1055:             normalized_data.get(\"type\") == \"transaction\"",
                "1056:             and normalized_data.get(\"fingerprint\") is not None",
                "1057:         ):",
                "1058:             with mock.patch(",
                "1059:                 \"sentry.event_manager._detect_performance_problems\","
            ]
        }
    ],
    "total": 2
}

Message 5 [Assistant]:
--------------------------------------------------------------------------------
Content:
现在让我查看这个函数被调用的上下文，特别是查看`store_event`函数，以了解这个函数的业务意图。
Tool Calls:
[
  {
    "name": "read_file",
    "args": {
      "file_path": "src/sentry/testutils/factories.py",
      "encoding": "utf-8"
    },
    "id": "call_00_Yvc41U6rmOs9XiRwp1dYwuZi",
    "type": "tool_call"
  }
]

Message 6 [Tool]:
--------------------------------------------------------------------------------
Tool: read_file
Tool Call ID: call_00_Yvc41U6rmOs9XiRwp1dYwuZi
Result:
{
    "content": "from __future__ import annotations\n\nimport contextlib\nimport copy\nimport io\nimport os\nimport random\nimport zipfile\nfrom base64 import b64encode\nfrom binascii import hexlify\nfrom collections.abc import Mapping, MutableMapping, Sequence\nfrom datetime import UTC, datetime\nfrom enum import Enum\nfrom hashlib import sha1\nfrom importlib import import_module\nfrom typing import Any\nfrom unittest import mock\nfrom uuid import uuid4\n\nimport orjson\nimport petname\nfrom django.conf import settings\nfrom django.contrib.auth.models import AnonymousUser\nfrom django.core.files.base import ContentFile\nfrom django.db import router, transaction\nfrom django.test.utils import override_settings\nfrom django.utils import timezone\nfrom django.utils.text import slugify\n\nfrom sentry.auth.access import RpcBackedAccess\nfrom sentry.auth.services.auth.model import RpcAuthState, RpcMemberSsoState\nfrom sentry.constants import SentryAppInstallationStatus, SentryAppStatus\nfrom sentry.event_manager import EventManager\nfrom sentry.eventstore.models import Event\nfrom sentry.hybridcloud.models.outbox import RegionOutbox, outbox_context\nfrom sentry.hybridcloud.models.webhookpayload import WebhookPayload\nfrom sentry.hybridcloud.outbox.category import OutboxCategory, OutboxScope\nfrom sentry.incidents.grouptype import MetricIssue\nfrom sentry.incidents.logic import (\n    create_alert_rule,\n    create_alert_rule_trigger,\n    create_alert_rule_trigger_action,\n    query_datasets_to_type,\n)\nfrom sentry.incidents.models.alert_rule import (\n    AlertRuleDetectionType,\n    AlertRuleThresholdType,\n    AlertRuleTriggerAction,\n)\nfrom sentry.incidents.models.incident import (\n    Incident,\n    IncidentActivity,\n    IncidentProject,\n    IncidentTrigger,\n    IncidentType,\n    TriggerStatus,\n)\nfrom sentry.integrations.models.doc_integration import DocIntegration\nfrom sentry.integrations.models.doc_integration_avatar import DocIntegrationAvatar\nfrom sentry.integrations.models.external_actor import ExternalActor\nfrom sentry.integrations.models.external_issue import ExternalIssue\nfrom sentry.integrations.models.integration import Integration\nfrom sentry.integrations.models.integration_external_project import IntegrationExternalProject\nfrom sentry.integrations.models.integration_feature import (\n    Feature,\n    IntegrationFeature,\n    IntegrationTypes,\n)\nfrom sentry.integrations.models.organization_integration import OrganizationIntegration\nfrom sentry.integrations.models.repository_project_path_config import RepositoryProjectPathConfig\nfrom sentry.integrations.types import ExternalProviders\nfrom sentry.issues.grouptype import get_group_type_by_type_id\nfrom sentry.models.activity import Activity\nfrom sentry.models.apikey import ApiKey\nfrom sentry.models.apitoken import ApiToken\nfrom sentry.models.artifactbundle import ArtifactBundle\nfrom sentry.models.authidentity import AuthIdentity\nfrom sentry.models.authprovider import AuthProvider\nfrom sentry.models.commit import Commit\nfrom sentry.models.commitauthor import CommitAuthor\nfrom sentry.models.commitfilechange import CommitFileChange\nfrom sentry.models.dashboard import Dashboard\nfrom sentry.models.dashboard_widget import (\n    DashboardWidget,\n    DashboardWidgetDisplayTypes,\n    DashboardWidgetQuery,\n)\nfrom sentry.models.debugfile import ProjectDebugFile\nfrom sentry.models.environment import Environment\nfrom sentry.models.files.control_file import ControlFile\nfrom sentry.models.files.file import File\nfrom sentry.models.group import Group\nfrom sentry.models.grouphistory import GroupHistory\nfrom sentry.models.grouplink import GroupLink\nfrom sentry.models.groupopenperiod import GroupOpenPeriod\nfrom sentry.models.grouprelease import GroupRelease\nfrom sentry.models.organization import Organization\nfrom sentry.models.organizationmapping import OrganizationMapping\nfrom sentry.models.organizationmember import OrganizationMember\nfrom sentry.models.organizationmemberinvite import OrganizationMemberInvite\nfrom sentry.models.organizationmemberteam import OrganizationMemberTeam\nfrom sentry.models.organizationslugreservation import OrganizationSlugReservation\nfrom sentry.models.orgauthtoken import OrgAuthToken\nfrom sentry.models.project import Project\nfrom sentry.models.projectbookmark import ProjectBookmark\nfrom sentry.models.projectcodeowners import ProjectCodeOwners\nfrom sentry.models.projecttemplate import ProjectTemplate\nfrom sentry.models.release import Release, ReleaseStatus\nfrom sentry.models.releasecommit import ReleaseCommit\nfrom sentry.models.releaseenvironment import ReleaseEnvironment\nfrom sentry.models.releasefile import ReleaseFile, update_artifact_index\nfrom sentry.models.releaseprojectenvironment import ReleaseProjectEnvironment\nfrom sentry.models.repository import Repository\nfrom sentry.models.rule import Rule\nfrom sentry.models.rulesnooze import RuleSnooze\nfrom sentry.models.savedsearch import SavedSearch\nfrom sentry.models.team import Team\nfrom sentry.models.userreport import UserReport\nfrom sentry.notifications.models.notificationaction import (\n    ActionService,\n    ActionTarget,\n    ActionTrigger,\n    NotificationAction,\n)\nfrom sentry.notifications.models.notificationsettingprovider import NotificationSettingProvider\nfrom sentry.organizations.services.organization import RpcOrganization, RpcUserOrganizationContext\nfrom sentry.performance_issues.performance_problem import PerformanceProblem\nfrom sentry.sentry_apps.installations import (\n    SentryAppInstallationCreator,\n    SentryAppInstallationTokenCreator,\n)\nfrom sentry.sentry_apps.logic import SentryAppCreator\nfrom sentry.sentry_apps.models.platformexternalissue import PlatformExternalIssue\nfrom sentry.sentry_apps.models.sentry_app import SentryApp\nfrom sentry.sentry_apps.models.sentry_app_avatar import SentryAppAvatar\nfrom sentry.sentry_apps.models.sentry_app_installation import SentryAppInstallation\nfrom sentry.sentry_apps.models.sentry_app_installation_for_provider import (\n    SentryAppInstallationForProvider,\n)\nfrom sentry.sentry_apps.models.servicehook import ServiceHook\nfrom sentry.sentry_apps.services.hook import hook_service\nfrom sentry.sentry_apps.token_exchange.grant_exchanger import GrantExchanger\nfrom sentry.signals import project_created\nfrom sentry.silo.base import SiloMode\nfrom sentry.snuba.dataset import Dataset\nfrom sentry.snuba.models import QuerySubscriptionDataSourceHandler\nfrom sentry.tempest.models import MessageType as TempestMessageType\nfrom sentry.tempest.models import TempestCredentials\nfrom sentry.testutils.outbox import outbox_runner\nfrom sentry.testutils.silo import assume_test_silo_mode\nfrom sentry.types.activity import ActivityType\nfrom sentry.types.actor import Actor\nfrom sentry.types.region import Region, get_local_region, get_region_by_name\nfrom sentry.types.token import AuthTokenType\nfrom sentry.uptime.models import (\n    IntervalSecondsLiteral,\n    ProjectUptimeSubscription,\n    UptimeStatus,\n    UptimeSubscription,\n    UptimeSubscriptionRegion,\n)\nfrom sentry.uptime.types import UptimeMonitorMode\nfrom sentry.users.models.identity import Identity, IdentityProvider, IdentityStatus\nfrom sentry.users.models.user import User\nfrom sentry.users.models.user_avatar import UserAvatar\nfrom sentry.users.models.user_option import UserOption\nfrom sentry.users.models.useremail import UserEmail\nfrom sentry.users.models.userpermission import UserPermission\nfrom sentry.users.models.userrole import UserRole\nfrom sentry.users.services.user import RpcUser\nfrom sentry.utils import loremipsum\nfrom sentry.workflow_engine.models import (\n    Action,\n    ActionAlertRuleTriggerAction,\n    AlertRuleDetector,\n    AlertRuleWorkflow,\n    DataCondition,\n    DataConditionGroup,\n    DataConditionGroupAction,\n    DataSource,\n    DataSourceDetector,\n    Detector,\n    DetectorState,\n    DetectorWorkflow,\n    IncidentGroupOpenPeriod,\n    Workflow,\n    WorkflowDataConditionGroup,\n)\nfrom sentry.workflow_engine.registry import data_source_type_registry\nfrom social_auth.models import UserSocialAuth\n\n\nclass EventType(Enum):\n    ERROR = \"error\"\n    DEFAULT = \"default\"\n\n\ndef get_fixture_path(*parts: str) -> str:\n    path = os.path.realpath(__file__)\n    for _ in range(4):  # src/sentry/testutils/{__file__}\n        path = os.path.dirname(path)\n    return os.path.join(path, \"fixtures\", *parts)\n\n\ndef make_sentence(words=None):\n    if words is None:\n        words = int(random.weibullvariate(8, 3))\n    return \" \".join(random.choice(loremipsum.words) for _ in range(words))\n\n\ndef make_word(words=None):\n    if words is None:\n        words = int(random.weibullvariate(8, 3))\n    return random.choice(loremipsum.words)\n\n\nDEFAULT_EVENT_DATA = {\n    \"extra\": {\n        \"loadavg\": [0.97607421875, 0.88330078125, 0.833984375],\n        \"sys.argv\": [\n            \"/Users/dcramer/.virtualenvs/sentry/bin/raven\",\n            \"test\",\n            \"https://ebc35f33e151401f9deac549978bda11:f3403f81e12e4c24942d505f086b2cad@sentry.io/1\",\n        ],\n        \"user\": \"dcramer\",\n    },\n    \"modules\": {\"raven\": \"3.1.13\"},\n    \"request\": {\n        \"cookies\": {},\n        \"data\": {},\n        \"env\": {},\n        \"headers\": {},\n        \"method\": \"GET\",\n        \"query_string\": \"\",\n        \"url\": \"http://example.com\",\n    },\n    \"stacktrace\": {\n        \"frames\": [\n            {\n                \"abs_path\": \"www/src/sentry/models/foo.py\",\n                \"context_line\": \"                        string_max_length=self.string_max_length)\",\n                \"filename\": \"sentry/models/foo.py\",\n                \"function\": \"build_msg\",\n                \"in_app\": True,\n                \"lineno\": 29,\n                \"module\": \"raven.base\",\n                \"post_context\": [\n                    \"                },\",\n                    \"            })\",\n                    \"\",\n                    \"        if 'stacktrace' in data:\",\n                    \"            if self.include_paths:\",\n                ],\n                \"pre_context\": [\n                    \"\",\n                    \"            data.update({\",\n                    \"                'stacktrace': {\",\n                    \"                    'frames': get_stack_info(frames,\",\n                    \"                        list_max_length=self.list_max_length,\",\n                ],\n                \"vars\": {\n                    \"culprit\": \"raven.scripts.runner\",\n                    \"date\": \"datetime.datetime(2013, 2, 14, 20, 6, 33, 479471)\",\n                    \"event_id\": \"598fb19363e745ec8be665e6ba88b1b2\",\n                    \"event_type\": \"raven.events.Message\",\n                    \"frames\": \"<generator object iter_stack_frames at 0x103fef050>\",\n                    \"handler\": \"<raven.events.Message object at 0x103feb710>\",\n                    \"k\": \"logentry\",\n                    \"public_key\": None,\n                    \"result\": {\n                        \"logentry\": \"{'message': 'This is a test message generated using ``raven test``', 'params': []}\"\n                    },\n                    \"self\": \"<raven.base.Client object at 0x104397f10>\",\n                    \"stack\": True,\n                    \"tags\": None,\n                    \"time_spent\": None,\n                },\n            },\n            {\n                \"abs_path\": \"/Users/dcramer/.virtualenvs/sentry/lib/python2.7/site-packages/raven/base.py\",\n                \"context_line\": \"                        string_max_length=self.string_max_length)\",\n                \"filename\": \"raven/base.py\",\n                \"function\": \"build_msg\",\n                \"in_app\": False,\n                \"lineno\": 290,\n                \"module\": \"raven.base\",\n                \"post_context\": [\n                    \"                },\",\n                    \"            })\",\n                    \"\",\n                    \"        if 'stacktrace' in data:\",\n                    \"            if self.include_paths:\",\n                ],\n                \"pre_context\": [\n                    \"\",\n                    \"            data.update({\",\n                    \"                'stacktrace': {\",\n                    \"                    'frames': get_stack_info(frames,\",\n                    \"                        list_max_length=self.list_max_length,\",\n                ],\n                \"vars\": {\n                    \"culprit\": \"raven.scripts.runner\",\n                    \"date\": \"datetime.datetime(2013, 2, 14, 20, 6, 33, 479471)\",\n                    \"event_id\": \"598fb19363e745ec8be665e6ba88b1b2\",\n                    \"event_type\": \"raven.events.Message\",\n                    \"frames\": \"<generator object iter_stack_frames at 0x103fef050>\",\n                    \"handler\": \"<raven.events.Message object at 0x103feb710>\",\n                    \"k\": \"logentry\",\n                    \"public_key\": None,\n                    \"result\": {\n                        \"logentry\": \"{'message': 'This is a test message generated using ``raven test``', 'params': []}\"\n                    },\n                    \"self\": \"<raven.base.Client object at 0x104397f10>\",\n                    \"stack\": True,\n                    \"tags\": None,\n                    \"time_spent\": None,\n                },\n            },\n        ]\n    },\n    \"tags\": [],\n    \"platform\": \"python\",\n}\n\ndefault_detector_config_data = {\n    MetricIssue.slug: {\"threshold_period\": 1, \"detection_type\": \"static\"}\n}\n\n\ndef _patch_artifact_manifest(path, org=None, release=None, project=None, extra_files=None):\n    with open(path, \"rb\") as fp:\n        manifest = orjson.loads(fp.read())\n    if org:\n        manifest[\"org\"] = org\n    if release:\n        manifest[\"release\"] = release\n    if project:\n        manifest[\"project\"] = project\n    for path in extra_files or {}:\n        manifest[\"files\"][path] = {\"url\": path}\n    return orjson.dumps(manifest).decode()\n\n\ndef _set_sample_rate_from_error_sampling(normalized_data: MutableMapping[str, Any]) -> None:\n    \"\"\"Set 'sample_rate' on normalized_data if contexts.error_sampling.client_sample_rate is present and valid.\"\"\"\n    client_sample_rate = None\n    try:\n        client_sample_rate = (\n            normalized_data.get(\"contexts\", {}).get(\"error_sampling\", {}).get(\"client_sample_rate\")\n        )\n    except Exception:\n        pass\n    if client_sample_rate:\n        try:\n            normalized_data[\"sample_rate\"] = float(client_sample_rate)\n        except Exception:\n            pass\n\n\n# TODO(dcramer): consider moving to something more scalable like factoryboy\nclass Factories:\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_organization(name=None, owner=None, region: Region | str | None = None, **kwargs):\n        if not name:\n            name = petname.generate(2, \" \", letters=10).title()\n\n        with contextlib.ExitStack() as ctx:\n            if region is None or SiloMode.get_current_mode() == SiloMode.MONOLITH:\n                region_name = get_local_region().name\n            else:\n                if isinstance(region, Region):\n                    region_name = region.name\n                else:\n                    region_obj = get_region_by_name(region)  # Verify it exists\n                    region_name = region_obj.name\n\n                ctx.enter_context(\n                    override_settings(SILO_MODE=SiloMode.REGION, SENTRY_REGION=region_name)\n                )\n\n            with outbox_context(flush=False):\n                org = Organization.objects.create(name=name, **kwargs)\n\n            with assume_test_silo_mode(SiloMode.CONTROL):\n                # Organization mapping creation relies on having a matching org slug reservation\n                OrganizationSlugReservation(\n                    organization_id=org.id,\n                    region_name=region_name,\n                    user_id=owner.id if owner else -1,\n                    slug=org.slug,\n                ).save(unsafe_write=True)\n\n            # Manually replicate org data after adding an org slug reservation\n            org.handle_async_replication(org.id)\n\n            # Flush remaining organization update outboxes accumulated by org create\n            RegionOutbox(\n                shard_identifier=org.id,\n                shard_scope=OutboxScope.ORGANIZATION_SCOPE,\n                category=OutboxCategory.ORGANIZATION_UPDATE,\n            ).drain_shard()\n\n        if owner:\n            Factories.create_member(organization=org, user_id=owner.id, role=\"owner\")\n        return org\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_org_mapping(org=None, **kwds):\n        if org:\n            kwds.setdefault(\"organization_id\", org.id)\n            kwds.setdefault(\"slug\", org.slug)\n            kwds.setdefault(\"name\", org.name)\n            kwds.setdefault(\"idempotency_key\", uuid4().hex)\n            kwds.setdefault(\"region_name\", \"na\")\n        return OrganizationMapping.objects.create(**kwds)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_member(teams=None, team_roles=None, **kwargs):\n        kwargs.setdefault(\"role\", \"member\")\n        teamRole = kwargs.pop(\"teamRole\", None)\n\n        # user_id will have precedence over user\n        user = kwargs.pop(\"user\", None)\n        user_id = kwargs.pop(\"user_id\", None)\n        if not user_id and user:\n            user_id = user.id\n        kwargs[\"user_id\"] = user_id\n\n        # inviter_id will have precedence over inviter\n        inviter = kwargs.pop(\"inviter\", None)\n        inviter_id = kwargs.pop(\"inviter_id\", None)\n        if not inviter_id and inviter:\n            inviter_id = inviter.id\n        kwargs[\"inviter_id\"] = inviter_id\n\n        om = OrganizationMember.objects.create(**kwargs)\n\n        if team_roles:\n            for team, role in team_roles:\n                Factories.create_team_membership(team=team, member=om, role=role)\n        elif teams:\n            for team in teams:\n                Factories.create_team_membership(team=team, member=om, role=teamRole)\n        return om\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_member_invite(\n        organization: Organization | None = None,\n        email: str | None = None,\n        **kwargs,\n    ) -> OrganizationMemberInvite:\n        if organization is None:\n            organization = Factories.create_organization()\n        if email is None:\n            email = f\"{petname.generate().title()}@email.com\"\n        om = OrganizationMember.objects.create(organization=organization)\n        return OrganizationMemberInvite.objects.create(\n            organization=organization, organization_member_id=om.id, email=email, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_team_membership(team, member=None, user=None, role=None):\n        if member is None:\n            member, created = OrganizationMember.objects.get_or_create(\n                user_id=user.id if user else None,\n                organization=team.organization,\n                defaults={\"role\": \"member\"},\n            )\n\n        return OrganizationMemberTeam.objects.create(\n            team=team, organizationmember=member, is_active=True, role=role\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_api_key(organization, **kwargs) -> ApiKey:\n        return ApiKey.objects.create(organization_id=organization.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_auth_provider(**kwargs):\n        return AuthProvider.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_auth_identity(**kwargs):\n        return AuthIdentity.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_auth_token(user, scope_list: list[str] | None = None, **kwargs) -> ApiToken:\n        if scope_list is None:\n            scope_list = []\n        return ApiToken.objects.create(\n            user=user,\n            scope_list=scope_list,\n            token_type=AuthTokenType.USER,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_org_auth_token(*args, **kwargs) -> OrgAuthToken:\n        return OrgAuthToken.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_team(organization, **kwargs):\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n        if not kwargs.get(\"slug\"):\n            kwargs[\"slug\"] = slugify(str(kwargs[\"name\"]))\n        members = kwargs.pop(\"members\", None)\n\n        team = Team.objects.create(organization=organization, **kwargs)\n        if members:\n            for user in members:\n                Factories.create_team_membership(team=team, user=user)\n        return team\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_environment(project, **kwargs):\n        name = kwargs.get(\"name\", petname.generate(3, \" \", letters=10)[:64])\n\n        organization = kwargs.get(\"organization\")\n        organization_id = organization.id if organization else project.organization_id\n\n        env = Environment.objects.create(organization_id=organization_id, name=name)\n        env.add_project(project, is_hidden=kwargs.get(\"is_hidden\"))\n        return env\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project(\n        organization=None, teams=None, fire_project_created=False, **kwargs\n    ) -> Project:\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n        if not kwargs.get(\"slug\"):\n            kwargs[\"slug\"] = slugify(str(kwargs[\"name\"]))\n        if not organization and teams:\n            organization = teams[0].organization\n\n        with transaction.atomic(router.db_for_write(Project)):\n            project = Project.objects.create(organization=organization, **kwargs)\n            if teams:\n                for team in teams:\n                    project.add_team(team)\n            if fire_project_created:\n                project_created.send(\n                    project=project, user=AnonymousUser(), default_rules=True, sender=Factories\n                )\n        return project\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_template(project=None, organization=None, **kwargs) -> ProjectTemplate:\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n\n        with transaction.atomic(router.db_for_write(Project)):\n            project_template = ProjectTemplate.objects.create(organization=organization, **kwargs)\n\n        return project_template\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_bookmark(project, user):\n        return ProjectBookmark.objects.create(project_id=project.id, user_id=user.id)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_rule(\n        project,\n        action_data=None,\n        allow_no_action_data=False,\n        condition_data=None,\n        name=\"Test Alert\",\n        action_match=\"all\",\n        filter_match=\"all\",\n        frequency=30,\n        **kwargs,\n    ):\n        actions = None\n        if not allow_no_action_data:\n            action_data = action_data or [\n                {\n                    \"id\": \"sentry.rules.actions.notify_event.NotifyEventAction\",\n                    \"name\": \"Send a notification (for all legacy integrations)\",\n                },\n                {\n                    \"id\": \"sentry.rules.actions.notify_event_service.NotifyEventServiceAction\",\n                    \"service\": \"mail\",\n                    \"name\": \"Send a notification via mail\",\n                },\n            ]\n            actions = action_data\n        condition_data = condition_data or [\n            {\n                \"id\": \"sentry.rules.conditions.first_seen_event.FirstSeenEventCondition\",\n                \"name\": \"A new issue is created\",\n            },\n            {\n                \"id\": \"sentry.rules.conditions.every_event.EveryEventCondition\",\n                \"name\": \"The event occurs\",\n            },\n        ]\n        data = {\n            \"conditions\": condition_data,\n            \"action_match\": action_match,\n            \"filter_match\": filter_match,\n            \"frequency\": frequency,\n        }\n        if actions:\n            data[\"actions\"] = actions\n\n        return Rule.objects.create(\n            label=name,\n            project=project,\n            data=data,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_slack_project_rule(project, integration_id, channel_id=None, channel_name=None):\n        action_data = [\n            {\n                \"id\": \"sentry.rules.actions.notify_event.SlackNotifyServiceAction\",\n                \"name\": \"Send a Slack notification\",\n                \"workspace\": integration_id,\n                \"channel_id\": channel_id or \"123453\",\n                \"channel\": channel_name or \"#general\",\n            }\n        ]\n        return Factories.create_project_rule(project, action_data)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_key(project):\n        return project.key_set.get_or_create()[0]\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_tempest_credentials(\n        project: Project,\n        created_by: User | None = None,\n        client_id: str | None = None,\n        client_secret: str | None = None,\n        message: str = \"\",\n        message_type: str | None = None,\n        latest_fetched_item_id: str | None = None,\n    ):\n        if client_id is None:\n            client_id = str(uuid4())\n        if client_secret is None:\n            client_secret = str(uuid4())\n        if message_type is None:\n            message_type = TempestMessageType.ERROR\n\n        return TempestCredentials.objects.create(\n            project=project,\n            created_by_id=created_by.id if created_by else None,\n            client_id=client_id,\n            client_secret=client_secret,\n            message=message,\n            message_type=message_type,\n            latest_fetched_item_id=latest_fetched_item_id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release(\n        project: Project,\n        user: User | None = None,\n        version: str | None = None,\n        date_added: datetime | None = None,\n        additional_projects: Sequence[Project] | None = None,\n        environments: Sequence[Environment] | None = None,\n        date_released: datetime | None = None,\n        adopted: datetime | None = None,\n        unadopted: datetime | None = None,\n        status: int | None = ReleaseStatus.OPEN,\n    ):\n        if version is None:\n            version = hexlify(os.urandom(20)).decode()\n\n        if date_added is None:\n            date_added = timezone.now()\n\n        if additional_projects is None:\n            additional_projects = []\n\n        release = Release.objects.create(\n            version=version,\n            organization_id=project.organization_id,\n            date_added=date_added,\n            date_released=date_released,\n            status=status,\n        )\n\n        release.add_project(project)\n        for additional_project in additional_projects:\n            release.add_project(additional_project)\n\n        for environment in environments or []:\n            ReleaseEnvironment.objects.create(\n                organization=project.organization, release=release, environment=environment\n            )\n            for project in [project, *additional_projects]:\n                ReleaseProjectEnvironment.objects.create(\n                    project=project,\n                    release=release,\n                    environment=environment,\n                    adopted=adopted,\n                    unadopted=unadopted,\n                )\n\n        Activity.objects.create(\n            type=ActivityType.RELEASE.value,\n            project=project,\n            ident=Activity.get_version_ident(version),\n            user_id=user.id if user else None,\n            data={\"version\": version},\n        )\n\n        # add commits\n        if user:\n            author = Factories.create_commit_author(project=project, user=user)\n            repo = Factories.create_repo(project, name=f\"organization-{project.slug}\")\n            commit = Factories.create_commit(\n                project=project,\n                repo=repo,\n                author=author,\n                release=release,\n                key=\"deadbeef\",\n                message=\"placeholder commit message\",\n            )\n\n            release.update(authors=[str(author.id)], commit_count=1, last_commit_id=commit.id)\n\n        return release\n\n    @staticmethod\n    def create_group_release(project: Project, group: Group, release: Release) -> GroupRelease:\n        return GroupRelease.objects.create(\n            project_id=project.id,\n            group_id=group.id,\n            release_id=release.id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release_file(release_id, file=None, name=None, dist_id=None):\n        if file is None:\n            file = Factories.create_file(\n                name=\"log.txt\",\n                size=32,\n                headers={\"Content-Type\": \"text/plain\"},\n                checksum=\"dc1e3f3e411979d336c3057cce64294f3420f93a\",\n            )\n\n        if name is None:\n            name = file.name\n\n        organization_id = Release.objects.get(pk=release_id).organization.id\n\n        return ReleaseFile.objects.create(\n            organization_id=organization_id,\n            release_id=release_id,\n            name=name,\n            file=file,\n            dist_id=dist_id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_artifact_bundle_zip(\n        org=None, release=None, project=None, extra_files=None, fixture_path=\"artifact_bundle\"\n    ):\n        bundle = io.BytesIO()\n        bundle_dir = get_fixture_path(fixture_path)\n        with zipfile.ZipFile(bundle, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n            for path, content in (extra_files or {}).items():\n                zipf.writestr(path, content)\n            for path, _, files in os.walk(bundle_dir):\n                for filename in files:\n                    fullpath = os.path.join(path, filename)\n                    relpath = os.path.relpath(fullpath, bundle_dir)\n                    if filename == \"manifest.json\":\n                        manifest = _patch_artifact_manifest(\n                            fullpath, org, release, project, extra_files\n                        )\n                        zipf.writestr(relpath, manifest)\n                    else:\n                        zipf.write(fullpath, relpath)\n\n        return bundle.getvalue()\n\n    @classmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release_archive(cls, org, release: str, project=None, dist=None):\n        bundle = cls.create_artifact_bundle_zip(org, release, project)\n        file = File.objects.create(name=\"release-artifacts.zip\")\n        file.putfile(ContentFile(bundle))\n        release_obj = Release.objects.get(organization__slug=org, version=release)\n        return update_artifact_index(release_obj, dist, file)\n\n    @classmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_artifact_bundle(\n        cls,\n        org,\n        bundle_id=None,\n        artifact_count=0,\n        fixture_path=\"artifact_bundle_debug_ids\",\n        date_uploaded=None,\n        date_last_modified=None,\n    ):\n        if date_uploaded is None:\n            date_uploaded = timezone.now()\n\n        bundle = cls.create_artifact_bundle_zip(org.slug, fixture_path=fixture_path)\n        file_ = File.objects.create(name=\"artifact-bundle.zip\")\n        file_.putfile(ContentFile(bundle))\n        # The 'artifact_count' should correspond to the 'bundle' contents but for the purpose of tests we can also\n        # mock it with an arbitrary value.\n        artifact_bundle = ArtifactBundle.objects.create(\n            organization_id=org.id,\n            bundle_id=bundle_id or uuid4(),\n            file=file_,\n            artifact_count=artifact_count,\n            date_uploaded=date_uploaded,\n            date_last_modified=date_last_modified,\n        )\n        return artifact_bundle\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_code_mapping(project, repo=None, organization_integration=None, **kwargs):\n        kwargs.setdefault(\"stack_root\", \"\")\n        kwargs.setdefault(\"source_root\", \"\")\n        kwargs.setdefault(\"default_branch\", \"master\")\n\n        if not repo:\n            repo = Factories.create_repo(project=project)\n        return RepositoryProjectPathConfig.objects.create(\n            project=project,\n            repository=repo,\n            organization_integration_id=organization_integration.id,\n            integration_id=organization_integration.integration_id,\n            organization_id=organization_integration.organization_id,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_repo(\n        project, name=None, provider=None, integration_id=None, url=None, external_id=None\n    ):\n        repo, _ = Repository.objects.get_or_create(\n            organization_id=project.organization_id,\n            name=name\n            or \"{}-{}\".format(petname.generate(2, \"\", letters=10), random.randint(1000, 9999)),\n            provider=provider,\n            integration_id=integration_id,\n            url=url,\n            external_id=external_id,\n        )\n        return repo\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit(\n        repo, project=None, author=None, release=None, message=None, key=None, date_added=None\n    ):\n        commit = Commit.objects.get_or_create(\n            organization_id=repo.organization_id,\n            repository_id=repo.id,\n            key=key or sha1(uuid4().hex.encode(\"utf-8\")).hexdigest(),\n            defaults={\n                \"message\": message or make_sentence(),\n                \"author\": author\n                or Factories.create_commit_author(organization_id=repo.organization_id),\n                \"date_added\": date_added or timezone.now(),\n            },\n        )[0]\n\n        if release:\n            assert project\n            ReleaseCommit.objects.create(\n                organization_id=repo.organization_id,\n                project_id=project.id,\n                release=release,\n                commit=commit,\n                order=1,\n            )\n\n        Factories.create_commit_file_change(commit=commit, filename=\"/models/foo.py\")\n        Factories.create_commit_file_change(commit=commit, filename=\"/worsematch/foo.py\")\n        Factories.create_commit_file_change(commit=commit, filename=\"/models/other.py\")\n\n        return commit\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit_author(organization_id=None, project=None, user=None, email=None):\n        if email:\n            user_email = email\n        else:\n            user_email = user.email if user else f\"{make_word()}@example.com\"\n        return CommitAuthor.objects.get_or_create(\n            organization_id=organization_id or project.organization_id,\n            email=user_email,\n            defaults={\"name\": user.name if user else make_word()},\n        )[0]\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit_file_change(commit, filename):\n        return CommitFileChange.objects.get_or_create(\n            organization_id=commit.organization_id, commit=commit, filename=filename, type=\"M\"\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user(\n        email=None, is_superuser=False, is_staff=False, is_active=True, **kwargs\n    ) -> User:\n        if email is None:\n            email = uuid4().hex + \"@example.com\"\n\n        kwargs.setdefault(\"username\", email)\n\n        user = User(\n            email=email, is_superuser=is_superuser, is_staff=is_staff, is_active=is_active, **kwargs\n        )\n        if kwargs.get(\"password\") is None:\n            user.set_password(\"admin\")\n        user.save()\n\n        # UserEmail is created by a signal\n        assert UserEmail.objects.filter(user=user, email=email).update(is_verified=True)\n\n        return user\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_useremail(user, email=None, **kwargs):\n        if not email:\n            email = uuid4().hex + \"@example.com\"\n\n        kwargs.setdefault(\"is_verified\", True)\n\n        useremail = UserEmail(user=user, email=email, **kwargs)\n        useremail.save()\n\n        return useremail\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_avatar(*args, **kwargs):\n        return UserAvatar.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_role(*args, **kwargs):\n        return UserRole.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_usersocialauth(\n        user: User,\n        provider: str | None = None,\n        uid: str | None = None,\n        extra_data: dict[str, Any] | None = None,\n    ):\n        if not provider:\n            provider = \"asana\"\n        if not uid:\n            uid = \"abc-123\"\n        usa = UserSocialAuth(user=user, provider=provider, uid=uid, extra_data=extra_data)\n        usa.save()\n        return usa\n\n    @staticmethod\n    def inject_performance_problems(jobs, _):\n        for job in jobs:\n            job[\"performance_problems\"] = []\n            for f in job[\"data\"][\"fingerprint\"]:\n                f_data = f.split(\"-\", 1)\n                if len(f_data) < 2:\n                    raise ValueError(\n                        \"Invalid performance fingerprint data. Format must be 'group_type-fingerprint'.\"\n                    )\n                group_type = get_group_type_by_type_id(int(f_data[0]))\n                perf_fingerprint = f_data[1]\n\n                job[\"performance_problems\"].append(\n                    PerformanceProblem(\n                        fingerprint=perf_fingerprint,\n                        op=\"db\",\n                        desc=\"\",\n                        type=group_type,\n                        parent_span_ids=None,\n                        cause_span_ids=None,\n                        offender_span_ids=[],\n                        evidence_data={},\n                        evidence_display=[],\n                    )\n                )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def store_event(\n        data,\n        project_id: int,\n        assert_no_errors: bool = True,\n        default_event_type: EventType | None = None,\n        sent_at: datetime | None = None,\n    ) -> Event:\n        \"\"\"\n        Like `create_event`, but closer to how events are actually\n        ingested. Prefer to use this method over `create_event`\n        \"\"\"\n\n        # this creates a basic message event\n        if default_event_type == EventType.DEFAULT:\n            data.update({\"stacktrace\": copy.deepcopy(DEFAULT_EVENT_DATA[\"stacktrace\"])})\n\n        # this creates an error event\n        elif default_event_type == EventType.ERROR:\n            data.update({\"exception\": [{\"value\": \"BadError\"}]})\n\n        manager = EventManager(data, sent_at=sent_at)\n        manager.normalize()\n        if assert_no_errors:\n            errors = manager.get_data().get(\"errors\")\n            assert not errors, errors\n\n        normalized_data = manager.get_data()\n\n        _set_sample_rate_from_error_sampling(normalized_data)\n\n        event = None\n\n        # When fingerprint is present on transaction, inject performance problems\n        if (\n            normalized_data.get(\"type\") == \"transaction\"\n            and normalized_data.get(\"fingerprint\") is not None\n        ):\n            with mock.patch(\n                \"sentry.event_manager._detect_performance_problems\",\n                Factories.inject_performance_problems,\n            ):\n                event = manager.save(project_id)\n\n        else:\n            event = manager.save(project_id)\n\n        if event.groups:\n            for group in event.groups:\n                group.save()\n\n        if event.group:\n            event.group.save()\n\n        return event\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_group(project, create_open_period=True, **kwargs):\n        from sentry.models.group import GroupStatus\n        from sentry.models.groupopenperiod import GroupOpenPeriod\n        from sentry.testutils.helpers.datetime import before_now\n        from sentry.types.group import GroupSubStatus\n\n        kwargs.setdefault(\"message\", \"Hello world\")\n        kwargs.setdefault(\"data\", {})\n        if \"type\" not in kwargs[\"data\"]:\n            kwargs[\"data\"].update({\"type\": \"default\", \"metadata\": {\"title\": kwargs[\"message\"]}})\n        if \"short_id\" not in kwargs:\n            kwargs[\"short_id\"] = project.next_short_id()\n        if \"metadata\" in kwargs:\n            metadata = kwargs.pop(\"metadata\")\n            kwargs[\"data\"].setdefault(\"metadata\", {}).update(metadata)\n        if \"status\" not in kwargs:\n            kwargs[\"status\"] = GroupStatus.UNRESOLVED\n            kwargs[\"substatus\"] = GroupSubStatus.NEW\n\n        group = Group.objects.create(project=project, **kwargs)\n        if create_open_period:\n            open_period = GroupOpenPeriod.objects.create(\n                group=group,\n                project=project,\n                date_started=group.first_seen or before_now(minutes=5),\n            )\n            if group.status == GroupStatus.RESOLVED:\n                open_period.update(\n                    date_ended=group.resolved_at if group.resolved_at else timezone.now()\n                )\n\n        return group\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_file(**kwargs):\n        return File.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_file_from_path(path, name=None, **kwargs):\n        if name is None:\n            name = os.path.basename(path)\n\n        file = Factories.create_file(name=name, **kwargs)\n        with open(path) as f:\n            file.putfile(f)\n        return file\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dif_file(\n        project,\n        debug_id=None,\n        object_name=None,\n        features=None,\n        data=None,\n        file=None,\n        cpu_name=None,\n        code_id=None,\n        **kwargs,\n    ):\n        if debug_id is None:\n            debug_id = str(uuid4())\n\n        if object_name is None:\n            object_name = \"%s.dSYM\" % debug_id\n\n        if features is not None:\n            if data is None:\n                data = {}\n            data[\"features\"] = features\n\n        if file is None:\n            file = Factories.create_file(\n                name=object_name,\n                size=42,\n                headers={\"Content-Type\": \"application/x-mach-binary\"},\n                checksum=\"dc1e3f3e411979d336c3057cce64294f3420f93a\",\n            )\n\n        return ProjectDebugFile.objects.create(\n            debug_id=debug_id,\n            code_id=code_id,\n            project_id=project.id,\n            object_name=object_name,\n            cpu_name=cpu_name or \"x86_64\",\n            file=file,\n            checksum=file.checksum,\n            data=data,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dif_from_path(path, object_name=None, **kwargs):\n        if object_name is None:\n            object_name = os.path.basename(path)\n\n        headers = {\"Content-Type\": \"application/x-mach-binary\"}\n        file = Factories.create_file_from_path(path, name=object_name, headers=headers)\n        return Factories.create_dif_file(file=file, object_name=object_name, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def add_user_permission(user, permission):\n        UserPermission.objects.create(user=user, permission=permission)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app(**kwargs):\n        published = kwargs.pop(\"published\", False)\n        args = Factories._sentry_app_kwargs(**kwargs)\n        user = args.pop(\"user\", None)\n        app = SentryAppCreator(is_internal=False, **args).run(user=user, request=None)\n\n        if published:\n            app.update(status=SentryAppStatus.PUBLISHED)\n\n        return app\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_avatar(*args, **kwargs):\n        return SentryAppAvatar.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_internal_integration(**kwargs) -> SentryApp:\n        args = Factories._sentry_app_kwargs(**kwargs)\n        args[\"verify_install\"] = False\n        user = args.pop(\"user\", None)\n        app = SentryAppCreator(is_internal=True, **args).run(\n            user=user, request=None, skip_default_auth_token=True\n        )\n        return app\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_internal_integration_token(\n        user,\n        internal_integration: SentryApp | None = None,\n        install: SentryAppInstallation | None = None,\n        request=None,\n    ) -> ApiToken:\n        if internal_integration and install:\n            raise ValueError(\"Only one of internal_integration or install arg can be provided\")\n        elif internal_integration is None and install is None:\n            raise ValueError(\"Must pass in either internal_integration or install arg\")\n\n        if internal_integration is not None and install is None:\n            # Fetch install from provided or created internal integration\n            with assume_test_silo_mode(SiloMode.CONTROL):\n                install = SentryAppInstallation.objects.get(\n                    sentry_app=internal_integration.id,\n                    organization_id=internal_integration.owner_id,\n                )\n        elif install is None:\n            raise AssertionError(\"unreachable\")\n\n        return SentryAppInstallationTokenCreator(sentry_app_installation=install).run(\n            user=user, request=request\n        )\n\n    @staticmethod\n    def _sentry_app_kwargs(**kwargs):\n        _kwargs = {\n            \"user\": kwargs.get(\"user\", Factories.create_user()),\n            \"name\": kwargs.get(\"name\", petname.generate(2, \" \", letters=10).title()),\n            \"organization_id\": kwargs.get(\n                \"organization_id\", kwargs.pop(\"organization\", Factories.create_organization()).id\n            ),\n            \"author\": kwargs.get(\"author\", \"A Company\"),\n            \"scopes\": kwargs.get(\"scopes\", ()),\n            \"verify_install\": kwargs.get(\"verify_install\", True),\n            \"webhook_url\": kwargs.get(\"webhook_url\", \"https://example.com/webhook\"),\n            \"events\": [],\n            \"schema\": {},\n        }\n\n        _kwargs.update(**kwargs)\n        return _kwargs\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_sentry_app_installation(\n        organization=None,\n        slug=None,\n        user=None,\n        status=None,\n        prevent_token_exchange=False,\n    ):\n        if not organization:\n            organization = Factories.create_organization()\n\n        Factories.create_project(organization=organization)\n\n        with assume_test_silo_mode(SiloMode.CONTROL):\n            install = SentryAppInstallationCreator(\n                slug=(slug or Factories.create_sentry_app(organization=organization).slug),\n                organization_id=organization.id,\n            ).run(\n                user=(user or Factories.create_user()),\n                request=None,\n            )\n\n            install.status = SentryAppInstallationStatus.INSTALLED if status is None else status\n            install.save()\n            if not prevent_token_exchange and (\n                install.sentry_app.status != SentryAppStatus.INTERNAL\n            ):\n                assert install.api_grant is not None\n                assert install.sentry_app.application is not None\n                assert install.sentry_app.proxy_user is not None\n                GrantExchanger(\n                    install=install,\n                    code=install.api_grant.code,\n                    client_id=install.sentry_app.application.client_id,\n                    user=install.sentry_app.proxy_user,\n                ).run()\n                install = SentryAppInstallation.objects.get(id=install.id)\n        return install\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_installation_for_provider(\n        sentry_app_id: int,\n        organization_id: int,\n        provider: str,\n    ) -> SentryAppInstallationForProvider:\n        installation = SentryAppInstallation.objects.get(\n            sentry_app_id=sentry_app_id, organization_id=organization_id\n        )\n        return SentryAppInstallationForProvider.objects.create(\n            organization_id=organization_id,\n            provider=provider,\n            sentry_app_installation=installation,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_stacktrace_link_schema():\n        return {\"type\": \"stacktrace-link\", \"uri\": \"/redirect/\"}\n\n    @staticmethod\n    def create_issue_link_schema():\n        return {\n            \"type\": \"issue-link\",\n            \"link\": {\n                \"uri\": \"/sentry/issues/link\",\n                \"required_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    }\n                ],\n            },\n            \"create\": {\n                \"uri\": \"/sentry/issues/create\",\n                \"required_fields\": [\n                    {\"type\": \"text\", \"name\": \"title\", \"label\": \"Title\"},\n                    {\"type\": \"text\", \"name\": \"summary\", \"label\": \"Summary\"},\n                ],\n                \"optional_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"points\",\n                        \"label\": \"Points\",\n                        \"options\": [[\"1\", \"1\"], [\"2\", \"2\"], [\"3\", \"3\"], [\"5\", \"5\"], [\"8\", \"8\"]],\n                    },\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    },\n                ],\n            },\n        }\n\n    @staticmethod\n    def create_alert_rule_action_schema():\n        return {\n            \"type\": \"alert-rule-action\",\n            \"title\": \"Create Task with App\",\n            \"settings\": {\n                \"type\": \"alert-rule-settings\",\n                \"uri\": \"/sentry/alert-rule\",\n                \"required_fields\": [\n                    {\"type\": \"text\", \"name\": \"title\", \"label\": \"Title\"},\n                    {\"type\": \"text\", \"name\": \"summary\", \"label\": \"Summary\"},\n                ],\n                \"optional_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"points\",\n                        \"label\": \"Points\",\n                        \"options\": [[\"1\", \"1\"], [\"2\", \"2\"], [\"3\", \"3\"], [\"5\", \"5\"], [\"8\", \"8\"]],\n                    },\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    },\n                ],\n            },\n        }\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_service_hook(\n        actor=None, org=None, project=None, events=None, url=None, project_ids=None, **kwargs\n    ):\n        if project:\n            if project_ids is not None:\n                raise ValueError(\"Cannot provide both project and project_ids\")\n            project_ids = [project.id]\n        if not actor:\n            actor = Factories.create_user()\n        if not org:\n            if project:\n                org = project.organization\n            else:\n                org = Factories.create_organization(owner=actor)\n        if project_ids is None:  # empty list for project_ids is valid and means no project filter\n            project_ids = [Factories.create_project(organization=org).id]\n        if events is None:\n            events = [\"event.created\"]\n        if not url:\n            url = \"https://example.com/sentry/webhook\"\n\n        app_id = kwargs.pop(\"application_id\", None)\n        if app_id is None and \"application\" in kwargs:\n            app_id = kwargs[\"application\"].id\n        installation_id = kwargs.pop(\"installation_id\", None)\n        if installation_id is None and \"installation\" in kwargs:\n            installation_id = kwargs[\"installation\"].id\n        hook_id = hook_service.create_service_hook(\n            application_id=app_id,\n            actor_id=actor.id,\n            installation_id=installation_id,\n            organization_id=org.id,\n            project_ids=project_ids,\n            events=events,\n            url=url,\n        ).id\n        return ServiceHook.objects.get(id=hook_id)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_feature(feature=None, sentry_app=None, description=None):\n        if not sentry_app:\n            sentry_app = Factories.create_sentry_app()\n\n        integration_feature = IntegrationFeature.objects.create(\n            target_id=sentry_app.id,\n            target_type=IntegrationTypes.SENTRY_APP.value,\n            feature=feature or Feature.API,\n        )\n\n        if description:\n            integration_feature.update(user_description=description)\n\n        return integration_feature\n\n    @staticmethod\n    def _doc_integration_kwargs(**kwargs):\n        _kwargs = {\n            \"name\": kwargs.get(\"name\", petname.generate(2, \" \", letters=10).title()),\n            \"author\": kwargs.get(\"author\", \"me\"),\n            \"description\": kwargs.get(\"description\", \"hi im a description\"),\n            \"url\": kwargs.get(\"url\", \"https://sentry.io\"),\n            \"popularity\": kwargs.get(\"popularity\", 1),\n            \"is_draft\": kwargs.get(\"is_draft\", True),\n            \"metadata\": kwargs.get(\"metadata\", {}),\n        }\n        _kwargs[\"slug\"] = slugify(_kwargs[\"name\"])\n        _kwargs.update(**kwargs)\n        return _kwargs\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration(features=None, has_avatar: bool = False, **kwargs) -> DocIntegration:\n        doc = DocIntegration.objects.create(**Factories._doc_integration_kwargs(**kwargs))\n        if features:\n            Factories.create_doc_integration_features(features=features, doc_integration=doc)\n        if has_avatar:\n            Factories.create_doc_integration_avatar(doc_integration=doc)\n        return doc\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration_features(\n        features=None, doc_integration=None\n    ) -> list[IntegrationFeature]:\n        if not features:\n            features = [Feature.API]\n        if not doc_integration:\n            doc_integration = Factories.create_doc_integration()\n        return IntegrationFeature.objects.bulk_create(\n            [\n                IntegrationFeature(\n                    target_id=doc_integration.id,\n                    target_type=IntegrationTypes.DOC_INTEGRATION.value,\n                    feature=feature,\n                )\n                for feature in features\n            ]\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration_avatar(doc_integration=None, **kwargs) -> DocIntegrationAvatar:\n        if not doc_integration:\n            doc_integration = Factories.create_doc_integration()\n        photo = ControlFile.objects.create(name=\"test.png\", type=\"avatar.file\")\n        photo.putfile(io.BytesIO(b\"imaginethiswasphotobytes\"))\n\n        return DocIntegrationAvatar.objects.create(\n            doc_integration=doc_integration, avatar_type=0, control_file_id=photo.id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_userreport(\n        project: Project, event_id: str | None = None, **kwargs: Any\n    ) -> UserReport:\n        event = Factories.store_event(\n            data={\n                \"timestamp\": datetime.now(UTC).isoformat(),\n                \"event_id\": event_id or \"a\" * 32,\n                \"message\": \"testing\",\n            },\n            project_id=project.id,\n        )\n        assert event.group is not None\n\n        return UserReport.objects.create(\n            group_id=event.group.id,\n            event_id=event.event_id,\n            project_id=project.id,\n            name=\"Jane Bloggs\",\n            email=\"jane@example.com\",\n            comments=\"the application crashed\",\n            **kwargs,\n        )\n\n    @staticmethod\n    def create_session():\n        engine = import_module(settings.SESSION_ENGINE)\n\n        session = engine.SessionStore()\n        session.save()\n        return session\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_platform_external_issue(\n        group=None, service_type=None, display_name=None, web_url=None\n    ):\n        return PlatformExternalIssue.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            service_type=service_type,\n            display_name=display_name,\n            web_url=web_url,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_integration_external_issue(group=None, integration=None, key=None, **kwargs):\n        external_issue = ExternalIssue.objects.create(\n            organization_id=group.organization.id, integration_id=integration.id, key=key, **kwargs\n        )\n\n        GroupLink.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            linked_type=GroupLink.LinkedType.issue,\n            linked_id=external_issue.id,\n            relationship=GroupLink.Relationship.references,\n        )\n\n        return external_issue\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_integration_external_project(\n        organization_id: int, integration_id: int, *args: Any, **kwargs: Any\n    ) -> IntegrationExternalProject:\n        oi = OrganizationIntegration.objects.get(\n            organization_id=organization_id, integration_id=integration_id\n        )\n        return IntegrationExternalProject.objects.create(\n            organization_integration_id=oi.id, *args, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident(\n        organization,\n        projects,\n        detection_uuid=None,\n        status=1,\n        title=None,\n        query=\"test query\",\n        date_started=None,\n        date_detected=None,\n        date_closed=None,\n        alert_rule=None,\n        subscription=None,\n    ):\n        if not title:\n            title = petname.generate(2, \" \", letters=10).title()\n        if alert_rule is None:\n            alert_rule = Factories.create_alert_rule(\n                organization, projects, query=query, time_window=1\n            )\n\n        incident = Incident.objects.create(\n            organization=organization,\n            detection_uuid=detection_uuid,\n            status=status,\n            title=title,\n            alert_rule=alert_rule,\n            date_started=date_started or timezone.now(),\n            date_detected=date_detected or timezone.now(),\n            date_closed=timezone.now() if date_closed is not None else date_closed,\n            type=IncidentType.ALERT_TRIGGERED.value,\n            subscription=subscription,\n        )\n        for project in projects:\n            IncidentProject.objects.create(incident=incident, project=project)\n\n        return incident\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_activity(incident, type, comment=None, user_id=None):\n        return IncidentActivity.objects.create(\n            incident=incident, type=type, comment=comment, user_id=user_id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule(\n        organization,\n        projects,\n        name=None,\n        owner=None,\n        query=\"level:error\",\n        aggregate=\"count()\",\n        time_window=10,\n        threshold_period=1,\n        environment=None,\n        date_added=None,\n        query_type=None,\n        dataset=Dataset.Events,\n        threshold_type=AlertRuleThresholdType.ABOVE,\n        resolve_threshold=None,\n        user=None,\n        event_types=None,\n        comparison_delta=None,\n        description=None,\n        sensitivity=None,\n        seasonality=None,\n        detection_type=AlertRuleDetectionType.STATIC,\n    ):\n        if not name:\n            name = petname.generate(2, \" \", letters=10).title()\n\n        if query_type is None:\n            query_type = query_datasets_to_type[dataset]\n\n        alert_rule = create_alert_rule(\n            organization,\n            projects,\n            name,\n            query,\n            aggregate,\n            time_window,\n            threshold_type,\n            threshold_period,\n            owner=owner,\n            resolve_threshold=resolve_threshold,\n            query_type=query_type,\n            dataset=dataset,\n            environment=environment,\n            user=user,\n            event_types=event_types,\n            comparison_delta=comparison_delta,\n            description=description,\n            sensitivity=sensitivity,\n            seasonality=seasonality,\n            detection_type=detection_type,\n        )\n\n        if date_added is not None:\n            alert_rule.update(date_added=date_added)\n\n        return alert_rule\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_trigger(alert_rule, label=None, alert_threshold=100):\n        if not label:\n            label = petname.generate(2, \" \", letters=10).title()\n\n        return create_alert_rule_trigger(alert_rule, label, alert_threshold)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_trigger(incident, alert_rule_trigger, status=None):\n        if status is None:\n            status = TriggerStatus.ACTIVE.value\n\n        return IncidentTrigger.objects.create(\n            alert_rule_trigger=alert_rule_trigger, incident=incident, status=status\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_trigger_action(\n        trigger,\n        type=AlertRuleTriggerAction.Type.EMAIL,\n        target_type=AlertRuleTriggerAction.TargetType.USER,\n        target_identifier=None,\n        integration=None,\n        sentry_app=None,\n        sentry_app_config=None,\n    ):\n        return create_alert_rule_trigger_action(\n            trigger,\n            type,\n            target_type,\n            target_identifier,\n            integration.id if integration else None,\n            sentry_app.id if sentry_app else None,\n            sentry_app_config=sentry_app_config,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_external_user(user: User, **kwargs: Any) -> ExternalActor:\n        kwargs.setdefault(\"provider\", ExternalProviders.GITHUB.value)\n        kwargs.setdefault(\"external_name\", \"\")\n\n        return ExternalActor.objects.create(user_id=user.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_external_team(team: Team, **kwargs: Any) -> ExternalActor:\n        kwargs.setdefault(\"provider\", ExternalProviders.GITHUB.value)\n        kwargs.setdefault(\"external_name\", \"@getsentry/ecosystem\")\n\n        return ExternalActor.objects.create(team_id=team.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_codeowners(project, code_mapping, **kwargs):\n        kwargs.setdefault(\"raw\", \"\")\n\n        return ProjectCodeOwners.objects.create(\n            project=project, repository_project_path_config=code_mapping, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_slack_integration(\n        organization: Organization, external_id: str, **kwargs: Any\n    ) -> Integration:\n        integration = Integration.objects.create(\n            provider=\"slack\",\n            name=\"Team A\",\n            external_id=external_id,\n            metadata={\n                \"access_token\": \"xoxp-xxxxxxxxx-xxxxxxxxxx-xxxxxxxxxxxx\",\n                \"installation_type\": \"born_as_bot\",\n            },\n        )\n        integration.add_organization(organization)\n        return integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_integration(\n        organization: Organization,\n        external_id: str,\n        oi_params: Mapping[str, Any] | None = None,\n        **integration_params: Any,\n    ) -> Integration:\n        integration = Integration.objects.create(external_id=external_id, **integration_params)\n        with outbox_runner():\n            organization_integration = integration.add_organization(organization)\n            assert organization_integration is not None\n        organization_integration.update(**(oi_params or {}))\n\n        return integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_provider_integration(**integration_params: Any) -> Integration:\n        return Integration.objects.create(**integration_params)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_provider_integration_for(\n        organization: Organization | RpcOrganization,\n        user: User | RpcUser | None,\n        **integration_params: Any,\n    ) -> tuple[Integration, OrganizationIntegration]:\n        integration = Integration.objects.create(**integration_params)\n        org_integration = integration.add_organization(organization, user)\n        assert org_integration is not None\n        return integration, org_integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity_integration(\n        user: User | RpcUser,\n        organization: Organization | RpcOrganization,\n        integration_params: Mapping[Any, Any],\n        identity_params: Mapping[Any, Any],\n    ) -> tuple[Integration, OrganizationIntegration, Identity, IdentityProvider]:\n        # Avoid common pitfalls in tests\n        assert \"provider\" in integration_params\n        assert \"external_id\" in integration_params\n        assert \"external_id\" in identity_params\n\n        integration = Factories.create_provider_integration(**integration_params)\n        identity_provider = Factories.create_identity_provider(integration=integration)\n        identity = Factories.create_identity(\n            user=user, identity_provider=identity_provider, **identity_params\n        )\n        organization_integration = integration.add_organization(\n            organization_id=organization.id, user=user, default_auth_id=identity.id\n        )\n        assert organization_integration is not None\n        return integration, organization_integration, identity, identity_provider\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_organization_integration(**integration_params: Any) -> OrganizationIntegration:\n        return OrganizationIntegration.objects.create(**integration_params)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity_provider(\n        integration: Integration | None = None,\n        config: dict[str, Any] | None = None,\n        **kwargs: Any,\n    ) -> IdentityProvider:\n        if integration is not None:\n            integration_values = dict(\n                type=integration.provider,\n                external_id=integration.external_id,\n            )\n            if any((key in kwargs) for key in integration_values):\n                raise ValueError(\n                    \"Values from integration should not be in kwargs: \"\n                    + repr(list(integration_values.keys()))\n                )\n            kwargs.update(integration_values)\n\n        return IdentityProvider.objects.create(config=config or {}, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity(\n        user: User | RpcUser, identity_provider: IdentityProvider, external_id: str, **kwargs: Any\n    ) -> Identity:\n        return Identity.objects.create(\n            external_id=external_id,\n            idp=identity_provider,\n            user_id=user.id,\n            status=IdentityStatus.VALID,\n            scopes=[],\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_group_history(\n        group: Group,\n        status: int,\n        release: Release | None = None,\n        user_id: int | None = None,\n        team_id: int | None = None,\n        prev_history: GroupHistory | None = None,\n        date_added: datetime | None = None,\n    ) -> GroupHistory:\n        prev_history_date = None\n        if prev_history:\n            prev_history_date = prev_history.date_added\n\n        kwargs = {}\n        if date_added:\n            kwargs[\"date_added\"] = date_added\n        return GroupHistory.objects.create(\n            organization=group.organization,\n            group=group,\n            project=group.project,\n            release=release,\n            user_id=user_id,\n            team_id=team_id,\n            status=status,\n            prev_history=prev_history,\n            prev_history_date=prev_history_date,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_comment(issue, project, user, text=\"hello world\"):\n        data = {\"text\": text}\n        return Activity.objects.create(\n            project=project,\n            group=issue,\n            type=ActivityType.NOTE.value,\n            user_id=user.id,\n            data=data,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_saved_search(name: str, **kwargs):\n        if \"owner\" in kwargs:\n            owner = kwargs.pop(\"owner\")\n            kwargs[\"owner_id\"] = owner.id if not isinstance(owner, int) else owner\n        return SavedSearch.objects.create(name=name, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_notification_action(\n        organization: Organization | None = None,\n        projects: list[Project] | None = None,\n        **kwargs,\n    ):\n        if not organization:\n            organization = Factories.create_organization()\n\n        if not projects:\n            projects = []\n\n        action_kwargs = {\n            \"organization\": organization,\n            \"type\": ActionService.SENTRY_NOTIFICATION,\n            \"target_type\": ActionTarget.USER,\n            \"target_identifier\": \"1\",\n            \"target_display\": \"Sentry User\",\n            \"trigger_type\": ActionTrigger.AUDIT_LOG,\n            **kwargs,\n        }\n\n        action = NotificationAction.objects.create(**action_kwargs)\n        action.projects.add(*projects)\n        action.save()\n\n        return action\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_notification_settings_provider(*args, **kwargs) -> NotificationSettingProvider:\n        return NotificationSettingProvider.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_option(*args, **kwargs) -> UserOption:\n        return UserOption.objects.create(*args, **kwargs)\n\n    @staticmethod\n    def create_basic_auth_header(username: str, password: str = \"\") -> bytes:\n        return b\"Basic \" + b64encode(f\"{username}:{password}\".encode())\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def snooze_rule(**kwargs):\n        return RuleSnooze.objects.create(**kwargs)\n\n    @staticmethod\n    def create_request_access(\n        sso_state: RpcMemberSsoState | None = None,\n        permissions: list | None = None,\n        org_context: RpcUserOrganizationContext | None = None,\n        scopes_upper_bound: frozenset | None = frozenset(),\n    ) -> RpcBackedAccess:\n        if not sso_state:\n            sso_state = RpcMemberSsoState()\n        if not permissions:\n            permissions = []\n        if not org_context:\n            org_context = RpcUserOrganizationContext()\n\n        auth_state = RpcAuthState(sso_state=sso_state, permissions=permissions)\n        return RpcBackedAccess(\n            rpc_user_organization_context=org_context,\n            auth_state=auth_state,\n            scopes_upper_bound=scopes_upper_bound,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_webhook_payload(mailbox_name: str, region_name: str, **kwargs) -> WebhookPayload:\n        payload_kwargs = {\n            \"request_method\": \"POST\",\n            \"request_path\": \"/extensions/github/webhook/\",\n            \"request_headers\": '{\"Content-Type\": \"application/json\"}',\n            \"request_body\": \"{}\",\n            **kwargs,\n        }\n        return WebhookPayload.objects.create(\n            mailbox_name=mailbox_name, region_name=region_name, **payload_kwargs\n        )\n\n    @staticmethod\n    def create_uptime_subscription(\n        type: str,\n        subscription_id: str | None,\n        status: UptimeSubscription.Status,\n        url: str | None,\n        url_domain: str,\n        url_domain_suffix: str,\n        host_provider_id: str,\n        host_provider_name: str,\n        interval_seconds: IntervalSecondsLiteral,\n        timeout_ms: int,\n        method,\n        headers,\n        body,\n        date_updated: datetime,\n        uptime_status: UptimeStatus,\n        uptime_status_update_date: datetime,\n        trace_sampling: bool = False,\n    ):\n        if url is None:\n            url = petname.generate().title()\n            url = f\"http://{url}.com\"\n\n        return UptimeSubscription.objects.create(\n            type=type,\n            subscription_id=subscription_id,\n            status=status.value,\n            url=url,\n            url_domain=url_domain,\n            url_domain_suffix=url_domain_suffix,\n            host_provider_id=host_provider_id,\n            host_provider_name=host_provider_name,\n            interval_seconds=interval_seconds,\n            timeout_ms=timeout_ms,\n            date_updated=date_updated,\n            method=method,\n            headers=headers,\n            body=body,\n            trace_sampling=trace_sampling,\n            uptime_status=uptime_status,\n            uptime_status_update_date=uptime_status_update_date,\n        )\n\n    @staticmethod\n    def create_project_uptime_subscription(\n        project: Project,\n        env: Environment | None,\n        uptime_subscription: UptimeSubscription,\n        status: int,\n        mode: UptimeMonitorMode,\n        name: str | None,\n        owner: Actor | None,\n        id: int | None,\n    ):\n        if name is None:\n            name = petname.generate().title()\n        owner_team_id = None\n        owner_user_id = None\n        if owner:\n            if owner.is_team:\n                owner_team_id = owner.id\n            elif owner.is_user:\n                owner_user_id = owner.id\n\n        return ProjectUptimeSubscription.objects.create(\n            uptime_subscription=uptime_subscription,\n            project=project,\n            environment=env,\n            status=status,\n            mode=mode,\n            name=name,\n            owner_team_id=owner_team_id,\n            owner_user_id=owner_user_id,\n            pk=id,\n        )\n\n    @staticmethod\n    def create_uptime_subscription_region(\n        subscription: UptimeSubscription,\n        region_slug: str,\n        mode: UptimeSubscriptionRegion.RegionMode,\n    ) -> UptimeSubscriptionRegion:\n        return UptimeSubscriptionRegion.objects.create(\n            uptime_subscription=subscription,\n            region_slug=region_slug,\n            mode=mode,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard(\n        organization: Organization | None = None,\n        title: str | None = None,\n        created_by: User | None = None,\n        **kwargs,\n    ):\n        if organization is None:\n            organization = Factories.create_organization()\n        if created_by is None:\n            created_by = Factories.create_user()\n            Factories.create_member(organization=organization, user=created_by, role=\"owner\")\n        if title is None:\n            title = petname.generate(2, \" \", letters=10).title()\n        return Dashboard.objects.create(\n            organization=organization, title=title, created_by_id=created_by.id, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard_widget(\n        order: int,\n        dashboard: Dashboard | None = None,\n        title: str | None = None,\n        display_type: int | None = None,\n        **kwargs,\n    ):\n        if dashboard is None:\n            dashboard = Factories.create_dashboard()\n        if display_type is None:\n            display_type = DashboardWidgetDisplayTypes.AREA_CHART\n        if title is None:\n            title = petname.generate(2, \" \", letters=10).title()\n\n        return DashboardWidget.objects.create(\n            dashboard=dashboard, title=title, display_type=display_type, order=order, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard_widget_query(\n        order: int,\n        widget: DashboardWidget | None = None,\n        name: str | None = None,\n        **kwargs,\n    ):\n        if widget is None:\n            widget = Factories.create_dashboard_widget(order=order)\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        return DashboardWidgetQuery.objects.create(widget=widget, name=name, order=order, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_workflow(\n        name: str | None = None,\n        organization: Organization | None = None,\n        config: dict[str, Any] | None = None,\n        **kwargs,\n    ) -> Workflow:\n        if organization is None:\n            organization = Factories.create_organization()\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        if config is None:\n            config = {}\n        return Workflow.objects.create(\n            organization=organization, name=name, config=config, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition_group(\n        organization: Organization | None = None,\n        **kwargs,\n    ) -> DataConditionGroup:\n        if organization is None:\n            organization = Factories.create_organization()\n        return DataConditionGroup.objects.create(organization=organization, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_workflow_data_condition_group(\n        workflow: Workflow | None = None,\n        condition_group: DataConditionGroup | None = None,\n        **kwargs,\n    ) -> WorkflowDataConditionGroup:\n        if workflow is None:\n            workflow = Factories.create_workflow()\n\n        if not condition_group:\n            condition_group = Factories.create_data_condition_group()\n\n        return WorkflowDataConditionGroup.objects.create(\n            workflow=workflow, condition_group=condition_group\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition(\n        condition_group: DataConditionGroup | None = None, **kwargs\n    ) -> DataCondition:\n        if condition_group is None:\n            condition_group = Factories.create_data_condition_group()\n        return DataCondition.objects.create(condition_group=condition_group, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_source(\n        organization: Organization | None = None,\n        source_id: str | None = None,\n        type: str | None = None,\n        **kwargs,\n    ) -> DataSource:\n        if organization is None:\n            organization = Factories.create_organization()\n        if source_id is None:\n            source_id = str(random.randint(1, 10000))\n        if type is None:\n            type = data_source_type_registry.get_key(QuerySubscriptionDataSourceHandler)\n        return DataSource.objects.create(organization=organization, source_id=source_id, type=type)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector(\n        name: str | None = None,\n        config: dict | None = None,\n        **kwargs,\n    ) -> Detector:\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        if config is None:\n            config = default_detector_config_data.get(kwargs[\"type\"], {})\n\n        return Detector.objects.create(\n            name=name,\n            config=config,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector_state(\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> DetectorState:\n        if detector is None:\n            detector = Factories.create_detector()\n\n        return DetectorState.objects.create(detector=detector, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_source_detector(\n        data_source: DataSource | None = None,\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> DataSourceDetector:\n        if data_source is None:\n            data_source = Factories.create_data_source()\n        if detector is None:\n            detector = Factories.create_detector()\n        return DataSourceDetector.objects.create(data_source=data_source, detector=detector)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_action(\n        config: dict[str, Any] | None = None,\n        type: Action.Type | None = None,\n        data: dict[str, Any] | None = None,\n        **kwargs,\n    ) -> Action:\n        if config is None and type is None and data is None:\n            # Default to a slack action with nice defaults so someone can just do\n            # self.create_action() and have a sane default\n            config = {\n                \"target_identifier\": \"1\",\n                \"target_display\": \"Sentry User\",\n                \"target_type\": ActionTarget.SPECIFIC,\n            }\n\n            data = {\"notes\": \"bufos are great\", \"tags\": \"bufo-bot\"}\n\n        if config is None:\n            config = {}\n\n        if data is None:\n            data = {}\n\n        if type is None:\n            type = Action.Type.SLACK\n\n        return Action.objects.create(type=type, config=config, data=data, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector_workflow(\n        detector: Detector | None = None,\n        workflow: Workflow | None = None,\n        **kwargs,\n    ) -> DetectorWorkflow:\n        if detector is None:\n            detector = Factories.create_detector()\n        if workflow is None:\n            workflow = Factories.create_workflow()\n        return DetectorWorkflow.objects.create(detector=detector, workflow=workflow, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_workflow(\n        alert_rule_id: int | None = None,\n        rule_id: int | None = None,\n        workflow: Workflow | None = None,\n        **kwargs,\n    ) -> AlertRuleWorkflow:\n        if rule_id is None and alert_rule_id is None:\n            raise ValueError(\"Either rule_id or alert_rule_id must be provided\")\n\n        if rule_id is not None and alert_rule_id is not None:\n            raise ValueError(\"Only one of rule_id or alert_rule_id can be provided\")\n\n        if workflow is None:\n            workflow = Factories.create_workflow()\n\n        return AlertRuleWorkflow.objects.create(\n            alert_rule_id=alert_rule_id, rule_id=rule_id, workflow=workflow, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_group_open_period(\n        incident: Incident,\n        group_open_period: GroupOpenPeriod,\n        **kwargs,\n    ) -> IncidentGroupOpenPeriod:\n        return IncidentGroupOpenPeriod.objects.create(\n            incident_id=incident.id,\n            incident_identifier=incident.identifier,\n            group_open_period=group_open_period,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_detector(\n        alert_rule_id: int | None = None,\n        rule_id: int | None = None,\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> AlertRuleDetector:\n        if rule_id is None and alert_rule_id is None:\n            raise ValueError(\"Either rule_id or alert_rule_id must be provided\")\n\n        if rule_id is not None and alert_rule_id is not None:\n            raise ValueError(\"Only one of rule_id or alert_rule_id can be provided\")\n\n        if detector is None:\n            detector = Factories.create_detector()\n\n        return AlertRuleDetector.objects.create(\n            alert_rule_id=alert_rule_id, rule_id=rule_id, detector=detector, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_action_alert_rule_trigger_action(\n        alert_rule_trigger_action_id: int,\n        action: Action | None = None,\n        **kwargs,\n    ) -> ActionAlertRuleTriggerAction:\n        if action is None:\n            action = Factories.create_action()\n\n        return ActionAlertRuleTriggerAction.objects.create(\n            action=action, alert_rule_trigger_action_id=alert_rule_trigger_action_id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition_group_action(\n        action: Action | None = None,\n        condition_group: DataConditionGroup | None = None,\n        **kwargs,\n    ) -> DataConditionGroupAction:\n        if action is None:\n            action = Factories.create_action()\n        if condition_group is None:\n            condition_group = Factories.create_data_condition_group()\n        return DataConditionGroupAction.objects.create(\n            action=action, condition_group=condition_group, **kwargs\n        )\n",
    "file_path": "/Users/wangyue/Code/CodeReviewData/ReviewDataset/sentry-greptile/src/sentry/testutils/factories.py",
    "line_count": 2375,
    "encoding": "utf-8",
    "error": null
}

Message 7 [Assistant]:
--------------------------------------------------------------------------------
Content:
让我搜索`store_event`函数，看看`_set_sample_rate_from_error_sampling`是如何被使用的。
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "def store_event",
      "is_regex": false,
      "case_sensitive": true,
      "context_lines": 30
    },
    "id": "call_00_5FjBED2bCcj9LnjzdADoPbfl",
    "type": "tool_call"
  }
]

Message 8 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_5FjBED2bCcj9LnjzdADoPbfl
Result:
{
    "matches": [
        {
            "file": "tests/snuba/search/test_backend.py",
            "line_number": 97,
            "matched_line": "    def store_event(self, data, *args, **kwargs):",
            "context_start_line": 67,
            "context_end_line": 127,
            "context": [
                "67:         date_from=None,",
                "68:         date_to=None,",
                "69:         cursor=None,",
                "70:         aggregate_kwargs=None,",
                "71:     ):",
                "72:         search_filters = []",
                "73:         projects = projects if projects is not None else [self.project]",
                "74:         if search_filter_query is not None:",
                "75:             search_filters = self.build_search_filter(",
                "76:                 search_filter_query, projects, user=user, environments=environments",
                "77:             )",
                "78: ",
                "79:         kwargs = {}",
                "80:         if limit is not None:",
                "81:             kwargs[\"limit\"] = limit",
                "82:         if aggregate_kwargs:",
                "83:             kwargs[\"aggregate_kwargs\"] = {\"trends\": {**aggregate_kwargs}}",
                "84: ",
                "85:         return self.backend.query(",
                "86:             projects,",
                "87:             search_filters=search_filters,",
                "88:             environments=environments,",
                "89:             count_hits=count_hits,",
                "90:             sort_by=sort_by,",
                "91:             date_from=date_from,",
                "92:             date_to=date_to,",
                "93:             cursor=cursor,",
                "94:             **kwargs,",
                "95:         )",
                "96: ",
                "97:     def store_event(self, data, *args, **kwargs):",
                "98:         event = super().store_event(data, *args, **kwargs)",
                "99:         environment_name = data.get(\"environment\")",
                "100:         if environment_name:",
                "101:             GroupEnvironment.objects.filter(",
                "102:                 group_id=event.group_id,",
                "103:                 environment__name=environment_name,",
                "104:                 first_seen__gt=event.datetime,",
                "105:             ).update(first_seen=event.datetime)",
                "106:         return event",
                "107: ",
                "108: ",
                "109: class EventsDatasetTestSetup(SharedSnubaMixin):",
                "110:     @property",
                "111:     def backend(self):",
                "112:         return EventsDatasetSnubaSearchBackend()",
                "113: ",
                "114:     def setUp(self):",
                "115:         super().setUp()",
                "116:         self.base_datetime = before_now(days=3).replace(microsecond=0)",
                "117: ",
                "118:         event1_timestamp = (self.base_datetime - timedelta(days=21)).isoformat()",
                "119:         self.event1 = self.store_event(",
                "120:             data={",
                "121:                 \"fingerprint\": [\"put-me-in-group1\"],",
                "122:                 \"event_id\": \"a\" * 32,",
                "123:                 \"message\": \"foo. Indeed, this message is intended to be greater than 256 characters such that we can put this unique string identifier after that point in the string. The purpose of this is in order to verify we are using snuba to search messages instead of Postgres (postgres truncates at 256 characters and clickhouse does not). santryrox.\",",
                "124:                 \"environment\": \"production\",",
                "125:                 \"tags\": {\"server\": \"example.com\", \"sentry:user\": \"event1@example.com\"},",
                "126:                 \"timestamp\": event1_timestamp,",
                "127:                 \"stacktrace\": {\"frames\": [{\"module\": \"group1\"}]},"
            ]
        },
        {
            "file": "tests/snuba/api/endpoints/test_organization_events_vitals.py",
            "line_number": 27,
            "matched_line": "    def store_event(self, data, measurements=None, **kwargs):",
            "context_start_line": 1,
            "context_end_line": 57,
            "context": [
                "1: from datetime import timedelta",
                "2: ",
                "3: import pytest",
                "4: from django.urls import reverse",
                "5: ",
                "6: from sentry.models.transaction_threshold import ProjectTransactionThreshold, TransactionMetric",
                "7: from sentry.testutils.cases import APITestCase, MetricsEnhancedPerformanceTestCase, SnubaTestCase",
                "8: from sentry.testutils.helpers.datetime import before_now",
                "9: from sentry.utils.samples import load_data",
                "10: ",
                "11: pytestmark = pytest.mark.sentry_metrics",
                "12: ",
                "13: ",
                "14: class OrganizationEventsVitalsEndpointTest(APITestCase, SnubaTestCase):",
                "15:     def setUp(self):",
                "16:         super().setUp()",
                "17:         self.start = before_now(days=1).replace(hour=10, minute=0, second=0, microsecond=0)",
                "18:         self.end = self.start + timedelta(hours=6)",
                "19: ",
                "20:         self.transaction_data = load_data(\"transaction\", timestamp=self.start)",
                "21:         self.query: dict[str, str | list[str]] = {",
                "22:             \"start\": self.start.isoformat(),",
                "23:             \"end\": self.end.isoformat(),",
                "24:         }",
                "25:         self.features = {}",
                "26: ",
                "27:     def store_event(self, data, measurements=None, **kwargs):",
                "28:         if measurements:",
                "29:             for vital, value in measurements.items():",
                "30:                 data[\"measurements\"][vital][\"value\"] = value",
                "31: ",
                "32:         return super().store_event(",
                "33:             data.copy(),",
                "34:             project_id=self.project.id,",
                "35:         )",
                "36: ",
                "37:     def do_request(self, query=None, features=None):",
                "38:         if features is None:",
                "39:             features = {\"organizations:discover-basic\": True}",
                "40:         features.update(self.features)",
                "41:         if query is None:",
                "42:             query = self.query",
                "43: ",
                "44:         self.login_as(user=self.user)",
                "45:         url = reverse(",
                "46:             \"sentry-api-0-organization-events-vitals\",",
                "47:             kwargs={\"organization_id_or_slug\": self.organization.slug},",
                "48:         )",
                "49: ",
                "50:         with self.feature(features):",
                "51:             return self.client.get(url, query, format=\"json\")",
                "52: ",
                "53:     def test_no_projects(self):",
                "54:         response = self.do_request()",
                "55:         assert response.status_code == 200, response.content",
                "56:         assert len(response.data) == 0",
                "57: "
            ]
        },
        {
            "file": "tests/sentry/tasks/test_weekly_reports.py",
            "line_number": 63,
            "matched_line": "    def store_event_outcomes(",
            "context_start_line": 33,
            "context_end_line": 93,
            "context": [
                "33:     group_status_to_color,",
                "34:     prepare_organization_report,",
                "35:     prepare_template_context,",
                "36:     schedule_organizations,",
                "37: )",
                "38: from sentry.testutils.cases import OutcomesSnubaTest, PerformanceIssueTestCase, SnubaTestCase",
                "39: from sentry.testutils.factories import EventType",
                "40: from sentry.testutils.helpers import with_feature",
                "41: from sentry.testutils.helpers.datetime import before_now, freeze_time",
                "42: from sentry.testutils.outbox import outbox_runner",
                "43: from sentry.testutils.silo import assume_test_silo_mode",
                "44: from sentry.types.group import GroupSubStatus",
                "45: from sentry.users.services.user_option import user_option_service",
                "46: from sentry.utils import redis",
                "47: from sentry.utils.dates import floor_to_utc_day",
                "48: from sentry.utils.outcomes import Outcome",
                "49: ",
                "50: DISABLED_ORGANIZATIONS_USER_OPTION_KEY = \"reports:disabled-organizations\"",
                "51: ",
                "52: ",
                "53: class WeeklyReportsTest(OutcomesSnubaTest, SnubaTestCase, PerformanceIssueTestCase):",
                "54:     def setUp(self):",
                "55:         super().setUp()",
                "56:         self.now = timezone.now()",
                "57:         self.timestamp = floor_to_utc_day(self.now).timestamp()",
                "58:         self.two_days_ago = self.now - timedelta(days=2)",
                "59:         self.three_days_ago = self.now - timedelta(days=3)",
                "60: ",
                "61:     _dummy_batch_id = \"20bd6c5b-7fac-4f31-9548-d6f8bb63226d\"",
                "62: ",
                "63:     def store_event_outcomes(",
                "64:         self,",
                "65:         organization_id,",
                "66:         project_id,",
                "67:         timestamp,",
                "68:         num_times,",
                "69:         outcome=Outcome.ACCEPTED,",
                "70:         category=DataCategory.ERROR,",
                "71:     ):",
                "72:         self.store_outcomes(",
                "73:             {",
                "74:                 \"org_id\": organization_id,",
                "75:                 \"project_id\": project_id,",
                "76:                 \"outcome\": outcome,",
                "77:                 \"category\": category,",
                "78:                 \"timestamp\": timestamp,",
                "79:                 \"key_id\": 1,",
                "80:             },",
                "81:             num_times=num_times,",
                "82:         )",
                "83: ",
                "84:     @freeze_time(before_now(days=2).replace(hour=0, minute=0, second=0, microsecond=0))",
                "85:     def test_integration(self):",
                "86:         with unguarded_write(using=router.db_for_write(Project)):",
                "87:             Project.objects.all().delete()",
                "88:         project = self.create_project(",
                "89:             organization=self.organization,",
                "90:             teams=[self.team],",
                "91:             date_added=self.now - timedelta(days=90),",
                "92:         )",
                "93:         member_set = set(project.teams.get().member_set.all())"
            ]
        },
        {
            "file": "tests/sentry/tasks/test_daily_summary.py",
            "line_number": 45,
            "matched_line": "    def store_event_and_outcomes(",
            "context_start_line": 15,
            "context_end_line": 75,
            "context": [
                "15: from sentry.models.group import GroupStatus",
                "16: from sentry.notifications.notifications.daily_summary import DailySummaryNotification",
                "17: from sentry.tasks.summaries.daily_summary import (",
                "18:     build_summary_data,",
                "19:     build_top_projects_map,",
                "20:     deliver_summary,",
                "21:     prepare_summary_data,",
                "22:     schedule_organizations,",
                "23: )",
                "24: from sentry.tasks.summaries.utils import ONE_DAY, DailySummaryProjectContext",
                "25: from sentry.testutils.cases import (",
                "26:     OutcomesSnubaTest,",
                "27:     PerformanceIssueTestCase,",
                "28:     SlackActivityNotificationTest,",
                "29:     SnubaTestCase,",
                "30: )",
                "31: from sentry.testutils.factories import EventType",
                "32: from sentry.testutils.helpers.datetime import before_now, freeze_time",
                "33: from sentry.testutils.helpers.features import with_feature",
                "34: from sentry.types.activity import ActivityType",
                "35: from sentry.types.actor import Actor",
                "36: from sentry.types.group import GroupSubStatus",
                "37: from sentry.users.services.user_option import user_option_service",
                "38: from sentry.utils.outcomes import Outcome",
                "39: ",
                "40: ",
                "41: @freeze_time(before_now(days=2).replace(hour=0, minute=5, second=0, microsecond=0))",
                "42: class DailySummaryTest(",
                "43:     OutcomesSnubaTest, SnubaTestCase, PerformanceIssueTestCase, SlackActivityNotificationTest",
                "44: ):",
                "45:     def store_event_and_outcomes(",
                "46:         self,",
                "47:         project_id,",
                "48:         timestamp,",
                "49:         fingerprint,",
                "50:         category,",
                "51:         release=None,",
                "52:         resolve=True,",
                "53:         level=\"error\",",
                "54:     ):",
                "55:         if category == DataCategory.ERROR:",
                "56:             data = {",
                "57:                 \"timestamp\": timestamp.isoformat(),",
                "58:                 \"fingerprint\": [fingerprint],",
                "59:                 \"level\": level,",
                "60:                 \"exception\": {",
                "61:                     \"values\": [",
                "62:                         {",
                "63:                             \"type\": \"IntegrationError\",",
                "64:                             \"value\": \"Identity not found.\",",
                "65:                         }",
                "66:                     ]",
                "67:                 },",
                "68:             }",
                "69:             if release:",
                "70:                 data[\"release\"] = release",
                "71: ",
                "72:             event = self.store_event(",
                "73:                 data=data,",
                "74:                 project_id=project_id,",
                "75:                 assert_no_errors=False,"
            ]
        },
        {
            "file": "tests/sentry/utils/sdk_crashes/conftest.py",
            "line_number": 12,
            "matched_line": "def store_event(default_project, factories):",
            "context_start_line": 1,
            "context_end_line": 37,
            "context": [
                "1: import pytest",
                "2: ",
                "3: from sentry.utils.sdk_crashes.path_replacer import FixedPathReplacer",
                "4: from sentry.utils.sdk_crashes.sdk_crash_detection_config import (",
                "5:     SDKCrashDetectionConfig,",
                "6:     SDKFrameConfig,",
                "7:     SdkName,",
                "8: )",
                "9: ",
                "10: ",
                "11: @pytest.fixture",
                "12: def store_event(default_project, factories):",
                "13:     def inner(data):",
                "14:         return factories.store_event(data=data, project_id=default_project.id)",
                "15: ",
                "16:     return inner",
                "17: ",
                "18: ",
                "19: @pytest.fixture",
                "20: def empty_cocoa_config() -> SDKCrashDetectionConfig:",
                "21:     return SDKCrashDetectionConfig(",
                "22:         sdk_name=SdkName.Cocoa,",
                "23:         project_id=0,",
                "24:         sample_rate=0.0,",
                "25:         organization_allowlist=[],",
                "26:         sdk_names={},",
                "27:         report_fatal_errors=False,",
                "28:         ignore_mechanism_type=set(),",
                "29:         allow_mechanism_type=set(),",
                "30:         system_library_path_patterns=set(),",
                "31:         sdk_frame_config=SDKFrameConfig(",
                "32:             function_patterns=set(),",
                "33:             path_patterns=set(),",
                "34:             path_replacer=FixedPathReplacer(path=\"\"),",
                "35:         ),",
                "36:         sdk_crash_ignore_matchers=set(),",
                "37:     )"
            ]
        },
        {
            "file": "tests/sentry/utils/sdk_crashes/test_event_stripper.py",
            "line_number": 22,
            "matched_line": "def store_event(default_project, factories):",
            "context_start_line": 1,
            "context_end_line": 52,
            "context": [
                "1: from collections.abc import Sequence",
                "2: ",
                "3: import pytest",
                "4: ",
                "5: from fixtures.sdk_crash_detection.crash_event_cocoa import (",
                "6:     get_crash_event,",
                "7:     get_crash_event_with_frames,",
                "8:     get_frames,",
                "9: )",
                "10: from sentry.testutils.helpers.options import override_options",
                "11: from sentry.testutils.pytest.fixtures import django_db_all",
                "12: from sentry.utils.safe import get_path, set_path",
                "13: from sentry.utils.sdk_crashes.event_stripper import strip_event_data",
                "14: from sentry.utils.sdk_crashes.sdk_crash_detection_config import (",
                "15:     SDKCrashDetectionConfig,",
                "16:     build_sdk_crash_detection_configs,",
                "17: )",
                "18: from sentry.utils.sdk_crashes.sdk_crash_detector import SDKCrashDetector",
                "19: ",
                "20: ",
                "21: @pytest.fixture",
                "22: def store_event(default_project, factories):",
                "23:     def inner(data):",
                "24:         return factories.store_event(data=data, project_id=default_project.id)",
                "25: ",
                "26:     return inner",
                "27: ",
                "28: ",
                "29: @pytest.fixture",
                "30: def configs() -> Sequence[SDKCrashDetectionConfig]:",
                "31:     with override_options(",
                "32:         {",
                "33:             \"issues.sdk_crash_detection.cocoa.project_id\": 1234,",
                "34:             \"issues.sdk_crash_detection.cocoa.sample_rate\": 1.0,",
                "35:             \"issues.sdk_crash_detection.react-native.project_id\": 2,",
                "36:             \"issues.sdk_crash_detection.react-native.sample_rate\": 0.2,",
                "37:             \"issues.sdk_crash_detection.react-native.organization_allowlist\": [1],",
                "38:             \"issues.sdk_crash_detection.java.project_id\": 3,",
                "39:             \"issues.sdk_crash_detection.java.sample_rate\": 0.3,",
                "40:             \"issues.sdk_crash_detection.java.organization_allowlist\": [2],",
                "41:         }",
                "42:     ):",
                "43:         return build_sdk_crash_detection_configs()",
                "44: ",
                "45: ",
                "46: @pytest.fixture",
                "47: def store_and_strip_event(configs, store_event):",
                "48:     def inner(data, config=configs[0]):",
                "49:         event = store_event(data=data)",
                "50:         return strip_event_data(event.data, SDKCrashDetector(config=config))",
                "51: ",
                "52:     return inner"
            ]
        },
        {
            "file": "tests/sentry/notifications/utils/test_participants.py",
            "line_number": 266,
            "matched_line": "    def store_event_owners(self, filename: str) -> Event:",
            "context_start_line": 236,
            "context_end_line": 296,
            "context": [
                "236:         user_2 = self.create_user()",
                "237:         team_2 = self.create_team(self.organization, members=[user_2])",
                "238:         project_2 = self.create_project(organization=self.organization, teams=[team_2])",
                "239: ",
                "240:         self.assert_recipients_are(",
                "241:             self.get_send_to_team(project_2, team_2.id), email=[user_2.id], slack=[user_2.id]",
                "242:         )",
                "243:         assert self.get_send_to_team(self.project, team_2.id) == {}",
                "244: ",
                "245:     def test_other_org_team(self):",
                "246:         org_2 = self.create_organization()",
                "247:         user_2 = self.create_user()",
                "248:         team_2 = self.create_team(org_2, members=[user_2])",
                "249:         project_2 = self.create_project(organization=org_2, teams=[team_2])",
                "250: ",
                "251:         self.assert_recipients_are(",
                "252:             self.get_send_to_team(project_2, team_2.id), email=[user_2.id], slack=[user_2.id]",
                "253:         )",
                "254:         assert self.get_send_to_team(self.project, team_2.id) == {}",
                "255: ",
                "256: ",
                "257: class GetSendToOwnersTest(_ParticipantsTest):",
                "258:     def get_send_to_owners(self, event: Event) -> Mapping[ExternalProviders, set[Actor]]:",
                "259:         return get_send_to(",
                "260:             self.project,",
                "261:             target_type=ActionTargetType.ISSUE_OWNERS,",
                "262:             target_identifier=None,",
                "263:             event=event,",
                "264:         )",
                "265: ",
                "266:     def store_event_owners(self, filename: str) -> Event:",
                "267:         return super().store_event(data=make_event_data(filename), project_id=self.project.id)",
                "268: ",
                "269:     def setUp(self):",
                "270:         self.user2 = self.create_user(email=\"baz@example.com\", is_active=True)",
                "271:         self.user3 = self.create_user(email=\"bar@example.com\", is_active=True)",
                "272:         self.user_suspect_committer = self.create_user(",
                "273:             email=\"suspectcommitter@example.com\", is_active=True",
                "274:         )",
                "275: ",
                "276:         self.team2 = self.create_team(",
                "277:             organization=self.organization, members=[self.user, self.user2]",
                "278:         )",
                "279:         self.team_suspect_committer = self.create_team(",
                "280:             organization=self.organization, members=[self.user_suspect_committer]",
                "281:         )",
                "282:         self.project.add_team(self.team2)",
                "283:         self.project.add_team(self.team_suspect_committer)",
                "284:         self.repo = Repository.objects.create(",
                "285:             organization_id=self.organization.id, name=self.organization.id",
                "286:         )",
                "287: ",
                "288:         user_ids = list(self.project.member_set.values_list(\"user_id\", flat=True))",
                "289:         with assume_test_silo_mode(SiloMode.CONTROL):",
                "290:             users = [Owner(\"user\", user.email) for user in User.objects.filter(id__in=user_ids)]",
                "291:         ProjectOwnership.objects.create(",
                "292:             project_id=self.project.id,",
                "293:             schema=dump_schema(",
                "294:                 [",
                "295:                     grammar.Rule(Matcher(\"path\", \"*.py\"), [Owner(\"team\", self.team2.slug)]),",
                "296:                     grammar.Rule(Matcher(\"path\", \"*.jsx\"), [Owner(\"user\", self.user.email)]),"
            ]
        },
        {
            "file": "tests/sentry/notifications/utils/test_participants.py",
            "line_number": 830,
            "matched_line": "    def store_event(self, filename: str, project: Project) -> Event:",
            "context_start_line": 800,
            "context_end_line": 860,
            "context": [
                "800:                 scope_type=\"team\",",
                "801:                 scope_identifier=self.team.id,",
                "802:                 provider=\"slack\",",
                "803:                 type=\"alerts\",",
                "804:                 value=\"never\",",
                "805:             )",
                "806:             NotificationSettingProvider.objects.create(",
                "807:                 user_id=self.user.id,",
                "808:                 scope_type=\"user\",",
                "809:                 scope_identifier=self.user.id,",
                "810:                 provider=\"slack\",",
                "811:                 type=\"alerts\",",
                "812:                 value=\"never\",",
                "813:             )",
                "814:             NotificationSettingOption.objects.all().delete()",
                "815: ",
                "816:     def get_send_to_fallthrough(",
                "817:         self,",
                "818:         event: Event,",
                "819:         project: Project,",
                "820:         fallthrough_choice: FallthroughChoiceType | None = None,",
                "821:     ) -> Mapping[ExternalProviders, set[Actor]]:",
                "822:         return get_send_to(",
                "823:             project,",
                "824:             target_type=ActionTargetType.ISSUE_OWNERS,",
                "825:             target_identifier=None,",
                "826:             event=event,",
                "827:             fallthrough_choice=fallthrough_choice,",
                "828:         )",
                "829: ",
                "830:     def store_event(self, filename: str, project: Project) -> Event:",
                "831:         return super().store_event(data=make_event_data(filename), project_id=project.id)",
                "832: ",
                "833:     def test_invalid_fallthrough_choice(self):",
                "834:         with pytest.raises(NotImplementedError) as e:",
                "835:             get_fallthrough_recipients(self.project, \"invalid\")  # type: ignore[arg-type]",
                "836:         assert str(e.value).startswith(\"Unknown fallthrough choice: invalid\")",
                "837: ",
                "838:     def test_fallthrough_setting_on(self):",
                "839:         \"\"\"",
                "840:         Test that the new fallthrough choice takes precedence even if the fallthrough setting is on.",
                "841:         \"\"\"",
                "842:         ProjectOwnership.objects.get(project_id=self.project.id).update(fallthrough=True)",
                "843: ",
                "844:         event = self.store_event(\"empty.lol\", self.project)",
                "845:         self.assert_recipients_are(",
                "846:             self.get_send_to_fallthrough(event, self.project, FallthroughChoiceType.ALL_MEMBERS),",
                "847:             email=[self.user.id, self.user2.id],",
                "848:         )",
                "849: ",
                "850:         event = self.store_event(\"empty.lol\", self.project)",
                "851:         assert self.get_send_to_fallthrough(event, self.project, FallthroughChoiceType.NO_ONE) == {}",
                "852: ",
                "853:     def test_no_fallthrough(self):",
                "854:         \"\"\"",
                "855:         Test the new fallthrough choice when no fallthrough choice is provided.\"\"\"",
                "856:         event = self.store_event(\"none.lol\", self.project)",
                "857:         assert self.get_send_to_fallthrough(event, self.project, fallthrough_choice=None) == {}",
                "858: ",
                "859:     def test_no_owners(self):",
                "860:         \"\"\""
            ]
        },
        {
            "file": "src/sentry/testutils/cases.py",
            "line_number": 1058,
            "matched_line": "    def store_event(self, *args, **kwargs):",
            "context_start_line": 1028,
            "context_end_line": 1088,
            "context": [
                "1028: @requires_snuba",
                "1029: class SnubaTestCase(BaseTestCase):",
                "1030:     \"\"\"",
                "1031:     Mixin for enabling test case classes to talk to snuba",
                "1032:     Useful when you are working on acceptance tests or integration",
                "1033:     tests that require snuba.",
                "1034:     \"\"\"",
                "1035: ",
                "1036:     # We need Django to flush all databases.",
                "1037:     databases: set[str] | str = \"__all__\"",
                "1038: ",
                "1039:     def setUp(self):",
                "1040:         super().setUp()",
                "1041:         self.init_snuba()",
                "1042: ",
                "1043:     @pytest.fixture(autouse=True)",
                "1044:     def initialize(self, reset_snuba, call_snuba):",
                "1045:         self.call_snuba = call_snuba",
                "1046: ",
                "1047:     def create_project(self, **kwargs) -> Project:",
                "1048:         if \"flags\" not in kwargs:",
                "1049:             # We insert events directly into snuba in tests, so we need to set has_transactions to True so the",
                "1050:             # application knows that events have been sent",
                "1051:             kwargs[\"flags\"] = Project.flags.has_transactions",
                "1052:         return super().create_project(**kwargs)",
                "1053: ",
                "1054:     def init_snuba(self):",
                "1055:         self.snuba_eventstream = SnubaEventStream()",
                "1056:         self.snuba_tagstore = SnubaTagStorage()",
                "1057: ",
                "1058:     def store_event(self, *args, **kwargs):",
                "1059:         \"\"\"",
                "1060:         Simulates storing an event for testing.",
                "1061: ",
                "1062:         To set event title:",
                "1063:         - use \"message\": \"{title}\" field for errors",
                "1064:         - use \"transaction\": \"{title}\" field for transactions",
                "1065:         More info on event payloads: https://develop.sentry.dev/sdk/event-payloads/",
                "1066:         \"\"\"",
                "1067:         with mock.patch(\"sentry.eventstream.insert\", self.snuba_eventstream.insert):",
                "1068:             stored_event = Factories.store_event(*args, **kwargs)",
                "1069: ",
                "1070:             # Error groups",
                "1071:             stored_group = stored_event.group",
                "1072:             if stored_group is not None:",
                "1073:                 self.store_group(stored_group)",
                "1074: ",
                "1075:             # Performance groups",
                "1076:             stored_groups = stored_event.groups",
                "1077:             if stored_groups is not None:",
                "1078:                 for group in stored_groups:",
                "1079:                     self.store_group(group)",
                "1080:             return stored_event",
                "1081: ",
                "1082:     def wait_for_event_count(self, project_id, total, attempts=2):",
                "1083:         \"\"\"",
                "1084:         Wait until the event count reaches the provided value or until attempts is reached.",
                "1085: ",
                "1086:         Useful when you're storing several events and need to ensure that snuba/clickhouse",
                "1087:         state has settled.",
                "1088:         \"\"\""
            ]
        },
        {
            "file": "src/sentry/testutils/factories.py",
            "line_number": 1021,
            "matched_line": "    def store_event(",
            "context_start_line": 991,
            "context_end_line": 1051,
            "context": [
                "991: ",
                "992:     @staticmethod",
                "993:     def inject_performance_problems(jobs, _):",
                "994:         for job in jobs:",
                "995:             job[\"performance_problems\"] = []",
                "996:             for f in job[\"data\"][\"fingerprint\"]:",
                "997:                 f_data = f.split(\"-\", 1)",
                "998:                 if len(f_data) < 2:",
                "999:                     raise ValueError(",
                "1000:                         \"Invalid performance fingerprint data. Format must be 'group_type-fingerprint'.\"",
                "1001:                     )",
                "1002:                 group_type = get_group_type_by_type_id(int(f_data[0]))",
                "1003:                 perf_fingerprint = f_data[1]",
                "1004: ",
                "1005:                 job[\"performance_problems\"].append(",
                "1006:                     PerformanceProblem(",
                "1007:                         fingerprint=perf_fingerprint,",
                "1008:                         op=\"db\",",
                "1009:                         desc=\"\",",
                "1010:                         type=group_type,",
                "1011:                         parent_span_ids=None,",
                "1012:                         cause_span_ids=None,",
                "1013:                         offender_span_ids=[],",
                "1014:                         evidence_data={},",
                "1015:                         evidence_display=[],",
                "1016:                     )",
                "1017:                 )",
                "1018: ",
                "1019:     @staticmethod",
                "1020:     @assume_test_silo_mode(SiloMode.REGION)",
                "1021:     def store_event(",
                "1022:         data,",
                "1023:         project_id: int,",
                "1024:         assert_no_errors: bool = True,",
                "1025:         default_event_type: EventType | None = None,",
                "1026:         sent_at: datetime | None = None,",
                "1027:     ) -> Event:",
                "1028:         \"\"\"",
                "1029:         Like `create_event`, but closer to how events are actually",
                "1030:         ingested. Prefer to use this method over `create_event`",
                "1031:         \"\"\"",
                "1032: ",
                "1033:         # this creates a basic message event",
                "1034:         if default_event_type == EventType.DEFAULT:",
                "1035:             data.update({\"stacktrace\": copy.deepcopy(DEFAULT_EVENT_DATA[\"stacktrace\"])})",
                "1036: ",
                "1037:         # this creates an error event",
                "1038:         elif default_event_type == EventType.ERROR:",
                "1039:             data.update({\"exception\": [{\"value\": \"BadError\"}]})",
                "1040: ",
                "1041:         manager = EventManager(data, sent_at=sent_at)",
                "1042:         manager.normalize()",
                "1043:         if assert_no_errors:",
                "1044:             errors = manager.get_data().get(\"errors\")",
                "1045:             assert not errors, errors",
                "1046: ",
                "1047:         normalized_data = manager.get_data()",
                "1048: ",
                "1049:         _set_sample_rate_from_error_sampling(normalized_data)",
                "1050: ",
                "1051:         event = None"
            ]
        }
    ],
    "total": 10
}

Message 9 [Assistant]:
--------------------------------------------------------------------------------
Content:
让我在factories.py文件中搜索`store_event`函数：
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "store_event",
      "is_regex": false,
      "case_sensitive": true,
      "context_lines": 5,
      "max_results": 20
    },
    "id": "call_00_fQuSXxeXulGtjWAEEJnrJazZ",
    "type": "tool_call"
  }
]

Message 10 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_fQuSXxeXulGtjWAEEJnrJazZ
Result:
{
    "matches": [
        {
            "file": "tests/integration/test_api.py",
            "line_number": 27,
            "matched_line": "        event = self.store_event(data={}, project_id=self.project.id)",
            "context_start_line": 22,
            "context_end_line": 32,
            "context": [
                "22: ",
                "23:         member = self.create_member(user=self.user, organization=self.organization, teams=[team])",
                "24: ",
                "25:         member.flags[\"sso:linked\"] = True",
                "26:         member.save()",
                "27:         event = self.store_event(data={}, project_id=self.project.id)",
                "28:         group_id = event.group_id",
                "29:         with assume_test_silo_mode(SiloMode.CONTROL):",
                "30:             auth_provider = AuthProvider.objects.create(",
                "31:                 organization_id=self.organization.id, provider=\"dummy\", flags=0",
                "32:             )"
            ]
        },
        {
            "file": "tests/relay_integration/lang/java/test_plugin.py",
            "line_number": 606,
            "matched_line": "        event = self.store_event(",
            "context_start_line": 601,
            "context_end_line": 611,
            "context": [
                "601:     def test_sets_inapp_after_resolving(self):",
                "602:         self.upload_proguard_mapping(PROGUARD_UUID, PROGUARD_SOURCE)",
                "603: ",
                "604:         version = \"org.slf4j@1.2.3\"",
                "605:         env_name = \"some_env\"",
                "606:         event = self.store_event(",
                "607:             data={\"release\": version, \"environment\": env_name}, project_id=self.project.id",
                "608:         )",
                "609: ",
                "610:         event_data = {",
                "611:             \"user\": {\"ip_address\": \"31.172.207.97\"},"
            ]
        },
        {
            "file": "tests/acceptance/test_performance_landing.py",
            "line_number": 38,
            "matched_line": "        self.store_event(data=event, project_id=self.project.id)",
            "context_start_line": 33,
            "context_end_line": 43,
            "context": [
                "33:     @patch(\"django.utils.timezone.now\")",
                "34:     def test_with_data(self, mock_now):",
                "35:         mock_now.return_value = before_now()",
                "36: ",
                "37:         event = load_data(\"transaction\", timestamp=before_now(minutes=10))",
                "38:         self.store_event(data=event, project_id=self.project.id)",
                "39:         self.project.update(flags=F(\"flags\").bitor(Project.flags.has_transactions))",
                "40: ",
                "41:         with self.feature(FEATURE_NAMES):",
                "42:             self.browser.get(self.path)",
                "43:             self.page.wait_until_loaded()"
            ]
        },
        {
            "file": "tests/acceptance/test_performance_landing.py",
            "line_number": 56,
            "matched_line": "        self.store_event(data=event, project_id=self.project.id)",
            "context_start_line": 51,
            "context_end_line": 61,
            "context": [
                "51:     @patch(\"django.utils.timezone.now\")",
                "52:     def test_with_data_and_new_widget_designs(self, mock_now):",
                "53:         mock_now.return_value = before_now()",
                "54: ",
                "55:         event = load_data(\"transaction\", timestamp=before_now(minutes=10))",
                "56:         self.store_event(data=event, project_id=self.project.id)",
                "57:         self.project.update(flags=F(\"flags\").bitor(Project.flags.has_transactions))",
                "58: ",
                "59:         FEATURES = (",
                "60:             \"organizations:discover-basic\",",
                "61:             \"organizations:performance-view\","
            ]
        },
        {
            "file": "tests/acceptance/test_issue_tag_values.py",
            "line_number": 29,
            "matched_line": "        return self.store_event(data=event_data, project_id=self.project.id)",
            "context_start_line": 24,
            "context_end_line": 34,
            "context": [
                "24: ",
                "25:     def create_issue(self):",
                "26:         event_data = load_data(\"javascript\")",
                "27:         event_data[\"timestamp\"] = before_now(minutes=1).isoformat()",
                "28:         event_data[\"tags\"] = {\"url\": \"http://example.org/path?key=value\"}",
                "29:         return self.store_event(data=event_data, project_id=self.project.id)",
                "30: ",
                "31:     def test_user_tag(self):",
                "32:         self.page.visit_tag_values(self.org.slug, self.event.group_id, \"user\")",
                "33:         assert self.browser.element_exists_by_test_id(\"group-tag-mail\")",
                "34: "
            ]
        },
        {
            "file": "tests/acceptance/test_project_release_tracking_settings.py",
            "line_number": 19,
            "matched_line": "        self.store_event(",
            "context_start_line": 14,
            "context_end_line": 24,
            "context": [
                "14: ",
                "15:         self.login_as(self.user)",
                "16:         self.path1 = f\"/{self.org.slug}/{self.project.slug}/settings/release-tracking/\"",
                "17: ",
                "18:     def test_tags_list(self):",
                "19:         self.store_event(",
                "20:             data={",
                "21:                 \"event_id\": \"a\" * 32,",
                "22:                 \"message\": \"oh no\",",
                "23:                 \"environment\": \"prod\",",
                "24:                 \"release\": \"first\","
            ]
        },
        {
            "file": "tests/acceptance/test_issue_details_workflow.py",
            "line_number": 29,
            "matched_line": "        event = self.store_event(",
            "context_start_line": 24,
            "context_end_line": 34,
            "context": [
                "24:         self.dismiss_assistant()",
                "25: ",
                "26:     def create_sample_event(self, platform, default=None, sample_name=None):",
                "27:         event_data = load_data(platform, default=default, sample_name=sample_name)",
                "28:         event_data[\"event_id\"] = \"d964fdbd649a4cf8bfc35d18082b6b0e\"",
                "29:         event = self.store_event(",
                "30:             data=event_data, project_id=self.project.id, assert_no_errors=False",
                "31:         )",
                "32:         event.group.update(",
                "33:             first_seen=datetime(2015, 8, 13, 3, 8, 25, tzinfo=timezone.utc),",
                "34:             last_seen=datetime(2016, 1, 13, 3, 8, 25, tzinfo=timezone.utc),"
            ]
        },
        {
            "file": "tests/acceptance/test_organization_events_v2.py",
            "line_number": 194,
            "matched_line": "        self.store_event(",
            "context_start_line": 189,
            "context_end_line": 199,
            "context": [
                "189:     def test_all_events_query(self, mock_now):",
                "190:         now = before_now()",
                "191:         mock_now.return_value = now",
                "192:         five_mins_ago = (now - timedelta(minutes=5)).isoformat()",
                "193:         ten_mins_ago = (now - timedelta(minutes=10)).isoformat()",
                "194:         self.store_event(",
                "195:             data={",
                "196:                 \"event_id\": \"a\" * 32,",
                "197:                 \"message\": \"oh no\",",
                "198:                 \"timestamp\": five_mins_ago,",
                "199:                 \"fingerprint\": [\"group-1\"],"
            ]
        },
        {
            "file": "tests/acceptance/test_organization_events_v2.py",
            "line_number": 204,
            "matched_line": "        self.store_event(",
            "context_start_line": 199,
            "context_end_line": 209,
            "context": [
                "199:                 \"fingerprint\": [\"group-1\"],",
                "200:             },",
                "201:             project_id=self.project.id,",
                "202:             assert_no_errors=False,",
                "203:         )",
                "204:         self.store_event(",
                "205:             data={",
                "206:                 \"event_id\": \"b\" * 32,",
                "207:                 \"message\": \"this is bad.\",",
                "208:                 \"timestamp\": ten_mins_ago,",
                "209:                 \"fingerprint\": [\"group-2\"],"
            ]
        },
        {
            "file": "tests/acceptance/test_organization_events_v2.py",
            "line_number": 248,
            "matched_line": "        self.store_event(",
            "context_start_line": 243,
            "context_end_line": 253,
            "context": [
                "243:     @patch(\"django.utils.timezone.now\")",
                "244:     def test_errors_query(self, mock_now):",
                "245:         now = before_now()",
                "246:         mock_now.return_value = now",
                "247:         ten_mins_ago = (now - timedelta(minutes=10)).isoformat()",
                "248:         self.store_event(",
                "249:             data={",
                "250:                 \"event_id\": \"a\" * 32,",
                "251:                 \"message\": \"oh no\",",
                "252:                 \"timestamp\": ten_mins_ago,",
                "253:                 \"fingerprint\": [\"group-1\"],"
            ]
        },
        {
            "file": "tests/acceptance/test_organization_events_v2.py",
            "line_number": 259,
            "matched_line": "        self.store_event(",
            "context_start_line": 254,
            "context_end_line": 264,
            "context": [
                "254:                 \"type\": \"error\",",
                "255:             },",
                "256:             project_id=self.project.id,",
                "257:             assert_no_errors=False,",
                "258:         )",
                "259:         self.store_event(",
                "260:             data={",
                "261:                 \"event_id\": \"b\" * 32,",
                "262:                 \"message\": \"oh no\",",
                "263:                 \"timestamp\": ten_mins_ago,",
                "264:                 \"fingerprint\": [\"group-1\"],"
            ]
        },
        {
            "file": "tests/acceptance/test_organization_events_v2.py",
            "line_number": 270,
            "matched_line": "        self.store_event(",
            "context_start_line": 265,
            "context_end_line": 275,
            "context": [
                "265:                 \"type\": \"error\",",
                "266:             },",
                "267:             project_id=self.project.id,",
                "268:             assert_no_errors=False,",
                "269:         )",
                "270:         self.store_event(",
                "271:             data={",
                "272:                 \"event_id\": \"c\" * 32,",
                "273:                 \"message\": \"this is bad.\",",
                "274:                 \"timestamp\": ten_mins_ago,",
                "275:                 \"fingerprint\": [\"group-2\"],"
            ]
        },
        {
            "file": "tests/acceptance/test_organization_events_v2.py",
            "line_number": 302,
            "matched_line": "        self.store_event(data=event_data, project_id=self.project.id, assert_no_errors=True)",
            "context_start_line": 297,
            "context_end_line": 307,
            "context": [
                "297:     def test_transactions_query(self, mock_now):",
                "298:         mock_now.return_value = before_now()",
                "299: ",
                "300:         event_data = generate_transaction()",
                "301: ",
                "302:         self.store_event(data=event_data, project_id=self.project.id, assert_no_errors=True)",
                "303: ",
                "304:         with self.feature(FEATURE_NAMES):",
                "305:             self.browser.get(self.result_path + \"?\" + transactions_query())",
                "306:             self.wait_until_loaded()",
                "307:             self.browser.wait_until_not("
            ]
        },
        {
            "file": "tests/acceptance/test_organization_events_v2.py",
            "line_number": 333,
            "matched_line": "        self.store_event(data=event_data, project_id=self.project.id, assert_no_errors=False)",
            "context_start_line": 328,
            "context_end_line": 338,
            "context": [
                "328:         event_data[\"contexts\"][\"trace\"] = {",
                "329:             \"type\": \"trace\",",
                "330:             \"trace_id\": \"a\" * 32,",
                "331:             \"span_id\": \"b\" * 16,",
                "332:         }",
                "333:         self.store_event(data=event_data, project_id=self.project.id, assert_no_errors=False)",
                "334: ",
                "335:         with self.feature(FEATURE_NAMES):",
                "336:             # Get the list page.",
                "337:             self.browser.get(self.result_path + \"?\" + all_events_query())",
                "338:             self.wait_until_loaded()"
            ]
        },
        {
            "file": "tests/acceptance/test_organization_events_v2.py",
            "line_number": 365,
            "matched_line": "        self.store_event(data=event_data, project_id=self.project.id)",
            "context_start_line": 360,
            "context_end_line": 370,
            "context": [
                "360:         event_data[\"contexts\"][\"trace\"] = {",
                "361:             \"type\": \"trace\",",
                "362:             \"trace_id\": \"a\" * 32,",
                "363:             \"span_id\": \"b\" * 16,",
                "364:         }",
                "365:         self.store_event(data=event_data, project_id=self.project.id)",
                "366:         self.wait_for_event_count(self.project.id, 1)",
                "367: ",
                "368:         with self.feature(FEATURE_NAMES):",
                "369:             # Get the list page",
                "370:             self.browser.get(self.result_path + \"?\" + errors_query() + \"&statsPeriod=24h\")"
            ]
        },
        {
            "file": "tests/acceptance/test_organization_events_v2.py",
            "line_number": 386,
            "matched_line": "        self.store_event(data=event_data, project_id=self.project.id, assert_no_errors=True)",
            "context_start_line": 381,
            "context_end_line": 391,
            "context": [
                "381:     @patch(\"django.utils.timezone.now\")",
                "382:     def test_event_detail_view_from_transactions_query(self, mock_now):",
                "383:         mock_now.return_value = before_now()",
                "384: ",
                "385:         event_data = generate_transaction(trace=\"a\" * 32, span=\"ab\" * 8)",
                "386:         self.store_event(data=event_data, project_id=self.project.id, assert_no_errors=True)",
                "387: ",
                "388:         # Create a child event that is linked to the parent so we have coverage",
                "389:         # of traversal buttons.",
                "390:         child_event = generate_transaction(",
                "391:             trace=event_data[\"contexts\"][\"trace\"][\"trace_id\"], span=\"bc\" * 8"
            ]
        },
        {
            "file": "tests/acceptance/test_organization_events_v2.py",
            "line_number": 397,
            "matched_line": "        self.store_event(data=child_event, project_id=self.project.id, assert_no_errors=True)",
            "context_start_line": 392,
            "context_end_line": 402,
            "context": [
                "392:         )",
                "393:         child_event[\"event_id\"] = \"b\" * 32",
                "394:         child_event[\"contexts\"][\"trace\"][\"parent_span_id\"] = event_data[\"spans\"][4][\"span_id\"]",
                "395:         child_event[\"transaction\"] = \"z-child-transaction\"",
                "396:         child_event[\"spans\"] = child_event[\"spans\"][0:3]",
                "397:         self.store_event(data=child_event, project_id=self.project.id, assert_no_errors=True)",
                "398: ",
                "399:         with self.feature(FEATURE_NAMES):",
                "400:             # Get the list page",
                "401:             self.browser.get(self.result_path + \"?\" + transactions_sorted_query())",
                "402:             self.wait_until_loaded()"
            ]
        },
        {
            "file": "tests/acceptance/test_organization_events_v2.py",
            "line_number": 459,
            "matched_line": "        self.store_event(data=event_data, project_id=self.project.id, assert_no_errors=True)",
            "context_start_line": 454,
            "context_end_line": 464,
            "context": [
                "454:             clone[\"op\"] = \"http\"",
                "455:             clone[\"description\"] = \"test\"",
                "456:             clone[\"span_id\"] = (str(\"bd\" * 6) + str(i)).ljust(16, \"0\")",
                "457:             event_data[\"spans\"].append(clone)",
                "458: ",
                "459:         self.store_event(data=event_data, project_id=self.project.id, assert_no_errors=True)",
                "460: ",
                "461:         # Create a child event that is linked to the parent so we have coverage",
                "462:         # of traversal buttons.",
                "463:         child_event = generate_transaction(",
                "464:             trace=event_data[\"contexts\"][\"trace\"][\"trace_id\"], span=\"bc\" * 8"
            ]
        },
        {
            "file": "tests/acceptance/test_organization_events_v2.py",
            "line_number": 470,
            "matched_line": "        self.store_event(data=child_event, project_id=self.project.id, assert_no_errors=True)",
            "context_start_line": 465,
            "context_end_line": 475,
            "context": [
                "465:         )",
                "466:         child_event[\"event_id\"] = \"b\" * 32",
                "467:         child_event[\"contexts\"][\"trace\"][\"parent_span_id\"] = event_data[\"spans\"][4][\"span_id\"]",
                "468:         child_event[\"transaction\"] = \"z-child-transaction\"",
                "469:         child_event[\"spans\"] = child_event[\"spans\"][0:3]",
                "470:         self.store_event(data=child_event, project_id=self.project.id, assert_no_errors=True)",
                "471: ",
                "472:         with self.feature(FEATURE_NAMES):",
                "473:             # Get the list page",
                "474:             self.browser.get(self.result_path + \"?\" + transactions_sorted_query())",
                "475:             self.wait_until_loaded()"
            ]
        },
        {
            "file": "tests/acceptance/test_organization_events_v2.py",
            "line_number": 508,
            "matched_line": "        self.store_event(data=event_data, project_id=self.project.id, assert_no_errors=True)",
            "context_start_line": 503,
            "context_end_line": 513,
            "context": [
                "503:     @patch(\"django.utils.timezone.now\")",
                "504:     def test_transaction_event_detail_view_ops_filtering(self, mock_now):",
                "505:         mock_now.return_value = before_now()",
                "506: ",
                "507:         event_data = generate_transaction(trace=\"a\" * 32, span=\"ab\" * 8)",
                "508:         self.store_event(data=event_data, project_id=self.project.id, assert_no_errors=True)",
                "509: ",
                "510:         with self.feature(FEATURE_NAMES):",
                "511:             # Get the list page",
                "512:             self.browser.get(self.result_path + \"?\" + transactions_query())",
                "513:             self.wait_until_loaded()"
            ]
        }
    ],
    "total": 20
}

Message 11 [Assistant]:
--------------------------------------------------------------------------------
Content:
让我查看factories.py文件中`store_event`方法的实现：
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "def store_event",
      "is_regex": false,
      "case_sensitive": true,
      "context_lines": 50,
      "max_results": 5,
      "include_patterns": [
        "*factories.py"
      ]
    },
    "id": "call_00_7IlXnv0I1WqH0TFNYlSpydWt",
    "type": "tool_call"
  }
]

Message 12 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_7IlXnv0I1WqH0TFNYlSpydWt
Result:
{
    "matches": [
        {
            "file": "src/sentry/testutils/factories.py",
            "line_number": 1021,
            "matched_line": "    def store_event(",
            "context_start_line": 971,
            "context_end_line": 1071,
            "context": [
                "971:     @staticmethod",
                "972:     @assume_test_silo_mode(SiloMode.CONTROL)",
                "973:     def create_user_role(*args, **kwargs):",
                "974:         return UserRole.objects.create(*args, **kwargs)",
                "975: ",
                "976:     @staticmethod",
                "977:     @assume_test_silo_mode(SiloMode.CONTROL)",
                "978:     def create_usersocialauth(",
                "979:         user: User,",
                "980:         provider: str | None = None,",
                "981:         uid: str | None = None,",
                "982:         extra_data: dict[str, Any] | None = None,",
                "983:     ):",
                "984:         if not provider:",
                "985:             provider = \"asana\"",
                "986:         if not uid:",
                "987:             uid = \"abc-123\"",
                "988:         usa = UserSocialAuth(user=user, provider=provider, uid=uid, extra_data=extra_data)",
                "989:         usa.save()",
                "990:         return usa",
                "991: ",
                "992:     @staticmethod",
                "993:     def inject_performance_problems(jobs, _):",
                "994:         for job in jobs:",
                "995:             job[\"performance_problems\"] = []",
                "996:             for f in job[\"data\"][\"fingerprint\"]:",
                "997:                 f_data = f.split(\"-\", 1)",
                "998:                 if len(f_data) < 2:",
                "999:                     raise ValueError(",
                "1000:                         \"Invalid performance fingerprint data. Format must be 'group_type-fingerprint'.\"",
                "1001:                     )",
                "1002:                 group_type = get_group_type_by_type_id(int(f_data[0]))",
                "1003:                 perf_fingerprint = f_data[1]",
                "1004: ",
                "1005:                 job[\"performance_problems\"].append(",
                "1006:                     PerformanceProblem(",
                "1007:                         fingerprint=perf_fingerprint,",
                "1008:                         op=\"db\",",
                "1009:                         desc=\"\",",
                "1010:                         type=group_type,",
                "1011:                         parent_span_ids=None,",
                "1012:                         cause_span_ids=None,",
                "1013:                         offender_span_ids=[],",
                "1014:                         evidence_data={},",
                "1015:                         evidence_display=[],",
                "1016:                     )",
                "1017:                 )",
                "1018: ",
                "1019:     @staticmethod",
                "1020:     @assume_test_silo_mode(SiloMode.REGION)",
                "1021:     def store_event(",
                "1022:         data,",
                "1023:         project_id: int,",
                "1024:         assert_no_errors: bool = True,",
                "1025:         default_event_type: EventType | None = None,",
                "1026:         sent_at: datetime | None = None,",
                "1027:     ) -> Event:",
                "1028:         \"\"\"",
                "1029:         Like `create_event`, but closer to how events are actually",
                "1030:         ingested. Prefer to use this method over `create_event`",
                "1031:         \"\"\"",
                "1032: ",
                "1033:         # this creates a basic message event",
                "1034:         if default_event_type == EventType.DEFAULT:",
                "1035:             data.update({\"stacktrace\": copy.deepcopy(DEFAULT_EVENT_DATA[\"stacktrace\"])})",
                "1036: ",
                "1037:         # this creates an error event",
                "1038:         elif default_event_type == EventType.ERROR:",
                "1039:             data.update({\"exception\": [{\"value\": \"BadError\"}]})",
                "1040: ",
                "1041:         manager = EventManager(data, sent_at=sent_at)",
                "1042:         manager.normalize()",
                "1043:         if assert_no_errors:",
                "1044:             errors = manager.get_data().get(\"errors\")",
                "1045:             assert not errors, errors",
                "1046: ",
                "1047:         normalized_data = manager.get_data()",
                "1048: ",
                "1049:         _set_sample_rate_from_error_sampling(normalized_data)",
                "1050: ",
                "1051:         event = None",
                "1052: ",
                "1053:         # When fingerprint is present on transaction, inject performance problems",
                "1054:         if (",
                "1055:             normalized_data.get(\"type\") == \"transaction\"",
                "1056:             and normalized_data.get(\"fingerprint\") is not None",
                "1057:         ):",
                "1058:             with mock.patch(",
                "1059:                 \"sentry.event_manager._detect_performance_problems\",",
                "1060:                 Factories.inject_performance_problems,",
                "1061:             ):",
                "1062:                 event = manager.save(project_id)",
                "1063: ",
                "1064:         else:",
                "1065:             event = manager.save(project_id)",
                "1066: ",
                "1067:         if event.groups:",
                "1068:             for group in event.groups:",
                "1069:                 group.save()",
                "1070: ",
                "1071:         if event.group:"
            ]
        }
    ],
    "total": 1
}

Message 13 [Assistant]:
--------------------------------------------------------------------------------
Content:
现在让我搜索关于`error_sampling`和`client_sample_rate`的更多信息，以了解这个上下文的业务意图：
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "error_sampling",
      "is_regex": false,
      "case_sensitive": true,
      "context_lines": 10,
      "max_results": 20
    },
    "id": "call_00_Lxl3aXMTS81IAVbK2vdbcNvN",
    "type": "tool_call"
  }
]

Message 14 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_Lxl3aXMTS81IAVbK2vdbcNvN
Result:
{
    "matches": [
        {
            "file": "tests/snuba/api/endpoints/test_organization_events_stats.py",
            "line_number": 3569,
            "matched_line": "        # Store some error events with error_sampling context",
            "context_start_line": 3559,
            "context_end_line": 3579,
            "context": [
                "3559:         self.login_as(user=self.user)",
                "3560:         self.authed_user = self.user",
                "3561: ",
                "3562:         self.day_ago = before_now(days=1).replace(hour=10, minute=0, second=0, microsecond=0)",
                "3563: ",
                "3564:         self.project = self.create_project()",
                "3565:         self.project2 = self.create_project()",
                "3566:         self.user = self.create_user()",
                "3567:         self.user2 = self.create_user()",
                "3568: ",
                "3569:         # Store some error events with error_sampling context",
                "3570:         self.store_event(",
                "3571:             data={",
                "3572:                 \"event_id\": \"a\" * 32,",
                "3573:                 \"message\": \"very bad\",",
                "3574:                 \"type\": \"error\",",
                "3575:                 \"exception\": [{\"type\": \"ValueError\", \"value\": \"Something went wrong 1\"}],",
                "3576:                 \"timestamp\": (self.day_ago + timedelta(minutes=1)).isoformat(),",
                "3577:                 \"fingerprint\": [\"group1\"],",
                "3578:                 \"tags\": {\"sentry:user\": self.user.email},",
                "3579:                 \"contexts\": {\"error_sampling\": {\"client_sample_rate\": 0.1}},"
            ]
        },
        {
            "file": "tests/snuba/api/endpoints/test_organization_events_stats.py",
            "line_number": 3579,
            "matched_line": "                \"contexts\": {\"error_sampling\": {\"client_sample_rate\": 0.1}},",
            "context_start_line": 3569,
            "context_end_line": 3589,
            "context": [
                "3569:         # Store some error events with error_sampling context",
                "3570:         self.store_event(",
                "3571:             data={",
                "3572:                 \"event_id\": \"a\" * 32,",
                "3573:                 \"message\": \"very bad\",",
                "3574:                 \"type\": \"error\",",
                "3575:                 \"exception\": [{\"type\": \"ValueError\", \"value\": \"Something went wrong 1\"}],",
                "3576:                 \"timestamp\": (self.day_ago + timedelta(minutes=1)).isoformat(),",
                "3577:                 \"fingerprint\": [\"group1\"],",
                "3578:                 \"tags\": {\"sentry:user\": self.user.email},",
                "3579:                 \"contexts\": {\"error_sampling\": {\"client_sample_rate\": 0.1}},",
                "3580:             },",
                "3581:             project_id=self.project.id,",
                "3582:         )",
                "3583:         self.store_event(",
                "3584:             data={",
                "3585:                 \"event_id\": \"b\" * 32,",
                "3586:                 \"message\": \"oh my\",",
                "3587:                 \"type\": \"error\",",
                "3588:                 \"exception\": [{\"type\": \"ValueError\", \"value\": \"Something went wrong 2\"}],",
                "3589:                 \"timestamp\": (self.day_ago + timedelta(hours=1, minutes=1)).isoformat(),"
            ]
        },
        {
            "file": "tests/snuba/api/endpoints/test_organization_events_stats.py",
            "line_number": 3592,
            "matched_line": "                \"contexts\": {\"error_sampling\": {\"client_sample_rate\": 0.1}},",
            "context_start_line": 3582,
            "context_end_line": 3602,
            "context": [
                "3582:         )",
                "3583:         self.store_event(",
                "3584:             data={",
                "3585:                 \"event_id\": \"b\" * 32,",
                "3586:                 \"message\": \"oh my\",",
                "3587:                 \"type\": \"error\",",
                "3588:                 \"exception\": [{\"type\": \"ValueError\", \"value\": \"Something went wrong 2\"}],",
                "3589:                 \"timestamp\": (self.day_ago + timedelta(hours=1, minutes=1)).isoformat(),",
                "3590:                 \"fingerprint\": [\"group2\"],",
                "3591:                 \"tags\": {\"sentry:user\": self.user2.email},",
                "3592:                 \"contexts\": {\"error_sampling\": {\"client_sample_rate\": 0.1}},",
                "3593:             },",
                "3594:             project_id=self.project2.id,",
                "3595:         )",
                "3596:         self.wait_for_event_count(self.project.id, 1)",
                "3597:         self.wait_for_event_count(self.project2.id, 1)",
                "3598: ",
                "3599:         self.url = reverse(",
                "3600:             \"sentry-api-0-organization-events-stats\",",
                "3601:             kwargs={\"organization_id_or_slug\": self.project.organization.slug},",
                "3602:         )"
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2879,
            "matched_line": "    def test_derive_client_error_sampling_rate_with_option_enabled(self) -> None:",
            "context_start_line": 2869,
            "context_end_line": 2889,
            "context": [
                "2869:     def test_new_group_metrics_logging_without_frame_mix(self) -> None:",
                "2870:         with patch(\"sentry.event_manager.metrics.incr\") as mock_metrics_incr:",
                "2871:             manager = EventManager(make_event(platform=\"javascript\"))",
                "2872:             event = manager.save(self.project.id)",
                "2873: ",
                "2874:             assert event.get_event_metadata().get(\"in_app_frame_mix\") is None",
                "2875: ",
                "2876:             metrics_logged = [call.args[0] for call in mock_metrics_incr.mock_calls]",
                "2877:             assert \"grouping.in_app_frame_mix\" not in metrics_logged",
                "2878: ",
                "2879:     def test_derive_client_error_sampling_rate_with_option_enabled(self) -> None:",
                "2880:         \"\"\"Test that sample_rate is extracted from contexts when option is enabled.\"\"\"",
                "2881:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2882:             event_data = make_event(",
                "2883:                 contexts={\"error_sampling\": {\"client_sample_rate\": 0.1}}, platform=\"python\"",
                "2884:             )",
                "2885: ",
                "2886:             manager = EventManager(event_data)",
                "2887:             manager.normalize()",
                "2888:             event = manager.save(self.project.id)",
                "2889: "
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2881,
            "matched_line": "        with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
            "context_start_line": 2871,
            "context_end_line": 2891,
            "context": [
                "2871:             manager = EventManager(make_event(platform=\"javascript\"))",
                "2872:             event = manager.save(self.project.id)",
                "2873: ",
                "2874:             assert event.get_event_metadata().get(\"in_app_frame_mix\") is None",
                "2875: ",
                "2876:             metrics_logged = [call.args[0] for call in mock_metrics_incr.mock_calls]",
                "2877:             assert \"grouping.in_app_frame_mix\" not in metrics_logged",
                "2878: ",
                "2879:     def test_derive_client_error_sampling_rate_with_option_enabled(self) -> None:",
                "2880:         \"\"\"Test that sample_rate is extracted from contexts when option is enabled.\"\"\"",
                "2881:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2882:             event_data = make_event(",
                "2883:                 contexts={\"error_sampling\": {\"client_sample_rate\": 0.1}}, platform=\"python\"",
                "2884:             )",
                "2885: ",
                "2886:             manager = EventManager(event_data)",
                "2887:             manager.normalize()",
                "2888:             event = manager.save(self.project.id)",
                "2889: ",
                "2890:             # Check that sample_rate was extracted and stored",
                "2891:             assert event.data[\"sample_rate\"] == 0.1"
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2883,
            "matched_line": "                contexts={\"error_sampling\": {\"client_sample_rate\": 0.1}}, platform=\"python\"",
            "context_start_line": 2873,
            "context_end_line": 2893,
            "context": [
                "2873: ",
                "2874:             assert event.get_event_metadata().get(\"in_app_frame_mix\") is None",
                "2875: ",
                "2876:             metrics_logged = [call.args[0] for call in mock_metrics_incr.mock_calls]",
                "2877:             assert \"grouping.in_app_frame_mix\" not in metrics_logged",
                "2878: ",
                "2879:     def test_derive_client_error_sampling_rate_with_option_enabled(self) -> None:",
                "2880:         \"\"\"Test that sample_rate is extracted from contexts when option is enabled.\"\"\"",
                "2881:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2882:             event_data = make_event(",
                "2883:                 contexts={\"error_sampling\": {\"client_sample_rate\": 0.1}}, platform=\"python\"",
                "2884:             )",
                "2885: ",
                "2886:             manager = EventManager(event_data)",
                "2887:             manager.normalize()",
                "2888:             event = manager.save(self.project.id)",
                "2889: ",
                "2890:             # Check that sample_rate was extracted and stored",
                "2891:             assert event.data[\"sample_rate\"] == 0.1",
                "2892: ",
                "2893:     def test_derive_client_error_sampling_rate_with_option_disabled(self) -> None:"
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2893,
            "matched_line": "    def test_derive_client_error_sampling_rate_with_option_disabled(self) -> None:",
            "context_start_line": 2883,
            "context_end_line": 2903,
            "context": [
                "2883:                 contexts={\"error_sampling\": {\"client_sample_rate\": 0.1}}, platform=\"python\"",
                "2884:             )",
                "2885: ",
                "2886:             manager = EventManager(event_data)",
                "2887:             manager.normalize()",
                "2888:             event = manager.save(self.project.id)",
                "2889: ",
                "2890:             # Check that sample_rate was extracted and stored",
                "2891:             assert event.data[\"sample_rate\"] == 0.1",
                "2892: ",
                "2893:     def test_derive_client_error_sampling_rate_with_option_disabled(self) -> None:",
                "2894:         \"\"\"Test that sample_rate is not extracted when option is disabled.\"\"\"",
                "2895:         # Option disabled (project not in allowlist)",
                "2896:         event_data = make_event(",
                "2897:             contexts={\"error_sampling\": {\"client_sample_rate\": 0.1}}, platform=\"python\"",
                "2898:         )",
                "2899: ",
                "2900:         manager = EventManager(event_data)",
                "2901:         manager.normalize()",
                "2902:         event = manager.save(self.project.id)",
                "2903: "
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2897,
            "matched_line": "            contexts={\"error_sampling\": {\"client_sample_rate\": 0.1}}, platform=\"python\"",
            "context_start_line": 2887,
            "context_end_line": 2907,
            "context": [
                "2887:             manager.normalize()",
                "2888:             event = manager.save(self.project.id)",
                "2889: ",
                "2890:             # Check that sample_rate was extracted and stored",
                "2891:             assert event.data[\"sample_rate\"] == 0.1",
                "2892: ",
                "2893:     def test_derive_client_error_sampling_rate_with_option_disabled(self) -> None:",
                "2894:         \"\"\"Test that sample_rate is not extracted when option is disabled.\"\"\"",
                "2895:         # Option disabled (project not in allowlist)",
                "2896:         event_data = make_event(",
                "2897:             contexts={\"error_sampling\": {\"client_sample_rate\": 0.1}}, platform=\"python\"",
                "2898:         )",
                "2899: ",
                "2900:         manager = EventManager(event_data)",
                "2901:         manager.normalize()",
                "2902:         event = manager.save(self.project.id)",
                "2903: ",
                "2904:         # Check that sample_rate was not extracted",
                "2905:         assert \"sample_rate\" not in event.data",
                "2906: ",
                "2907:     def test_derive_client_error_sampling_rate_malformed_context(self) -> None:"
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2907,
            "matched_line": "    def test_derive_client_error_sampling_rate_malformed_context(self) -> None:",
            "context_start_line": 2897,
            "context_end_line": 2917,
            "context": [
                "2897:             contexts={\"error_sampling\": {\"client_sample_rate\": 0.1}}, platform=\"python\"",
                "2898:         )",
                "2899: ",
                "2900:         manager = EventManager(event_data)",
                "2901:         manager.normalize()",
                "2902:         event = manager.save(self.project.id)",
                "2903: ",
                "2904:         # Check that sample_rate was not extracted",
                "2905:         assert \"sample_rate\" not in event.data",
                "2906: ",
                "2907:     def test_derive_client_error_sampling_rate_malformed_context(self) -> None:",
                "2908:         \"\"\"Test that sample_rate extraction handles malformed error_sampling context gracefully.\"\"\"",
                "2909:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2910:             # Test with error_sampling as a number instead of a dict",
                "2911:             event_data = make_event(",
                "2912:                 contexts={\"error_sampling\": 0.1},",
                "2913:                 platform=\"python\",  # Should be a dict, not a number",
                "2914:             )",
                "2915: ",
                "2916:             manager = EventManager(event_data)",
                "2917:             manager.normalize()"
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2908,
            "matched_line": "        \"\"\"Test that sample_rate extraction handles malformed error_sampling context gracefully.\"\"\"",
            "context_start_line": 2898,
            "context_end_line": 2918,
            "context": [
                "2898:         )",
                "2899: ",
                "2900:         manager = EventManager(event_data)",
                "2901:         manager.normalize()",
                "2902:         event = manager.save(self.project.id)",
                "2903: ",
                "2904:         # Check that sample_rate was not extracted",
                "2905:         assert \"sample_rate\" not in event.data",
                "2906: ",
                "2907:     def test_derive_client_error_sampling_rate_malformed_context(self) -> None:",
                "2908:         \"\"\"Test that sample_rate extraction handles malformed error_sampling context gracefully.\"\"\"",
                "2909:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2910:             # Test with error_sampling as a number instead of a dict",
                "2911:             event_data = make_event(",
                "2912:                 contexts={\"error_sampling\": 0.1},",
                "2913:                 platform=\"python\",  # Should be a dict, not a number",
                "2914:             )",
                "2915: ",
                "2916:             manager = EventManager(event_data)",
                "2917:             manager.normalize()",
                "2918:             event = manager.save(self.project.id)"
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2909,
            "matched_line": "        with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
            "context_start_line": 2899,
            "context_end_line": 2919,
            "context": [
                "2899: ",
                "2900:         manager = EventManager(event_data)",
                "2901:         manager.normalize()",
                "2902:         event = manager.save(self.project.id)",
                "2903: ",
                "2904:         # Check that sample_rate was not extracted",
                "2905:         assert \"sample_rate\" not in event.data",
                "2906: ",
                "2907:     def test_derive_client_error_sampling_rate_malformed_context(self) -> None:",
                "2908:         \"\"\"Test that sample_rate extraction handles malformed error_sampling context gracefully.\"\"\"",
                "2909:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2910:             # Test with error_sampling as a number instead of a dict",
                "2911:             event_data = make_event(",
                "2912:                 contexts={\"error_sampling\": 0.1},",
                "2913:                 platform=\"python\",  # Should be a dict, not a number",
                "2914:             )",
                "2915: ",
                "2916:             manager = EventManager(event_data)",
                "2917:             manager.normalize()",
                "2918:             event = manager.save(self.project.id)",
                "2919: "
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2910,
            "matched_line": "            # Test with error_sampling as a number instead of a dict",
            "context_start_line": 2900,
            "context_end_line": 2920,
            "context": [
                "2900:         manager = EventManager(event_data)",
                "2901:         manager.normalize()",
                "2902:         event = manager.save(self.project.id)",
                "2903: ",
                "2904:         # Check that sample_rate was not extracted",
                "2905:         assert \"sample_rate\" not in event.data",
                "2906: ",
                "2907:     def test_derive_client_error_sampling_rate_malformed_context(self) -> None:",
                "2908:         \"\"\"Test that sample_rate extraction handles malformed error_sampling context gracefully.\"\"\"",
                "2909:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2910:             # Test with error_sampling as a number instead of a dict",
                "2911:             event_data = make_event(",
                "2912:                 contexts={\"error_sampling\": 0.1},",
                "2913:                 platform=\"python\",  # Should be a dict, not a number",
                "2914:             )",
                "2915: ",
                "2916:             manager = EventManager(event_data)",
                "2917:             manager.normalize()",
                "2918:             event = manager.save(self.project.id)",
                "2919: ",
                "2920:             # Check that no sample_rate was added due to malformed context"
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2912,
            "matched_line": "                contexts={\"error_sampling\": 0.1},",
            "context_start_line": 2902,
            "context_end_line": 2922,
            "context": [
                "2902:         event = manager.save(self.project.id)",
                "2903: ",
                "2904:         # Check that sample_rate was not extracted",
                "2905:         assert \"sample_rate\" not in event.data",
                "2906: ",
                "2907:     def test_derive_client_error_sampling_rate_malformed_context(self) -> None:",
                "2908:         \"\"\"Test that sample_rate extraction handles malformed error_sampling context gracefully.\"\"\"",
                "2909:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2910:             # Test with error_sampling as a number instead of a dict",
                "2911:             event_data = make_event(",
                "2912:                 contexts={\"error_sampling\": 0.1},",
                "2913:                 platform=\"python\",  # Should be a dict, not a number",
                "2914:             )",
                "2915: ",
                "2916:             manager = EventManager(event_data)",
                "2917:             manager.normalize()",
                "2918:             event = manager.save(self.project.id)",
                "2919: ",
                "2920:             # Check that no sample_rate was added due to malformed context",
                "2921:             assert \"sample_rate\" not in event.data",
                "2922: "
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2923,
            "matched_line": "    def test_derive_client_error_sampling_rate_invalid_range(self) -> None:",
            "context_start_line": 2913,
            "context_end_line": 2933,
            "context": [
                "2913:                 platform=\"python\",  # Should be a dict, not a number",
                "2914:             )",
                "2915: ",
                "2916:             manager = EventManager(event_data)",
                "2917:             manager.normalize()",
                "2918:             event = manager.save(self.project.id)",
                "2919: ",
                "2920:             # Check that no sample_rate was added due to malformed context",
                "2921:             assert \"sample_rate\" not in event.data",
                "2922: ",
                "2923:     def test_derive_client_error_sampling_rate_invalid_range(self) -> None:",
                "2924:         \"\"\"Test that sample_rate is not set when client_sample_rate is outside valid range (0-1).\"\"\"",
                "2925:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2926:             # Test with sample rate > 1",
                "2927:             event_data = make_event(",
                "2928:                 contexts={\"error_sampling\": {\"client_sample_rate\": 1.5}}, platform=\"python\"",
                "2929:             )",
                "2930: ",
                "2931:             manager = EventManager(event_data)",
                "2932:             manager.normalize()",
                "2933:             event = manager.save(self.project.id)"
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2925,
            "matched_line": "        with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
            "context_start_line": 2915,
            "context_end_line": 2935,
            "context": [
                "2915: ",
                "2916:             manager = EventManager(event_data)",
                "2917:             manager.normalize()",
                "2918:             event = manager.save(self.project.id)",
                "2919: ",
                "2920:             # Check that no sample_rate was added due to malformed context",
                "2921:             assert \"sample_rate\" not in event.data",
                "2922: ",
                "2923:     def test_derive_client_error_sampling_rate_invalid_range(self) -> None:",
                "2924:         \"\"\"Test that sample_rate is not set when client_sample_rate is outside valid range (0-1).\"\"\"",
                "2925:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2926:             # Test with sample rate > 1",
                "2927:             event_data = make_event(",
                "2928:                 contexts={\"error_sampling\": {\"client_sample_rate\": 1.5}}, platform=\"python\"",
                "2929:             )",
                "2930: ",
                "2931:             manager = EventManager(event_data)",
                "2932:             manager.normalize()",
                "2933:             event = manager.save(self.project.id)",
                "2934: ",
                "2935:             # Check that sample_rate was not set due to invalid range"
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2928,
            "matched_line": "                contexts={\"error_sampling\": {\"client_sample_rate\": 1.5}}, platform=\"python\"",
            "context_start_line": 2918,
            "context_end_line": 2938,
            "context": [
                "2918:             event = manager.save(self.project.id)",
                "2919: ",
                "2920:             # Check that no sample_rate was added due to malformed context",
                "2921:             assert \"sample_rate\" not in event.data",
                "2922: ",
                "2923:     def test_derive_client_error_sampling_rate_invalid_range(self) -> None:",
                "2924:         \"\"\"Test that sample_rate is not set when client_sample_rate is outside valid range (0-1).\"\"\"",
                "2925:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2926:             # Test with sample rate > 1",
                "2927:             event_data = make_event(",
                "2928:                 contexts={\"error_sampling\": {\"client_sample_rate\": 1.5}}, platform=\"python\"",
                "2929:             )",
                "2930: ",
                "2931:             manager = EventManager(event_data)",
                "2932:             manager.normalize()",
                "2933:             event = manager.save(self.project.id)",
                "2934: ",
                "2935:             # Check that sample_rate was not set due to invalid range",
                "2936:             assert \"sample_rate\" not in event.data",
                "2937: ",
                "2938:             # Test with negative sample rate"
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2940,
            "matched_line": "                contexts={\"error_sampling\": {\"client_sample_rate\": -0.1}}, platform=\"python\"",
            "context_start_line": 2930,
            "context_end_line": 2950,
            "context": [
                "2930: ",
                "2931:             manager = EventManager(event_data)",
                "2932:             manager.normalize()",
                "2933:             event = manager.save(self.project.id)",
                "2934: ",
                "2935:             # Check that sample_rate was not set due to invalid range",
                "2936:             assert \"sample_rate\" not in event.data",
                "2937: ",
                "2938:             # Test with negative sample rate",
                "2939:             event_data = make_event(",
                "2940:                 contexts={\"error_sampling\": {\"client_sample_rate\": -0.1}}, platform=\"python\"",
                "2941:             )",
                "2942: ",
                "2943:             manager = EventManager(event_data)",
                "2944:             manager.normalize()",
                "2945:             event = manager.save(self.project.id)",
                "2946: ",
                "2947:             # Check that sample_rate was not set due to invalid range",
                "2948:             assert \"sample_rate\" not in event.data",
                "2949: ",
                "2950:     def test_times_seen_new_group_default_behavior(self) -> None:"
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2993,
            "matched_line": "        with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
            "context_start_line": 2983,
            "context_end_line": 3003,
            "context": [
                "2983:         # Should be the same group",
                "2984:         assert event2.group_id == event1.group_id",
                "2985: ",
                "2986:         # Refresh group from database to get updated times_seen",
                "2987:         group.refresh_from_db()",
                "2988:         assert group.times_seen == initial_times_seen + 1",
                "2989: ",
                "2990:     def test_times_seen_weighted_with_sample_rate_option_enabled(self) -> None:",
                "2991:         \"\"\"Test that times_seen is weighted by 1/sample_rate when the project is in the allowlist\"\"\"",
                "2992: ",
                "2993:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2994:             # Create event with a sample rate of 0.5 (50%)",
                "2995:             event_data = make_event(",
                "2996:                 message=\"sampled event\", contexts={\"error_sampling\": {\"client_sample_rate\": 0.5}}",
                "2997:             )",
                "2998: ",
                "2999:             manager = EventManager(event_data)",
                "3000:             manager.normalize()",
                "3001: ",
                "3002:             with self.tasks():",
                "3003:                 event = manager.save(self.project.id)"
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2996,
            "matched_line": "                message=\"sampled event\", contexts={\"error_sampling\": {\"client_sample_rate\": 0.5}}",
            "context_start_line": 2986,
            "context_end_line": 3006,
            "context": [
                "2986:         # Refresh group from database to get updated times_seen",
                "2987:         group.refresh_from_db()",
                "2988:         assert group.times_seen == initial_times_seen + 1",
                "2989: ",
                "2990:     def test_times_seen_weighted_with_sample_rate_option_enabled(self) -> None:",
                "2991:         \"\"\"Test that times_seen is weighted by 1/sample_rate when the project is in the allowlist\"\"\"",
                "2992: ",
                "2993:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2994:             # Create event with a sample rate of 0.5 (50%)",
                "2995:             event_data = make_event(",
                "2996:                 message=\"sampled event\", contexts={\"error_sampling\": {\"client_sample_rate\": 0.5}}",
                "2997:             )",
                "2998: ",
                "2999:             manager = EventManager(event_data)",
                "3000:             manager.normalize()",
                "3001: ",
                "3002:             with self.tasks():",
                "3003:                 event = manager.save(self.project.id)",
                "3004: ",
                "3005:             group = event.group",
                "3006:             assert group is not None"
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 3015,
            "matched_line": "            message=\"sampled event\", contexts={\"error_sampling\": {\"client_sample_rate\": 0.5}}",
            "context_start_line": 3005,
            "context_end_line": 3025,
            "context": [
                "3005:             group = event.group",
                "3006:             assert group is not None",
                "3007:             # With sample rate 0.5, times_seen should be 1/0.5 = 2",
                "3008:             assert group.times_seen == 2",
                "3009: ",
                "3010:     def test_times_seen_weighted_with_sample_rate_option_disabled(self) -> None:",
                "3011:         \"\"\"Test that times_seen is not weighted when the project is not in the allowlist\"\"\"",
                "3012: ",
                "3013:         # Create event with a sample rate of 0.5 (50%) but project not in allowlist",
                "3014:         event_data = make_event(",
                "3015:             message=\"sampled event\", contexts={\"error_sampling\": {\"client_sample_rate\": 0.5}}",
                "3016:         )",
                "3017: ",
                "3018:         manager = EventManager(event_data)",
                "3019:         manager.normalize()",
                "3020: ",
                "3021:         with self.tasks():",
                "3022:             event = manager.save(self.project.id)",
                "3023: ",
                "3024:         group = event.group",
                "3025:         assert group is not None"
            ]
        }
    ],
    "total": 20
}

Message 15 [Assistant]:
--------------------------------------------------------------------------------
Content:
让我查看测试文件以了解`error_sampling`的预期行为：
Tool Calls:
[
  {
    "name": "read_file",
    "args": {
      "file_path": "tests/sentry/event_manager/test_event_manager.py",
      "encoding": "utf-8"
    },
    "id": "call_00_HaepZZIUOs8fouCJmHpAUjEn",
    "type": "tool_call"
  }
]

Message 16 [Tool]:
--------------------------------------------------------------------------------
Tool: read_file
Tool Call ID: call_00_HaepZZIUOs8fouCJmHpAUjEn
Result:
{
    "content": "from __future__ import annotations\n\nimport logging\nimport uuid\nfrom collections.abc import Mapping\nfrom datetime import UTC, datetime, timedelta\nfrom time import time\nfrom typing import Any\nfrom unittest import mock\nfrom unittest.mock import MagicMock, patch\n\nimport pytest\nfrom arroyo.backends.kafka.consumer import KafkaPayload\nfrom arroyo.backends.local.backend import LocalBroker\nfrom arroyo.backends.local.storages.memory import MemoryMessageStorage\nfrom arroyo.types import Partition, Topic\nfrom django.conf import settings\nfrom django.core.cache import cache\nfrom django.db.models import F\nfrom django.utils import timezone\n\nfrom sentry import eventstore, nodestore, tsdb\nfrom sentry.attachments import CachedAttachment, attachment_cache\nfrom sentry.constants import MAX_VERSION_LENGTH, DataCategory, InsightModules\nfrom sentry.dynamic_sampling import (\n    ExtendedBoostedRelease,\n    Platform,\n    ProjectBoostedReleases,\n    get_redis_client_for_ds,\n)\nfrom sentry.event_manager import (\n    EventManager,\n    _get_event_instance,\n    get_event_type,\n    has_pending_commit_resolution,\n    materialize_metadata,\n    save_grouphash_and_group,\n)\nfrom sentry.eventstore.models import Event\nfrom sentry.exceptions import HashDiscarded\nfrom sentry.grouping.api import GroupingConfig, load_grouping_config\nfrom sentry.grouping.grouptype import ErrorGroupType\nfrom sentry.grouping.utils import hash_from_values\nfrom sentry.ingest.inbound_filters import FilterStatKeys\nfrom sentry.ingest.transaction_clusterer import ClustererNamespace\nfrom sentry.integrations.models.external_issue import ExternalIssue\nfrom sentry.issues.grouptype import (\n    GroupCategory,\n    PerformanceNPlusOneGroupType,\n    PerformanceSlowDBQueryGroupType,\n)\nfrom sentry.issues.issue_occurrence import IssueEvidence\nfrom sentry.models.activity import Activity\nfrom sentry.models.commit import Commit\nfrom sentry.models.environment import Environment\nfrom sentry.models.group import Group, GroupStatus\nfrom sentry.models.groupenvironment import GroupEnvironment\nfrom sentry.models.grouphash import GroupHash\nfrom sentry.models.grouplink import GroupLink\nfrom sentry.models.groupopenperiod import GroupOpenPeriod\nfrom sentry.models.grouprelease import GroupRelease\nfrom sentry.models.groupresolution import GroupResolution\nfrom sentry.models.grouptombstone import GroupTombstone\nfrom sentry.models.project import Project\nfrom sentry.models.pullrequest import PullRequest, PullRequestCommit\nfrom sentry.models.release import Release\nfrom sentry.models.releasecommit import ReleaseCommit\nfrom sentry.models.releaseprojectenvironment import ReleaseProjectEnvironment\nfrom sentry.projectoptions.defaults import DEFAULT_GROUPING_CONFIG\nfrom sentry.signals import (\n    first_event_with_minified_stack_trace_received,\n    first_insight_span_received,\n    first_transaction_received,\n)\nfrom sentry.spans.grouping.utils import hash_values\nfrom sentry.testutils.asserts import assert_mock_called_once_with_partial\nfrom sentry.testutils.cases import (\n    PerformanceIssueTestCase,\n    SnubaTestCase,\n    TestCase,\n    TransactionTestCase,\n)\nfrom sentry.testutils.helpers import apply_feature_flag_on_cls, override_options\nfrom sentry.testutils.helpers.datetime import before_now, freeze_time\nfrom sentry.testutils.helpers.features import with_feature\nfrom sentry.testutils.helpers.usage_accountant import usage_accountant_backend\nfrom sentry.testutils.performance_issues.event_generators import get_event\nfrom sentry.testutils.pytest.fixtures import django_db_all\nfrom sentry.testutils.skips import requires_snuba\nfrom sentry.tsdb.base import TSDBModel\nfrom sentry.types.activity import ActivityType\nfrom sentry.types.group import PriorityLevel\nfrom sentry.utils import json\nfrom sentry.utils.cache import cache_key_for_event\nfrom sentry.utils.eventuser import EventUser\nfrom sentry.utils.outcomes import Outcome\nfrom sentry.utils.samples import load_data\n\npytestmark = [requires_snuba]\n\n\ndef make_event(**kwargs: Any) -> dict[str, Any]:\n    result = {\n        \"event_id\": uuid.uuid1().hex,\n        \"level\": logging.ERROR,\n        \"logger\": \"default\",\n        \"tags\": [],\n    }\n    result.update(kwargs)\n    return result\n\n\nclass EventManagerTestMixin:\n    def make_release_event(self, release_name: str, project_id: int) -> Event:\n        manager = EventManager(make_event(release=release_name))\n        manager.normalize()\n        event = manager.save(project_id)\n        return event\n\n\nclass EventManagerTest(TestCase, SnubaTestCase, EventManagerTestMixin, PerformanceIssueTestCase):\n    def test_ephemeral_interfaces_removed_on_save(self) -> None:\n        manager = EventManager(make_event(platform=\"python\"))\n        manager.normalize()\n        event = manager.save(self.project.id)\n\n        group = event.group\n        assert group is not None\n        assert group.platform == \"python\"\n        assert event.platform == \"python\"\n\n    @mock.patch(\"sentry.event_manager.eventstream.backend.insert\")\n    def test_dupe_message_id(self, eventstream_insert: mock.MagicMock) -> None:\n        # Saves the latest event to nodestore and eventstream\n        project_id = self.project.id\n        event_id = \"a\" * 32\n        node_id = Event.generate_node_id(project_id, event_id)\n\n        manager = EventManager(make_event(event_id=event_id, message=\"first\"))\n        manager.normalize()\n        manager.save(project_id)\n        assert nodestore.backend.get(node_id)[\"logentry\"][\"formatted\"] == \"first\"\n\n        manager = EventManager(make_event(event_id=event_id, message=\"second\"))\n        manager.normalize()\n        manager.save(project_id)\n        assert nodestore.backend.get(node_id)[\"logentry\"][\"formatted\"] == \"second\"\n\n        assert eventstream_insert.call_count == 2\n\n    def test_materialze_metadata_simple(self) -> None:\n        manager = EventManager(make_event(transaction=\"/dogs/are/great/\"))\n        event = manager.save(self.project.id)\n\n        event_type = get_event_type(event.data)\n        event_metadata = event_type.get_metadata(event.data)\n\n        assert materialize_metadata(event.data, event_type, event_metadata) == {\n            \"type\": \"default\",\n            \"culprit\": \"/dogs/are/great/\",\n            \"metadata\": {\"title\": \"<unlabeled event>\"},\n            \"title\": \"<unlabeled event>\",\n            \"location\": None,\n        }\n\n    def test_materialze_metadata_preserves_existing_metadata(self) -> None:\n        manager = EventManager(make_event())\n        event = manager.save(self.project.id)\n\n        event.data.setdefault(\"metadata\", {})\n        event.data[\"metadata\"][\"dogs\"] = \"are great\"  # should not get clobbered\n\n        event_type = get_event_type(event.data)\n        event_metadata_from_type = event_type.get_metadata(event.data)\n        materialized = materialize_metadata(event.data, event_type, event_metadata_from_type)\n\n        assert materialized[\"metadata\"] == {\"title\": \"<unlabeled event>\", \"dogs\": \"are great\"}\n\n    def test_react_error_picks_cause_error_title_subtitle(self) -> None:\n        cause_error_value = \"Load failed\"\n        # React 19 hydration error include the hydration error and a cause\n        # If we derive the title from the cause error the developer will more easily distinguish them\n        manager = EventManager(\n            make_event(\n                exception={\n                    \"values\": [\n                        {\n                            \"type\": \"TypeError\",\n                            \"value\": cause_error_value,\n                            \"mechanism\": {\n                                \"type\": \"onerror\",\n                                \"handled\": False,\n                                \"source\": \"cause\",\n                                \"exception_id\": 1,\n                                \"parent_id\": 0,\n                            },\n                        },\n                        {\n                            \"type\": \"Error\",\n                            \"value\": \"There was an error during concurrent rendering but React was able to recover by instead synchronously rendering the entire root.\",\n                            \"mechanism\": {\n                                \"type\": \"generic\",\n                                \"handled\": True,\n                                \"exception_id\": 0,\n                            },\n                        },\n                    ]\n                },\n            )\n        )\n        event = manager.save(self.project.id)\n        assert event.data[\"metadata\"][\"value\"] == cause_error_value\n        assert event.data[\"metadata\"][\"type\"] == \"TypeError\"\n        assert event.group is not None\n        assert event.group.title == f\"TypeError: {cause_error_value}\"\n\n    def test_react_hydration_error_picks_cause_error_title_subtitle(self) -> None:\n        cause_error_value = \"Cannot read properties of undefined (reading 'nodeName')\"\n        # React 19 hydration error include the hydration error and a cause\n        # If we derive the title from the cause error the developer will more easily distinguish them\n        manager = EventManager(\n            make_event(\n                exception={\n                    \"values\": [\n                        {\n                            \"type\": \"TypeError\",\n                            \"value\": cause_error_value,\n                            \"mechanism\": {\n                                \"type\": \"chained\",\n                                \"source\": \"cause\",\n                                \"exception_id\": 1,\n                                \"parent_id\": 0,\n                            },\n                        },\n                        {\n                            \"type\": \"Error\",\n                            \"value\": \"There was an error while hydrating but React was able to recover by instead client rendering from the nearest Suspense boundary.\",\n                            \"mechanism\": {\n                                \"type\": \"generic\",\n                                \"exception_id\": 0,\n                            },\n                        },\n                    ]\n                },\n            )\n        )\n        event = manager.save(self.project.id)\n        assert event.data[\"metadata\"][\"value\"] == cause_error_value\n        assert event.data[\"metadata\"][\"type\"] == \"TypeError\"\n        assert event.group is not None\n        assert event.group.title == f\"TypeError: {cause_error_value}\"\n\n    @mock.patch(\"sentry.signals.issue_unresolved.send_robust\")\n    @with_feature(\"organizations:issue-open-periods\")\n    def test_unresolve_auto_resolved_group(self, send_robust: mock.MagicMock) -> None:\n        ts = before_now(minutes=5).isoformat()\n\n        # N.B. EventManager won't unresolve the group unless the event2 has a\n        # later timestamp than event1.\n        manager = EventManager(make_event(event_id=\"a\" * 32, checksum=\"a\" * 32, timestamp=ts))\n        with self.tasks():\n            event = manager.save(self.project.id)\n\n        group = Group.objects.get(id=event.group_id)\n        group.status = GroupStatus.RESOLVED\n        group.substatus = None\n        group.save()\n        assert group.is_resolved()\n\n        resolved_at = before_now(minutes=4)\n        activity = Activity.objects.create(\n            group=group,\n            project=group.project,\n            type=ActivityType.SET_RESOLVED_BY_AGE.value,\n            datetime=resolved_at,\n        )\n\n        GroupOpenPeriod.objects.get(group=group, date_ended__isnull=True).close_open_period(\n            resolution_time=resolved_at,\n            resolution_activity=activity,\n        )\n\n        manager = EventManager(\n            make_event(\n                event_id=\"b\" * 32, checksum=\"a\" * 32, timestamp=before_now(minutes=3).isoformat()\n            )\n        )\n        event2 = manager.save(self.project.id)\n        assert event.group_id == event2.group_id\n\n        group = Group.objects.get(id=group.id)\n        assert not group.is_resolved()\n        assert send_robust.called\n\n        open_periods = GroupOpenPeriod.objects.filter(group=group).order_by(\"-date_started\")\n        assert len(open_periods) == 2\n        open_period = open_periods[0]\n        assert open_period.date_started == event2.datetime\n        assert open_period.date_ended is None\n        open_period = open_periods[1]\n        assert open_period.date_started == group.first_seen\n        assert open_period.date_ended == resolved_at\n\n    @mock.patch(\"sentry.signals.issue_unresolved.send_robust\")\n    @with_feature(\"organizations:issue-open-periods\")\n    def test_unresolves_group(self, send_robust: mock.MagicMock) -> None:\n        ts = before_now(minutes=5).isoformat()\n\n        # N.B. EventManager won't unresolve the group unless the event2 has a\n        # later timestamp than event1.\n        manager = EventManager(make_event(event_id=\"a\" * 32, checksum=\"a\" * 32, timestamp=ts))\n        with self.tasks():\n            event = manager.save(self.project.id)\n\n        group = Group.objects.get(id=event.group_id)\n        group.status = GroupStatus.RESOLVED\n        group.substatus = None\n        group.save()\n        assert group.is_resolved()\n\n        resolved_at = before_now(minutes=4)\n        activity = Activity.objects.create(\n            group=group,\n            project=group.project,\n            type=ActivityType.SET_RESOLVED.value,\n            datetime=resolved_at,\n        )\n        GroupOpenPeriod.objects.get(group=group, date_ended__isnull=True).close_open_period(\n            resolution_time=resolved_at,\n            resolution_activity=activity,\n        )\n\n        manager = EventManager(\n            make_event(\n                event_id=\"b\" * 32, checksum=\"a\" * 32, timestamp=before_now(minutes=3).isoformat()\n            )\n        )\n        event2 = manager.save(self.project.id)\n        assert event.group_id == event2.group_id\n\n        group = Group.objects.get(id=group.id)\n        assert not group.is_resolved()\n        assert send_robust.called\n\n        open_periods = GroupOpenPeriod.objects.filter(group=group).order_by(\"-date_started\")\n        assert len(open_periods) == 2\n        open_period = open_periods[0]\n        assert open_period.date_started == event2.datetime\n        assert open_period.date_ended is None\n        open_period = open_periods[1]\n        assert open_period.date_started == group.first_seen\n        assert open_period.date_ended == resolved_at\n\n    @mock.patch(\"sentry.signals.issue_unresolved.send_robust\")\n    @with_feature(\"organizations:issue-open-periods\")\n    def test_unresolves_group_without_open_period(self, send_robust: mock.MagicMock) -> None:\n        ts = before_now(minutes=5).isoformat()\n\n        # N.B. EventManager won't unresolve the group unless the event2 has a\n        # later timestamp than event1.\n        manager = EventManager(make_event(event_id=\"a\" * 32, checksum=\"a\" * 32, timestamp=ts))\n        with self.tasks():\n            event = manager.save(self.project.id)\n\n        group = Group.objects.get(id=event.group_id)\n        group.status = GroupStatus.RESOLVED\n        group.substatus = None\n        group.save()\n        assert group.is_resolved()\n\n        GroupOpenPeriod.objects.all().delete()\n        manager = EventManager(\n            make_event(\n                event_id=\"b\" * 32, checksum=\"a\" * 32, timestamp=before_now(minutes=3).isoformat()\n            )\n        )\n        event2 = manager.save(self.project.id)\n        assert event.group_id == event2.group_id\n\n        group = Group.objects.get(id=group.id)\n        assert not group.is_resolved()\n        assert send_robust.called\n\n        assert GroupOpenPeriod.objects.filter(group=group).count() == 0\n\n    @mock.patch(\"sentry.event_manager.plugin_is_regression\")\n    def test_does_not_unresolve_group(self, plugin_is_regression: mock.MagicMock) -> None:\n        # N.B. EventManager won't unresolve the group unless the event2 has a\n        # later timestamp than event1.\n        plugin_is_regression.return_value = False\n\n        manager = EventManager(\n            make_event(event_id=\"a\" * 32, checksum=\"a\" * 32, timestamp=1403007314)\n        )\n        with self.tasks():\n            manager.normalize()\n            event = manager.save(self.project.id)\n\n        group = Group.objects.get(id=event.group_id)\n        group.status = GroupStatus.RESOLVED\n        group.substatus = None\n        group.save()\n        assert group.is_resolved()\n\n        manager = EventManager(\n            make_event(event_id=\"b\" * 32, checksum=\"a\" * 32, timestamp=1403007315)\n        )\n        manager.normalize()\n        event2 = manager.save(self.project.id)\n        assert event.group_id == event2.group_id\n\n        group = Group.objects.get(id=group.id)\n        assert group.is_resolved()\n\n    @mock.patch(\"sentry.tasks.activity.send_activity_notifications.delay\")\n    @mock.patch(\"sentry.event_manager.plugin_is_regression\")\n    def test_marks_as_unresolved_with_new_release(\n        self,\n        plugin_is_regression: mock.MagicMock,\n        mock_send_activity_notifications_delay: mock.MagicMock,\n    ) -> None:\n        plugin_is_regression.return_value = True\n\n        old_release = Release.objects.create(\n            version=\"a\",\n            organization_id=self.project.organization_id,\n            date_added=timezone.now() - timedelta(minutes=30),\n        )\n        old_release.add_project(self.project)\n\n        manager = EventManager(\n            make_event(\n                event_id=\"a\" * 32,\n                checksum=\"a\" * 32,\n                timestamp=time() - 50000,  # need to work around active_at\n                release=old_release.version,\n            )\n        )\n        event = manager.save(self.project.id)\n\n        assert event.group is not None\n        group = event.group\n\n        group.update(status=GroupStatus.RESOLVED, substatus=None)\n\n        resolution = GroupResolution.objects.create(release=old_release, group=group)\n        activity = Activity.objects.create(\n            group=group,\n            project=group.project,\n            type=ActivityType.SET_RESOLVED_IN_RELEASE.value,\n            ident=resolution.id,\n            data={\"version\": \"\"},\n        )\n\n        manager = EventManager(\n            make_event(\n                event_id=\"b\" * 32, checksum=\"a\" * 32, timestamp=time(), release=old_release.version\n            )\n        )\n        event = manager.save(self.project.id)\n        assert event.group_id == group.id\n\n        group = Group.objects.get(id=group.id)\n        assert group.status == GroupStatus.RESOLVED\n\n        activity = Activity.objects.get(id=activity.id)\n        assert activity.data[\"version\"] == \"\"\n\n        assert GroupResolution.objects.filter(group=group).exists()\n\n        manager = EventManager(\n            make_event(event_id=\"c\" * 32, checksum=\"a\" * 32, timestamp=time(), release=\"b\")\n        )\n        event = manager.save(self.project.id)\n        assert event.group_id == group.id\n\n        group = Group.objects.get(id=group.id)\n        assert group.status == GroupStatus.UNRESOLVED\n\n        activity = Activity.objects.get(id=activity.id)\n        assert activity.data[\"version\"] == \"b\"\n\n        assert not GroupResolution.objects.filter(group=group).exists()\n\n        activity = Activity.objects.get(group=group, type=ActivityType.SET_REGRESSION.value)\n\n        mock_send_activity_notifications_delay.assert_called_once_with(activity.id)\n\n    @mock.patch(\"sentry.tasks.activity.send_activity_notifications.delay\")\n    @mock.patch(\"sentry.event_manager.plugin_is_regression\")\n    def test_that_release_in_latest_activity_prior_to_regression_is_not_overridden(\n        self,\n        plugin_is_regression: mock.MagicMock,\n        mock_send_activity_notifications_delay: mock.MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that ensures in the case where a regression occurs, the release prior to the latest\n        activity to that regression is not overridden.\n        It should only be overridden if the activity was awaiting the upcoming release\n        \"\"\"\n        plugin_is_regression.return_value = True\n\n        # Create a release and a group associated with it\n        old_release = self.create_release(\n            version=\"foobar\", date_added=timezone.now() - timedelta(minutes=30)\n        )\n        manager = EventManager(\n            make_event(\n                event_id=\"a\" * 32,\n                checksum=\"a\" * 32,\n                timestamp=time() - 50000,  # need to work around active_at\n                release=old_release.version,\n            )\n        )\n        event = manager.save(self.project.id)\n        assert event.group is not None\n        group = event.group\n        group.update(status=GroupStatus.RESOLVED, substatus=None)\n\n        # Resolve the group in old_release\n        resolution = GroupResolution.objects.create(release=old_release, group=group)\n        activity = Activity.objects.create(\n            group=group,\n            project=group.project,\n            type=ActivityType.SET_RESOLVED_IN_RELEASE.value,\n            ident=resolution.id,\n            data={\"version\": \"foobar\"},\n        )\n\n        # Create a regression\n        manager = EventManager(\n            make_event(event_id=\"c\" * 32, checksum=\"a\" * 32, timestamp=time(), release=\"b\")\n        )\n        event = manager.save(self.project.id)\n        assert event.group_id == group.id\n\n        group = Group.objects.get(id=group.id)\n        assert group.status == GroupStatus.UNRESOLVED\n\n        activity = Activity.objects.get(id=activity.id)\n        assert activity.data[\"version\"] == \"foobar\"\n\n        regressed_activity = Activity.objects.get(\n            group=group, type=ActivityType.SET_REGRESSION.value\n        )\n        assert regressed_activity.data[\"version\"] == \"b\"\n        assert regressed_activity.data[\"follows_semver\"] is False\n\n        mock_send_activity_notifications_delay.assert_called_once_with(regressed_activity.id)\n\n    @mock.patch(\"sentry.tasks.activity.send_activity_notifications.delay\")\n    @mock.patch(\"sentry.event_manager.plugin_is_regression\")\n    def test_current_release_version_in_latest_activity_prior_to_regression_is_not_overridden(\n        self,\n        plugin_is_regression: mock.MagicMock,\n        mock_send_activity_notifications_delay: mock.MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that ensures in the case where a regression occurs, the release prior to the latest\n        activity to that regression is overridden with the release regression occurred in but the\n        value of `current_release_version` used for semver is not lost in the update.\n        \"\"\"\n        plugin_is_regression.return_value = True\n\n        # Create a release and a group associated with it\n        old_release = self.create_release(\n            version=\"a\", date_added=timezone.now() - timedelta(minutes=30)\n        )\n        manager = EventManager(\n            make_event(\n                event_id=\"a\" * 32,\n                checksum=\"a\" * 32,\n                timestamp=time() - 50000,  # need to work around active_at\n                release=old_release.version,\n            )\n        )\n        event = manager.save(self.project.id)\n        assert event.group is not None\n        group = event.group\n        group.update(status=GroupStatus.RESOLVED, substatus=None)\n\n        # Resolve the group in old_release\n        resolution = GroupResolution.objects.create(release=old_release, group=group)\n        activity = Activity.objects.create(\n            group=group,\n            project=group.project,\n            type=ActivityType.SET_RESOLVED_IN_RELEASE.value,\n            ident=resolution.id,\n            data={\"version\": \"\", \"current_release_version\": \"pre foobar\"},\n        )\n\n        # Create a regression\n        manager = EventManager(\n            make_event(event_id=\"c\" * 32, checksum=\"a\" * 32, timestamp=time(), release=\"b\")\n        )\n        event = manager.save(self.project.id)\n        assert event.group_id == group.id\n\n        group = Group.objects.get(id=group.id)\n        assert group.status == GroupStatus.UNRESOLVED\n\n        activity = Activity.objects.get(id=activity.id)\n        assert activity.data[\"version\"] == \"b\"\n        assert activity.data[\"current_release_version\"] == \"pre foobar\"\n\n        regressed_activity = Activity.objects.get(\n            group=group, type=ActivityType.SET_REGRESSION.value\n        )\n        assert regressed_activity.data[\"version\"] == \"b\"\n\n        mock_send_activity_notifications_delay.assert_called_once_with(regressed_activity.id)\n\n    @mock.patch(\"sentry.event_manager.plugin_is_regression\")\n    def test_resolved_in_release_regression_activity_follows_semver(\n        self, plugin_is_regression: mock.MagicMock\n    ) -> None:\n        \"\"\"\n        Issue was marked resolved in 1.0.0, regression occurred in 2.0.0.\n        If the project follows semver then the regression activity should have `follows_semver` set.\n        We should also record which version the issue was resolved in as `resolved_in_version`.\n\n        This allows the UI to say the issue was resolved in 1.0.0, regressed in 2.0.0 and\n        the versions were compared using semver.\n        \"\"\"\n        plugin_is_regression.return_value = True\n\n        # Create a release and a group associated with it\n        old_release = self.create_release(\n            version=\"foo@1.0.0\", date_added=timezone.now() - timedelta(minutes=30)\n        )\n        manager = EventManager(\n            make_event(\n                event_id=\"a\" * 32,\n                checksum=\"a\" * 32,\n                timestamp=time() - 50000,  # need to work around active_at\n                release=old_release.version,\n            )\n        )\n        event = manager.save(self.project.id)\n        assert event.group is not None\n        group = event.group\n        group.update(status=GroupStatus.RESOLVED, substatus=None)\n\n        # Resolve the group in old_release\n        resolution = GroupResolution.objects.create(release=old_release, group=group)\n        activity = Activity.objects.create(\n            group=group,\n            project=group.project,\n            type=ActivityType.SET_RESOLVED_IN_RELEASE.value,\n            ident=resolution.id,\n            data={\"version\": \"foo@1.0.0\"},\n        )\n\n        # Create a regression\n        manager = EventManager(\n            make_event(event_id=\"c\" * 32, checksum=\"a\" * 32, timestamp=time(), release=\"foo@2.0.0\")\n        )\n        event = manager.save(self.project.id)\n        assert event.group_id == group.id\n\n        group = Group.objects.get(id=group.id)\n        assert group.status == GroupStatus.UNRESOLVED\n\n        activity = Activity.objects.get(id=activity.id)\n        assert activity.data[\"version\"] == \"foo@1.0.0\"\n\n        regressed_activity = Activity.objects.get(\n            group=group, type=ActivityType.SET_REGRESSION.value\n        )\n        assert regressed_activity.data[\"version\"] == \"foo@2.0.0\"\n        assert regressed_activity.data[\"follows_semver\"] is True\n        assert regressed_activity.data[\"resolved_in_version\"] == \"foo@1.0.0\"\n\n    def test_has_pending_commit_resolution(self) -> None:\n        project_id = self.project.id\n        event = self.make_release_event(\"1.0\", project_id)\n\n        group = event.group\n        assert group is not None\n        assert group.first_release is not None\n        assert group.first_release.version == \"1.0\"\n        assert not has_pending_commit_resolution(group)\n\n        # Add a commit with no associated release\n        repo = self.create_repo(project=group.project)\n        commit = Commit.objects.create(\n            organization_id=group.project.organization_id, repository_id=repo.id, key=\"a\" * 40\n        )\n        GroupLink.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            linked_type=GroupLink.LinkedType.commit,\n            linked_id=commit.id,\n            relationship=GroupLink.Relationship.resolves,\n        )\n\n        assert has_pending_commit_resolution(group)\n\n    def test_multiple_pending_commit_resolution(self) -> None:\n        project_id = self.project.id\n        event = self.make_release_event(\"1.0\", project_id)\n        group = event.group\n        assert group is not None\n        assert group.first_release is not None\n\n        # Add a few commits with no associated release\n        repo = self.create_repo(project=group.project)\n        for key in [\"a\", \"b\", \"c\"]:\n            commit = Commit.objects.create(\n                organization_id=group.project.organization_id,\n                repository_id=repo.id,\n                key=key * 40,\n            )\n            GroupLink.objects.create(\n                group_id=group.id,\n                project_id=group.project_id,\n                linked_type=GroupLink.LinkedType.commit,\n                linked_id=commit.id,\n                relationship=GroupLink.Relationship.resolves,\n            )\n\n        pending = has_pending_commit_resolution(group)\n        assert pending\n\n        # Most recent commit has been associated with a release\n        latest_commit = Commit.objects.create(\n            organization_id=group.project.organization_id, repository_id=repo.id, key=\"d\" * 40\n        )\n        GroupLink.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            linked_type=GroupLink.LinkedType.commit,\n            linked_id=latest_commit.id,\n            relationship=GroupLink.Relationship.resolves,\n        )\n        ReleaseCommit.objects.create(\n            organization_id=group.project.organization_id,\n            release=group.first_release,\n            commit=latest_commit,\n            order=0,\n        )\n\n        pending = has_pending_commit_resolution(group)\n        assert pending is False\n\n    def test_has_pending_commit_resolution_issue_regression(self) -> None:\n        project_id = self.project.id\n        event = self.make_release_event(\"1.0\", project_id)\n        group = event.group\n        assert group is not None\n        repo = self.create_repo(project=group.project)\n\n        # commit that resolved the issue is part of a PR, but all commits within the PR are unreleased\n        commit = Commit.objects.create(\n            organization_id=group.project.organization_id, repository_id=repo.id, key=\"a\" * 40\n        )\n\n        second_commit = Commit.objects.create(\n            organization_id=group.project.organization_id, repository_id=repo.id, key=\"b\" * 40\n        )\n\n        GroupLink.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            linked_type=GroupLink.LinkedType.commit,\n            linked_id=commit.id,\n            relationship=GroupLink.Relationship.resolves,\n        )\n\n        pr = PullRequest.objects.create(\n            organization_id=group.project.organization_id,\n            repository_id=repo.id,\n            key=\"1\",\n        )\n\n        PullRequestCommit.objects.create(pull_request_id=pr.id, commit_id=commit.id)\n        PullRequestCommit.objects.create(pull_request_id=pr.id, commit_id=second_commit.id)\n\n        assert PullRequestCommit.objects.filter(pull_request_id=pr.id, commit_id=commit.id).exists()\n        assert PullRequestCommit.objects.filter(\n            pull_request_id=pr.id, commit_id=second_commit.id\n        ).exists()\n\n        assert not ReleaseCommit.objects.filter(commit__pullrequestcommit__id=commit.id).exists()\n        assert not ReleaseCommit.objects.filter(\n            commit__pullrequestcommit__id=second_commit.id\n        ).exists()\n\n        pending = has_pending_commit_resolution(group)\n        assert pending\n\n    def test_has_pending_commit_resolution_issue_regression_released_commits(self) -> None:\n        project_id = self.project.id\n        event = self.make_release_event(\"1.0\", project_id)\n        group = event.group\n        assert group is not None\n        release = self.create_release(project=self.project, version=\"1.1\")\n\n        repo = self.create_repo(project=group.project)\n\n        # commit 1 is part of the PR, it resolves the issue in the commit message, and is unreleased\n        commit = Commit.objects.create(\n            organization_id=group.project.organization_id, repository_id=repo.id, key=\"a\" * 38\n        )\n\n        GroupLink.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            linked_type=GroupLink.LinkedType.commit,\n            linked_id=commit.id,\n            relationship=GroupLink.Relationship.resolves,\n        )\n\n        # commit 2 is part of the PR, but does not resolve the issue, and is released\n        released_commit = Commit.objects.create(\n            organization_id=group.project.organization_id, repository_id=repo.id, key=\"b\" * 38\n        )\n\n        # commit 3 is part of the PR, but does not resolve the issue, and is unreleased\n        unreleased_commit = Commit.objects.create(\n            organization_id=group.project.organization_id, repository_id=repo.id, key=\"c\" * 38\n        )\n\n        pr = PullRequest.objects.create(\n            organization_id=group.project.organization_id,\n            repository_id=repo.id,\n            key=\"19\",\n        )\n\n        PullRequestCommit.objects.create(pull_request_id=pr.id, commit_id=commit.id)\n\n        released_pr_commit = PullRequestCommit.objects.create(\n            pull_request_id=pr.id, commit_id=released_commit.id\n        )\n\n        unreleased_pr_commit = PullRequestCommit.objects.create(\n            pull_request_id=pr.id, commit_id=unreleased_commit.id\n        )\n\n        ReleaseCommit.objects.create(\n            organization_id=group.project.organization_id,\n            release=release,\n            commit=released_commit,\n            order=1,\n        )\n\n        assert Commit.objects.all().count() == 3\n        assert PullRequestCommit.objects.filter(pull_request_id=pr.id, commit_id=commit.id).exists()\n        assert PullRequestCommit.objects.filter(\n            pull_request_id=pr.id, commit_id=released_commit.id\n        ).exists()\n        assert PullRequestCommit.objects.filter(commit__id=unreleased_pr_commit.commit.id).exists()\n        assert ReleaseCommit.objects.filter(\n            commit__pullrequestcommit__id=released_pr_commit.id\n        ).exists()\n\n        pending = has_pending_commit_resolution(group)\n        assert pending is False\n\n    @mock.patch(\"sentry.integrations.example.integration.ExampleIntegration.sync_status_outbound\")\n    @mock.patch(\"sentry.tasks.activity.send_activity_notifications.delay\")\n    @mock.patch(\"sentry.event_manager.plugin_is_regression\")\n    def test_marks_as_unresolved_with_new_release_with_integration(\n        self,\n        plugin_is_regression: mock.MagicMock,\n        mock_send_activity_notifications_delay: mock.MagicMock,\n        mock_sync_status_outbound: mock.MagicMock,\n    ) -> None:\n        plugin_is_regression.return_value = True\n\n        old_release = Release.objects.create(\n            version=\"a\",\n            organization_id=self.project.organization_id,\n            date_added=timezone.now() - timedelta(minutes=30),\n        )\n        old_release.add_project(self.project)\n\n        manager = EventManager(\n            make_event(\n                event_id=\"a\" * 32,\n                checksum=\"a\" * 32,\n                timestamp=time() - 50000,  # need to work around active_at\n                release=old_release.version,\n            )\n        )\n        event = manager.save(self.project.id)\n\n        assert event.group is not None\n        group = event.group\n\n        org = group.organization\n\n        integration = self.create_integration(\n            organization=org,\n            external_id=\"example\",\n            oi_params={\n                \"config\": {\n                    \"sync_comments\": True,\n                    \"sync_status_outbound\": True,\n                    \"sync_status_inbound\": True,\n                    \"sync_assignee_outbound\": True,\n                    \"sync_assignee_inbound\": True,\n                }\n            },\n            provider=\"example\",\n            name=\"Example\",\n        )\n\n        external_issue = ExternalIssue.objects.get_or_create(\n            organization_id=org.id, integration_id=integration.id, key=\"APP-%s\" % group.id\n        )[0]\n\n        GroupLink.objects.get_or_create(\n            group_id=group.id,\n            project_id=group.project_id,\n            linked_type=GroupLink.LinkedType.issue,\n            linked_id=external_issue.id,\n            relationship=GroupLink.Relationship.references,\n        )[0]\n\n        group.update(status=GroupStatus.RESOLVED, substatus=None)\n\n        resolution = GroupResolution.objects.create(release=old_release, group=group)\n        activity = Activity.objects.create(\n            group=group,\n            project=group.project,\n            type=ActivityType.SET_RESOLVED_IN_RELEASE.value,\n            ident=resolution.id,\n            data={\"version\": \"\"},\n        )\n\n        manager = EventManager(\n            make_event(\n                event_id=\"b\" * 32, checksum=\"a\" * 32, timestamp=time(), release=old_release.version\n            )\n        )\n\n        with self.tasks():\n            with self.feature({\"organizations:integrations-issue-sync\": True}):\n                event = manager.save(self.project.id)\n                assert event.group_id == group.id\n\n                group = Group.objects.get(id=group.id)\n                assert group.status == GroupStatus.RESOLVED\n\n                activity = Activity.objects.get(id=activity.id)\n                assert activity.data[\"version\"] == \"\"\n\n                assert GroupResolution.objects.filter(group=group).exists()\n\n                manager = EventManager(\n                    make_event(event_id=\"c\" * 32, checksum=\"a\" * 32, timestamp=time(), release=\"b\")\n                )\n                event = manager.save(self.project.id)\n                assert event.group is not None\n                mock_sync_status_outbound.assert_called_once_with(\n                    external_issue, False, event.group.project_id\n                )\n                assert event.group_id == group.id\n\n                group = Group.objects.get(id=group.id)\n                assert group.status == GroupStatus.UNRESOLVED\n\n                activity = Activity.objects.get(id=activity.id)\n                assert activity.data[\"version\"] == \"b\"\n\n                assert not GroupResolution.objects.filter(group=group).exists()\n\n                activity = Activity.objects.get(group=group, type=ActivityType.SET_REGRESSION.value)\n\n                mock_send_activity_notifications_delay.assert_called_once_with(activity.id)\n\n    @mock.patch(\"sentry.tasks.activity.send_activity_notifications.delay\")\n    @mock.patch(\"sentry.event_manager.plugin_is_regression\")\n    def test_does_not_mark_as_unresolved_with_pending_commit(\n        self,\n        plugin_is_regression: mock.MagicMock,\n        mock_send_activity_notifications_delay: mock.MagicMock,\n    ) -> None:\n        plugin_is_regression.return_value = True\n\n        repo = self.create_repo(project=self.project)\n        commit = self.create_commit(repo=repo)\n\n        manager = EventManager(\n            make_event(\n                event_id=\"a\" * 32,\n                checksum=\"a\" * 32,\n                timestamp=time() - 50000,  # need to work around active_at\n            )\n        )\n        event = manager.save(self.project.id)\n\n        group = event.group\n        assert group is not None\n\n        group.update(status=GroupStatus.RESOLVED, substatus=None)\n        GroupLink.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            linked_id=commit.id,\n            linked_type=GroupLink.LinkedType.commit,\n            relationship=GroupLink.Relationship.resolves,\n        )\n\n        manager = EventManager(make_event(event_id=\"b\" * 32, checksum=\"a\" * 32, timestamp=time()))\n        event = manager.save(self.project.id)\n        assert event.group is not None\n        assert event.group_id == group.id\n\n        assert Group.objects.get(id=group.id).status == GroupStatus.RESOLVED\n\n    @mock.patch(\"sentry.tasks.activity.send_activity_notifications.delay\")\n    @mock.patch(\"sentry.event_manager.plugin_is_regression\")\n    def test_mark_as_unresolved_with_released_commit(\n        self,\n        plugin_is_regression: mock.MagicMock,\n        mock_send_activity_notifications_delay: mock.MagicMock,\n    ) -> None:\n        plugin_is_regression.return_value = True\n\n        release = self.create_release(project=self.project)\n        repo = self.create_repo(project=self.project)\n        commit = self.create_commit(repo=repo, release=release, project=self.project)\n\n        manager = EventManager(\n            make_event(\n                event_id=\"a\" * 32,\n                checksum=\"a\" * 32,\n                timestamp=time() - 50000,  # need to work around active_at\n            )\n        )\n        event = manager.save(self.project.id)\n\n        group = event.group\n        assert group is not None\n\n        group.update(status=GroupStatus.RESOLVED, substatus=None)\n\n        GroupLink.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            linked_id=commit.id,\n            linked_type=GroupLink.LinkedType.commit,\n            relationship=GroupLink.Relationship.resolves,\n        )\n\n        manager = EventManager(make_event(event_id=\"b\" * 32, checksum=\"a\" * 32, timestamp=time()))\n\n        event = manager.save(self.project.id)\n        assert event.group is not None\n        assert event.group_id == group.id\n\n        assert Group.objects.get(id=group.id).status == GroupStatus.UNRESOLVED\n\n    @mock.patch(\"sentry.models.Group.is_resolved\")\n    @with_feature(\"organizations:issue-open-periods\")\n    def test_unresolves_group_with_auto_resolve(self, mock_is_resolved: mock.MagicMock) -> None:\n        ts = before_now(minutes=5).isoformat()\n        mock_is_resolved.return_value = False\n        manager = EventManager(make_event(event_id=\"a\" * 32, checksum=\"a\" * 32, timestamp=ts))\n        with self.tasks():\n            event = manager.save(self.project.id)\n        assert event.group is not None\n\n        resolved_at = before_now(minutes=4)\n        activity = Activity.objects.create(\n            group=event.group,\n            project=event.group.project,\n            type=ActivityType.SET_RESOLVED.value,\n            datetime=resolved_at,\n        )\n        GroupOpenPeriod.objects.get(group=event.group, date_ended__isnull=True).close_open_period(\n            resolution_time=resolved_at,\n            resolution_activity=activity,\n        )\n\n        mock_is_resolved.return_value = True\n        manager = EventManager(\n            make_event(\n                event_id=\"b\" * 32, checksum=\"a\" * 32, timestamp=before_now(minutes=3).isoformat()\n            )\n        )\n        with self.tasks():\n            event2 = manager.save(self.project.id)\n        assert event2.group is not None\n        assert event.group_id == event2.group_id\n\n        group = Group.objects.get(id=event.group.id)\n        assert group.active_at\n        assert group.active_at.replace(second=0) == event2.datetime.replace(second=0)\n        assert group.active_at.replace(second=0) != event.datetime.replace(second=0)\n\n        open_periods = GroupOpenPeriod.objects.filter(group=group).order_by(\"-date_started\")\n        assert len(open_periods) == 2\n        open_period = open_periods[0]\n        assert open_period.date_started == event2.datetime\n        assert open_period.date_ended is None\n        open_period = open_periods[1]\n        assert open_period.date_started == group.first_seen\n        assert open_period.date_ended == resolved_at\n\n    def test_invalid_transaction(self) -> None:\n        dict_input = {\"messages\": \"foo\"}\n        manager = EventManager(make_event(transaction=dict_input))\n        manager.normalize()\n        event = manager.save(self.project.id)\n        assert event.transaction is None\n\n    def test_transaction_as_culprit(self) -> None:\n        manager = EventManager(make_event(transaction=\"foobar\"))\n        manager.normalize()\n        event = manager.save(self.project.id)\n        assert event.transaction == \"foobar\"\n        assert event.culprit == \"foobar\"\n\n    def test_culprit_is_not_transaction(self) -> None:\n        manager = EventManager(make_event(culprit=\"foobar\"))\n        manager.normalize()\n        event1 = manager.save(self.project.id)\n        assert event1.transaction is None\n        assert event1.culprit == \"foobar\"\n\n    def test_culprit_after_stacktrace_processing(self) -> None:\n        from sentry.grouping.enhancer import Enhancements\n\n        enhancements_str = Enhancements.from_rules_text(\n            \"\"\"\n            function:in_app_function +app\n            function:not_in_app_function -app\n            \"\"\"\n        ).base64_string\n\n        grouping_config = {\"id\": DEFAULT_GROUPING_CONFIG, \"enhancements\": enhancements_str}\n\n        with patch(\n            \"sentry.grouping.ingest.hashing.get_grouping_config_dict_for_project\",\n            return_value=grouping_config,\n        ):\n            manager = EventManager(\n                make_event(\n                    platform=\"native\",\n                    exception={\n                        \"values\": [\n                            {\n                                \"type\": \"Hello\",\n                                \"stacktrace\": {\n                                    \"frames\": [\n                                        {\n                                            \"function\": \"not_in_app_function\",\n                                        },\n                                        {\n                                            \"function\": \"in_app_function\",\n                                        },\n                                    ]\n                                },\n                            }\n                        ]\n                    },\n                )\n            )\n            manager.normalize()\n            event1 = manager.save(self.project.id)\n            assert event1.transaction is None\n            assert event1.culprit == \"in_app_function\"\n\n    def test_inferred_culprit_from_empty_stacktrace(self) -> None:\n        manager = EventManager(make_event(stacktrace={\"frames\": []}))\n        manager.normalize()\n        event = manager.save(self.project.id)\n        assert event.culprit == \"\"\n\n    def test_transaction_and_culprit(self) -> None:\n        manager = EventManager(make_event(transaction=\"foobar\", culprit=\"baz\"))\n        manager.normalize()\n        event1 = manager.save(self.project.id)\n        assert event1.transaction == \"foobar\"\n        assert event1.culprit == \"baz\"\n\n    def test_release_with_empty_version(self) -> None:\n        cases = [\"\", \" \", \"\\t\", \"\\n\"]\n        for case in cases:\n            event = self.make_release_event(case, self.project.id)\n            assert event.group is not None\n            assert not event.group.first_release\n            assert Release.objects.filter(projects__in=[self.project.id]).count() == 0\n            assert Release.objects.filter(organization_id=self.project.organization_id).count() == 0\n\n    def test_first_release(self) -> None:\n        project_id = self.project.id\n        event = self.make_release_event(\"1.0\", project_id)\n\n        group = event.group\n        assert group is not None\n        assert group.first_release is not None\n        assert group.first_release.version == \"1.0\"\n\n        event = self.make_release_event(\"2.0\", project_id)\n\n        group = event.group\n        assert group is not None\n        assert group.first_release is not None\n        assert group.first_release.version == \"1.0\"\n\n    def test_release_project_slug(self) -> None:\n        project = self.create_project(name=\"foo\")\n        release = Release.objects.create(version=\"foo-1.0\", organization=project.organization)\n        release.add_project(project)\n\n        event = self.make_release_event(\"1.0\", project.id)\n\n        group = event.group\n        assert group is not None\n        assert group.first_release is not None\n        assert group.first_release.version == \"foo-1.0\"\n        release_tag = [v for k, v in event.tags if k == \"sentry:release\"][0]\n        assert release_tag == \"foo-1.0\"\n\n        event = self.make_release_event(\"2.0\", project.id)\n\n        group = event.group\n        assert group is not None\n        assert group.first_release is not None\n        assert group.first_release.version == \"foo-1.0\"\n\n    def test_release_project_slug_long(self) -> None:\n        project = self.create_project(name=\"foo\")\n        partial_version_len = MAX_VERSION_LENGTH - 4\n        release = Release.objects.create(\n            version=\"foo-{}\".format(\"a\" * partial_version_len), organization=project.organization\n        )\n        release.add_project(project)\n\n        event = self.make_release_event(\"a\" * partial_version_len, project.id)\n\n        group = event.group\n        assert group is not None\n        assert group.first_release is not None\n        assert group.first_release.version == \"foo-{}\".format(\"a\" * partial_version_len)\n        release_tag = [v for k, v in event.tags if k == \"sentry:release\"][0]\n        assert release_tag == \"foo-{}\".format(\"a\" * partial_version_len)\n\n    def test_group_release_no_env(self) -> None:\n        project_id = self.project.id\n        event = self.make_release_event(\"1.0\", project_id)\n        assert event.group_id is not None\n\n        release = Release.objects.get(version=\"1.0\", projects=event.project_id)\n\n        assert GroupRelease.objects.filter(\n            release_id=release.id, group_id=event.group_id, environment=\"\"\n        ).exists()\n\n        # ensure we're not erroring on second creation\n        self.make_release_event(\"1.0\", project_id)\n\n    def test_group_release_with_env(self) -> None:\n        manager = EventManager(make_event(release=\"1.0\", environment=\"prod\", event_id=\"a\" * 32))\n        manager.normalize()\n        event = manager.save(self.project.id)\n        assert event.group_id is not None\n\n        release = Release.objects.get(version=\"1.0\", projects=event.project_id)\n\n        assert GroupRelease.objects.filter(\n            release_id=release.id, group_id=event.group_id, environment=\"prod\"\n        ).exists()\n\n        manager = EventManager(make_event(release=\"1.0\", environment=\"staging\", event_id=\"b\" * 32))\n        event = manager.save(self.project.id)\n\n        release = Release.objects.get(version=\"1.0\", projects=event.project_id)\n\n        assert event.group_id is not None\n        assert GroupRelease.objects.filter(\n            release_id=release.id, group_id=event.group_id, environment=\"staging\"\n        ).exists()\n\n    def test_tsdb(self) -> None:\n        project = self.project\n        manager = EventManager(\n            make_event(\n                fingerprint=[\"totally unique super duper fingerprint\"],\n                environment=\"totally unique super duper environment\",\n            )\n        )\n        event = manager.save(project.id)\n        assert event.group is not None\n\n        def query(model: TSDBModel, key: int, **kwargs: Any) -> int:\n            return tsdb.backend.get_timeseries_sums(\n                model,\n                [key],\n                event.datetime,\n                event.datetime,\n                tenant_ids={\"organization_id\": 123, \"referrer\": \"r\"},\n                **kwargs,\n            )[key]\n\n        assert query(TSDBModel.project, project.id) == 1\n        assert query(TSDBModel.group, event.group.id) == 1\n\n        environment_id = Environment.get_for_organization_id(\n            event.project.organization_id, \"totally unique super duper environment\"\n        ).id\n        assert query(TSDBModel.project, project.id, environment_id=environment_id) == 1\n        assert query(TSDBModel.group, event.group.id, environment_id=environment_id) == 1\n\n    def test_event_user(self) -> None:\n        event_id = uuid.uuid4().hex\n        manager = EventManager(\n            make_event(\n                event_id=event_id, environment=\"totally unique environment\", **{\"user\": {\"id\": \"1\"}}\n            )\n        )\n        manager.normalize()\n        with self.tasks():\n            event = manager.save(self.project.id)\n        assert event.group is not None\n\n        environment_id = Environment.get_for_organization_id(\n            event.project.organization_id, \"totally unique environment\"\n        ).id\n\n        assert tsdb.backend.get_distinct_counts_totals(\n            TSDBModel.users_affected_by_group,\n            (event.group.id,),\n            event.datetime,\n            event.datetime,\n            tenant_ids={\"referrer\": \"r\", \"organization_id\": 123},\n        ) == {event.group.id: 1}\n\n        assert tsdb.backend.get_distinct_counts_totals(\n            TSDBModel.users_affected_by_project,\n            (event.project.id,),\n            event.datetime,\n            event.datetime,\n            tenant_ids={\"organization_id\": 123, \"referrer\": \"r\"},\n        ) == {event.project.id: 1}\n\n        assert tsdb.backend.get_distinct_counts_totals(\n            TSDBModel.users_affected_by_group,\n            (event.group.id,),\n            event.datetime,\n            event.datetime,\n            environment_id=environment_id,\n            tenant_ids={\"organization_id\": 123, \"referrer\": \"r\"},\n        ) == {event.group.id: 1}\n\n        assert tsdb.backend.get_distinct_counts_totals(\n            TSDBModel.users_affected_by_project,\n            (event.project.id,),\n            event.datetime,\n            event.datetime,\n            environment_id=environment_id,\n            tenant_ids={\"organization_id\": 123, \"referrer\": \"r\"},\n        ) == {event.project.id: 1}\n\n        saved_event = eventstore.backend.get_event_by_id(self.project.id, event_id)\n        assert saved_event is not None\n        euser = EventUser.from_event(saved_event)\n        assert event.get_tag(\"sentry:user\") == euser.tag_value\n\n        # clear the cache otherwise the cached EventUser from prev\n        # manager.save() will be used instead of jane\n        cache.clear()\n\n        # ensure event user is mapped to tags in second attempt\n        event_id_2 = uuid.uuid4().hex\n        manager = EventManager(\n            make_event(event_id=event_id_2, **{\"user\": {\"id\": \"1\", \"name\": \"jane\"}})\n        )\n        manager.normalize()\n        with self.tasks():\n            manager.save(self.project.id)\n\n        saved_event = eventstore.backend.get_event_by_id(self.project.id, event_id_2)\n        assert saved_event is not None\n        euser = EventUser.from_event(saved_event)\n        assert event.get_tag(\"sentry:user\") == euser.tag_value\n        assert euser.name == \"jane\"\n        assert euser.user_ident == \"1\"\n\n    def test_event_user_invalid_ip(self) -> None:\n        event_id = uuid.uuid4().hex\n        manager = EventManager(\n            make_event(\n                event_id=event_id, environment=\"totally unique environment\", **{\"user\": {\"id\": \"1\"}}\n            )\n        )\n\n        manager.normalize()\n\n        # This can happen as part of PII stripping, which happens after normalization\n        manager._data[\"user\"][\"ip_address\"] = \"[ip]\"\n\n        with self.tasks():\n            manager.save(self.project.id)\n\n        saved_event = eventstore.backend.get_event_by_id(self.project.id, event_id)\n        assert saved_event is not None\n        euser = EventUser.from_event(saved_event)\n        assert euser.ip_address is None\n\n    def test_event_user_unicode_identifier(self) -> None:\n        event_id = uuid.uuid4().hex\n        manager = EventManager(make_event(event_id=event_id, **{\"user\": {\"username\": \"foô\"}}))\n        manager.normalize()\n        with self.tasks():\n            manager.save(self.project.id)\n\n        saved_event = eventstore.backend.get_event_by_id(self.project.id, event_id)\n        assert saved_event is not None\n        euser = EventUser.from_event(saved_event)\n        assert euser.username == \"foô\"\n\n    def test_environment(self) -> None:\n        manager = EventManager(make_event(**{\"environment\": \"beta\"}))\n        manager.normalize()\n        event = manager.save(self.project.id)\n\n        assert dict(event.tags).get(\"environment\") == \"beta\"\n\n    def test_invalid_environment(self) -> None:\n        manager = EventManager(make_event(**{\"environment\": \"bad/name\"}))\n        manager.normalize()\n        event = manager.save(self.project.id)\n        assert dict(event.tags).get(\"environment\") is None\n\n    def test_invalid_tags(self) -> None:\n        manager = EventManager(make_event(**{\"tags\": [42]}))\n        manager.normalize()\n        assert None in manager.get_data().get(\"tags\", [])\n        assert 42 not in manager.get_data().get(\"tags\", [])\n        event = manager.save(self.project.id)\n        assert 42 not in event.tags\n        assert None not in event.tags\n\n    @mock.patch(\"sentry.event_manager.eventstream.backend.insert\")\n    def test_group_environment(self, eventstream_insert: mock.MagicMock) -> None:\n        release_version = \"1.0\"\n\n        def save_event() -> Event:\n            manager = EventManager(\n                make_event(\n                    **{\n                        \"message\": \"foo\",\n                        \"event_id\": uuid.uuid1().hex,\n                        \"environment\": \"beta\",\n                        \"release\": release_version,\n                    }\n                )\n            )\n            manager.normalize()\n            return manager.save(self.project.id)\n\n        event = save_event()\n        assert event.group_id is not None\n\n        # Ensure the `GroupEnvironment` record was created.\n        instance = GroupEnvironment.objects.get(\n            group_id=event.group_id,\n            environment_id=Environment.objects.get(\n                organization_id=self.project.organization_id, name=event.get_tag(\"environment\")\n            ).id,\n        )\n\n        assert Release.objects.get(id=instance.first_release_id).version == release_version\n\n        group_states1 = {\n            \"is_new\": True,\n            \"is_regression\": False,\n            \"is_new_group_environment\": True,\n        }\n        # Ensure that the first event in the (group, environment) pair is\n        # marked as being part of a new environment.\n        assert event.group is not None\n        eventstream_insert.assert_called_with(\n            event=event,\n            **group_states1,\n            primary_hash=\"acbd18db4cc2f85cedef654fccc4a4d8\",\n            skip_consume=False,\n            received_timestamp=event.data[\"received\"],\n            group_states=[{\"id\": event.group.id, **group_states1}],\n        )\n\n        event = save_event()\n\n        group_states2 = {\n            \"is_new\": False,\n            \"is_regression\": False,\n            \"is_new_group_environment\": False,\n        }\n\n        # Ensure that the next event in the (group, environment) pair is *not*\n        # marked as being part of a new environment.\n        assert event.group is not None\n        eventstream_insert.assert_called_with(\n            event=event,\n            **group_states2,\n            primary_hash=\"acbd18db4cc2f85cedef654fccc4a4d8\",\n            skip_consume=False,\n            received_timestamp=event.data[\"received\"],\n            group_states=[{\"id\": event.group.id, **group_states2}],\n        )\n\n    def test_default_event_type(self) -> None:\n        manager = EventManager(make_event(message=\"foo bar\"))\n        manager.normalize()\n        data = manager.get_data()\n        assert data[\"type\"] == \"default\"\n        event = manager.save(self.project.id)\n        group = event.group\n        assert group is not None\n        assert group.data[\"type\"] == \"default\"\n        assert group.data[\"metadata\"][\"title\"] == \"foo bar\"\n\n    def test_message_event_type(self) -> None:\n        manager = EventManager(\n            make_event(\n                **{\n                    \"message\": \"\",\n                    \"logentry\": {\"formatted\": \"foo bar\", \"message\": \"foo %s\", \"params\": [\"bar\"]},\n                }\n            )\n        )\n        manager.normalize()\n        data = manager.get_data()\n        assert data[\"type\"] == \"default\"\n        event = manager.save(self.project.id)\n        group = event.group\n        assert group is not None\n        assert group.data[\"type\"] == \"default\"\n        assert group.data[\"metadata\"][\"title\"] == \"foo bar\"\n\n    @with_feature(\"organizations:issue-open-periods\")\n    def test_error_event_type(self) -> None:\n        from sentry.models.groupopenperiod import GroupOpenPeriod\n\n        manager = EventManager(\n            make_event(**{\"exception\": {\"values\": [{\"type\": \"Foo\", \"value\": \"bar\"}]}})\n        )\n        manager.normalize()\n        data = manager.get_data()\n        assert data[\"type\"] == \"error\"\n        event = manager.save(self.project.id)\n        group = event.group\n        assert group is not None\n        assert group.data.get(\"type\") == \"error\"\n        assert group.data.get(\"metadata\") == {\n            \"type\": \"Foo\",\n            \"value\": \"bar\",\n            \"initial_priority\": PriorityLevel.HIGH,\n        }\n\n        open_period = GroupOpenPeriod.objects.filter(group=group)\n        assert len(open_period) == 1\n        assert open_period[0].group_id == group.id\n        assert open_period[0].project_id == self.project.id\n        assert open_period[0].date_started == group.first_seen\n        assert open_period[0].date_ended is None\n\n    @with_feature(\"organizations:issue-open-periods\")\n    def test_error_event_with_minified_stacktrace(self) -> None:\n        with patch(\n            \"sentry.receivers.onboarding.record_event_with_first_minified_stack_trace_for_project\",  # autospec=True\n        ) as mock_record_event_with_first_minified_stack_trace_for_project:\n\n            first_event_with_minified_stack_trace_received.connect(\n                mock_record_event_with_first_minified_stack_trace_for_project, weak=False\n            )\n\n        manager = EventManager(\n            make_event(\n                **{\n                    \"exception\": {\n                        \"values\": [\n                            {\n                                \"type\": \"Foo\",\n                                \"value\": \"bar\",\n                                \"stacktrace\": {\n                                    \"frames\": [\n                                        {\"filename\": \"minified.js\", \"function\": \"minifiedFunction\"}\n                                    ]\n                                },\n                                \"raw_stacktrace\": {\n                                    \"frames\": [\n                                        {\n                                            \"filename\": \"minified.js\",\n                                            \"function\": \"minifiedFunction\",\n                                            \"in_app\": True,\n                                        }\n                                    ]\n                                },\n                            }\n                        ]\n                    }\n                }\n            )\n        )\n        manager.normalize()\n        data = manager.get_data()\n        assert data[\"type\"] == \"error\"\n\n        manager.save(self.project.id)\n\n        assert mock_record_event_with_first_minified_stack_trace_for_project.call_count == 1\n        self.project.refresh_from_db()\n        assert self.project.flags.has_minified_stack_trace\n\n    def test_csp_event_type(self) -> None:\n        manager = EventManager(\n            make_event(\n                **{\n                    \"csp\": {\n                        \"effective_directive\": \"script-src\",\n                        \"blocked_uri\": \"http://example.com\",\n                    },\n                    # this normally is noramlized in relay as part of ingest\n                    \"logentry\": {\"message\": \"Blocked 'script' from 'example.com'\"},\n                }\n            )\n        )\n        manager.normalize()\n        data = manager.get_data()\n        assert data[\"type\"] == \"csp\"\n        event = manager.save(self.project.id)\n        group = event.group\n        assert group is not None\n        assert group.data.get(\"type\") == \"csp\"\n        assert group.data.get(\"metadata\") == {\n            \"directive\": \"script-src\",\n            \"initial_priority\": PriorityLevel.HIGH,\n            \"uri\": \"example.com\",\n            \"message\": \"Blocked 'script' from 'example.com'\",\n        }\n        assert group.title == \"Blocked 'script' from 'example.com'\"\n\n    def test_transaction_event_type(self) -> None:\n        manager = EventManager(\n            make_event(\n                **{\n                    \"transaction\": \"wait\",\n                    \"contexts\": {\n                        \"trace\": {\n                            \"parent_span_id\": \"bce14471e0e9654d\",\n                            \"op\": \"foobar\",\n                            \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                            \"span_id\": \"bf5be759039ede9a\",\n                        }\n                    },\n                    \"spans\": [],\n                    \"timestamp\": \"2019-06-14T14:01:40Z\",\n                    \"start_timestamp\": \"2019-06-14T14:01:40Z\",\n                    \"type\": \"transaction\",\n                }\n            )\n        )\n        manager.normalize()\n        data = manager.get_data()\n        assert data[\"type\"] == \"transaction\"\n\n    def test_transaction_event_span_grouping(self) -> None:\n        manager = EventManager(\n            make_event(\n                **{\n                    \"transaction\": \"wait\",\n                    \"contexts\": {\n                        \"trace\": {\n                            \"parent_span_id\": \"bce14471e0e9654d\",\n                            \"op\": \"foobar\",\n                            \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                            \"span_id\": \"bf5be759039ede9a\",\n                        }\n                    },\n                    \"spans\": [\n                        {\n                            \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                            \"parent_span_id\": \"bf5be759039ede9a\",\n                            \"span_id\": \"a\" * 16,\n                            \"start_timestamp\": 0,\n                            \"timestamp\": 1,\n                            \"same_process_as_parent\": True,\n                            \"op\": \"default\",\n                            \"description\": \"span a\",\n                        },\n                        {\n                            \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                            \"parent_span_id\": \"bf5be759039ede9a\",\n                            \"span_id\": \"b\" * 16,\n                            \"start_timestamp\": 0,\n                            \"timestamp\": 1,\n                            \"same_process_as_parent\": True,\n                            \"op\": \"default\",\n                            \"description\": \"span a\",\n                        },\n                        {\n                            \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                            \"parent_span_id\": \"bf5be759039ede9a\",\n                            \"span_id\": \"c\" * 16,\n                            \"start_timestamp\": 0,\n                            \"timestamp\": 1,\n                            \"same_process_as_parent\": True,\n                            \"op\": \"default\",\n                            \"description\": \"span b\",\n                        },\n                    ],\n                    \"timestamp\": \"2019-06-14T14:01:40Z\",\n                    \"start_timestamp\": \"2019-06-14T14:01:40Z\",\n                    \"type\": \"transaction\",\n                }\n            )\n        )\n        manager.normalize()\n        event = manager.save(self.project.id)\n        data = event.data\n        assert data[\"type\"] == \"transaction\"\n        assert data[\"span_grouping_config\"][\"id\"] == \"default:2022-10-27\"\n        spans = [{\"hash\": span[\"hash\"]} for span in data[\"spans\"]]\n        # the basic strategy is to simply use the description\n        assert spans == [{\"hash\": hash_values([span[\"description\"]])} for span in data[\"spans\"]]\n\n    def test_transaction_sampler_and_receive(self) -> None:\n        # make sure with the option on we don't get any errors\n        manager = EventManager(\n            make_event(\n                **{\n                    \"transaction\": \"wait\",\n                    \"contexts\": {\n                        \"trace\": {\n                            \"parent_span_id\": \"bce14471e0e9654d\",\n                            \"op\": \"foobar\",\n                            \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                            \"span_id\": \"bf5be759039ede9a\",\n                        }\n                    },\n                    \"spans\": [\n                        {\n                            \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                            \"parent_span_id\": \"bf5be759039ede9a\",\n                            \"span_id\": \"a\" * 16,\n                            \"start_timestamp\": 0,\n                            \"timestamp\": 1,\n                            \"same_process_as_parent\": True,\n                            \"op\": \"default\",\n                            \"description\": \"span a\",\n                        },\n                        {\n                            \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                            \"parent_span_id\": \"bf5be759039ede9a\",\n                            \"span_id\": \"b\" * 16,\n                            \"start_timestamp\": 0,\n                            \"timestamp\": 1,\n                            \"same_process_as_parent\": True,\n                            \"op\": \"default\",\n                            \"description\": \"span a\",\n                        },\n                        {\n                            \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                            \"parent_span_id\": \"bf5be759039ede9a\",\n                            \"span_id\": \"c\" * 16,\n                            \"start_timestamp\": 0,\n                            \"timestamp\": 1,\n                            \"same_process_as_parent\": True,\n                            \"op\": \"default\",\n                            \"description\": \"span b\",\n                        },\n                    ],\n                    \"timestamp\": \"2019-06-14T14:01:40Z\",\n                    \"start_timestamp\": \"2019-06-14T14:01:40Z\",\n                    \"type\": \"transaction\",\n                    \"transaction_info\": {\n                        \"source\": \"url\",\n                    },\n                }\n            )\n        )\n        manager.normalize()\n        manager.save(self.project.id)\n\n    @patch(\"sentry.event_manager.record_release_received\")\n    @patch(\"sentry.ingest.transaction_clusterer.datasource.redis._record_sample\")\n    def test_transaction_sampler_and_receive_mock_called(\n        self,\n        mock_record_sample: mock.MagicMock,\n        mock_record_release: mock.MagicMock,\n    ) -> None:\n        self.project.update(flags=F(\"flags\").bitand(~Project.flags.has_transactions))\n\n        with (\n            patch(\n                \"sentry.receivers.onboarding.record_first_transaction\",  # autospec=True\n            ) as mock_record_transaction,\n            patch(\n                \"sentry.receivers.onboarding.record_first_insight_span\",  # autospec=True\n            ) as mock_record_insight,\n        ):\n            first_transaction_received.connect(mock_record_transaction, weak=False)\n            first_insight_span_received.connect(mock_record_insight, weak=False)\n        manager = EventManager(\n            make_event(\n                **{\n                    \"transaction\": \"wait\",\n                    \"contexts\": {\n                        \"trace\": {\n                            \"parent_span_id\": \"bce14471e0e9654d\",\n                            \"op\": \"foobar\",\n                            \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                            \"span_id\": \"bf5be759039ede9a\",\n                        }\n                    },\n                    \"spans\": [\n                        {\n                            \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                            \"parent_span_id\": \"bf5be759039ede9a\",\n                            \"span_id\": \"a\" * 16,\n                            \"start_timestamp\": 0,\n                            \"timestamp\": 1,\n                            \"same_process_as_parent\": True,\n                            \"op\": \"db.redis\",\n                            \"description\": \"EXEC *\",\n                            \"sentry_tags\": {\n                                \"description\": \"EXEC *\",\n                                \"category\": \"db\",\n                                \"op\": \"db.redis\",\n                                \"transaction\": \"/app/index\",\n                            },\n                        },\n                        {\n                            \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                            \"parent_span_id\": \"bf5be759039ede9a\",\n                            \"span_id\": \"b\" * 16,\n                            \"start_timestamp\": 0,\n                            \"timestamp\": 1,\n                            \"same_process_as_parent\": True,\n                            \"op\": \"default\",\n                            \"description\": \"span a\",\n                        },\n                        {\n                            \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                            \"parent_span_id\": \"bf5be759039ede9a\",\n                            \"span_id\": \"c\" * 16,\n                            \"start_timestamp\": 0,\n                            \"timestamp\": 1,\n                            \"same_process_as_parent\": True,\n                            \"op\": \"default\",\n                            \"description\": \"span b\",\n                        },\n                    ],\n                    \"timestamp\": \"2019-06-14T14:01:40Z\",\n                    \"start_timestamp\": \"2019-06-14T14:01:40Z\",\n                    \"type\": \"transaction\",\n                    \"release\": \"foo@1.0.0\",\n                    \"transaction_info\": {\n                        \"source\": \"url\",\n                    },\n                }\n            )\n        )\n        manager.normalize()\n        event = manager.save(self.project.id)\n\n        mock_record_release.assert_called_once_with(self.project, \"foo@1.0.0\")\n        mock_record_insight.assert_called_once_with(\n            signal=first_insight_span_received,\n            sender=Project,\n            project=self.project,\n            module=InsightModules.DB,\n        )\n        mock_record_transaction.assert_called_once_with(\n            signal=first_transaction_received,\n            sender=Project,\n            project=self.project,\n            event=event,\n        )\n        assert mock_record_sample.mock_calls == [\n            mock.call(ClustererNamespace.TRANSACTIONS, self.project, \"wait\")\n        ]\n\n    def test_first_insight_span(self) -> None:\n        event_data = make_event(\n            transaction=\"test_transaction\",\n            contexts={\n                \"trace\": {\n                    \"parent_span_id\": \"bce14471e0e9654d\",\n                    \"op\": \"foobar\",\n                    \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                    \"span_id\": \"bf5be759039ede9a\",\n                }\n            },\n            spans=[\n                {\n                    \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                    \"parent_span_id\": \"bf5be759039ede9a\",\n                    \"span_id\": \"a\" * 16,\n                    \"start_timestamp\": 0,\n                    \"timestamp\": 1,\n                    \"same_process_as_parent\": True,\n                    \"op\": \"db.redis\",\n                    \"description\": \"EXEC *\",\n                    \"sentry_tags\": {\n                        \"description\": \"EXEC *\",\n                        \"category\": \"db\",\n                        \"op\": \"db.redis\",\n                        \"transaction\": \"/app/index\",\n                    },\n                }\n            ],\n            timestamp=\"2019-06-14T14:01:40Z\",\n            start_timestamp=\"2019-06-14T14:01:40Z\",\n            type=\"transaction\",\n        )\n\n        assert not self.project.flags.has_insights_db\n\n        manager = EventManager(event_data)\n        manager.normalize()\n        manager.save(self.project.id)\n\n        self.project.refresh_from_db()\n        assert self.project.flags.has_insights_db\n\n    def test_sdk(self) -> None:\n        manager = EventManager(make_event(**{\"sdk\": {\"name\": \"sentry-unity\", \"version\": \"1.0\"}}))\n        manager.normalize()\n        event = manager.save(self.project.id)\n\n        assert event.data[\"sdk\"] == {\n            \"name\": \"sentry-unity\",\n            \"version\": \"1.0\",\n            \"integrations\": None,\n            \"packages\": None,\n        }\n\n    def test_sdk_group_tagging(self) -> None:\n        manager = EventManager(\n            make_event(**{\"sdk\": {\"name\": \"sentry-native-unity\", \"version\": \"1.0\"}})\n        )\n        manager.normalize()\n        event = manager.save(self.project.id)\n        assert event.group is not None\n\n        sdk_metadata = event.group.data[\"metadata\"][\"sdk\"]\n        assert sdk_metadata[\"name\"] == \"sentry-native-unity\"\n        assert sdk_metadata[\"name_normalized\"] == \"sentry.native.unity\"\n\n    def test_no_message(self) -> None:\n        # test that the message is handled gracefully\n        manager = EventManager(\n            make_event(**{\"message\": None, \"logentry\": {\"message\": \"hello world\"}})\n        )\n        manager.normalize()\n        event = manager.save(self.project.id)\n\n        assert event.message == \"hello world\"\n\n    def test_search_message_simple(self) -> None:\n        manager = EventManager(\n            make_event(\n                **{\n                    \"message\": \"test\",\n                    \"transaction\": \"sentry.tasks.process\",\n                }\n            )\n        )\n        manager.normalize()\n        event = manager.save(self.project.id)\n\n        search_message = event.search_message\n        assert \"test\" in search_message\n        assert \"sentry.tasks.process\" in search_message\n\n    def test_search_message_prefers_log_entry_message(self) -> None:\n        manager = EventManager(\n            make_event(\n                **{\n                    \"message\": \"test\",\n                    \"logentry\": {\"message\": \"hello world\"},\n                    \"transaction\": \"sentry.tasks.process\",\n                }\n            )\n        )\n        manager.normalize()\n        event = manager.save(self.project.id)\n\n        search_message = event.search_message\n        assert \"test\" not in search_message\n        assert \"hello world\" in search_message\n        assert \"sentry.tasks.process\" in search_message\n\n    def test_search_message_skips_requested_keys(self) -> None:\n        from sentry.eventstore import models\n\n        with patch.object(models, \"SEARCH_MESSAGE_SKIPPED_KEYS\", (\"dogs\",)):\n            manager = EventManager(\n                make_event(\n                    **{\n                        \"logentry\": {\"message\": \"hello world\"},\n                        \"transaction\": \"sentry.tasks.process\",\n                    }\n                )\n            )\n            manager.normalize()\n            # Normalizing nukes any metadata we might pass when creating the event and event\n            # manager, so we have to add it in here\n            manager._data[\"metadata\"] = {\"dogs\": \"are great\", \"maisey\": \"silly\", \"charlie\": \"goofy\"}\n\n            event = manager.save(\n                self.project.id,\n            )\n\n            search_message = event.search_message\n            assert \"hello world\" in search_message\n            assert \"sentry.tasks.process\" in search_message\n            assert \"silly\" in search_message\n            assert \"goofy\" in search_message\n            assert \"are great\" not in search_message  # \"dogs\" key is skipped\n\n    def test_search_message_skips_bools_and_numbers(self) -> None:\n        from sentry.eventstore import models\n\n        with patch.object(models, \"SEARCH_MESSAGE_SKIPPED_KEYS\", (\"dogs\",)):\n            manager = EventManager(\n                make_event(\n                    **{\n                        \"logentry\": {\"message\": \"hello world\"},\n                        \"transaction\": \"sentry.tasks.process\",\n                    }\n                )\n            )\n            manager.normalize()\n            # Normalizing nukes any metadata we might pass when creating the event and event\n            # manager, so we have to add it in here\n            manager._data[\"metadata\"] = {\n                \"dogs are great\": True,\n                \"maisey\": 12312012,\n                \"charlie\": 1121.2012,\n                \"adopt\": \"don't shop\",\n            }\n\n            event = manager.save(\n                self.project.id,\n            )\n\n            search_message = event.search_message\n            assert \"hello world\" in search_message\n            assert \"sentry.tasks.process\" in search_message\n            assert \"True\" not in search_message  # skipped because it's a boolean\n            assert \"12312012\" not in search_message  # skipped because it's an int\n            assert \"1121.2012\" not in search_message  # skipped because it's a float\n            assert \"don't shop\" in search_message\n\n    def test_stringified_message(self) -> None:\n        manager = EventManager(make_event(**{\"message\": 1234}))\n        manager.normalize()\n        event = manager.save(self.project.id)\n\n        assert event.data[\"logentry\"] == {\"formatted\": \"1234\", \"message\": None, \"params\": None}\n\n    def test_bad_message(self) -> None:\n        # test that invalid messages are rejected\n        manager = EventManager(make_event(**{\"message\": [\"asdf\"]}))\n        manager.normalize()\n        event = manager.save(self.project.id)\n\n        assert event.message == '[\"asdf\"]'\n        assert \"logentry\" in event.data\n\n    def test_message_attribute_goes_to_interface(self) -> None:\n        manager = EventManager(make_event(**{\"message\": \"hello world\"}))\n        manager.normalize()\n        event = manager.save(self.project.id)\n        assert event.data[\"logentry\"] == {\n            \"formatted\": \"hello world\",\n            \"message\": None,\n            \"params\": None,\n        }\n\n    def test_message_attribute_shadowing(self) -> None:\n        # Logentry shadows the legacy message attribute.\n        manager = EventManager(\n            make_event(**{\"message\": \"world hello\", \"logentry\": {\"message\": \"hello world\"}})\n        )\n        manager.normalize()\n        event = manager.save(self.project.id)\n        assert event.data[\"logentry\"] == {\n            \"formatted\": \"hello world\",\n            \"message\": None,\n            \"params\": None,\n        }\n\n    def test_message_attribute_interface_both_strings(self) -> None:\n        manager = EventManager(\n            make_event(**{\"logentry\": \"a plain string\", \"message\": \"another string\"})\n        )\n        manager.normalize()\n        event = manager.save(self.project.id)\n        assert event.data[\"logentry\"] == {\n            \"formatted\": \"a plain string\",\n            \"message\": None,\n            \"params\": None,\n        }\n\n    def test_throws_when_matches_discarded_hash(self) -> None:\n        manager = EventManager(make_event(message=\"foo\", event_id=\"a\" * 32, fingerprint=[\"a\" * 32]))\n        with self.tasks():\n            event = manager.save(self.project.id)\n\n        group = Group.objects.get(id=event.group_id)\n        tombstone = GroupTombstone.objects.create(\n            project_id=group.project_id,\n            level=group.level,\n            message=group.message,\n            culprit=group.culprit,\n            data=group.data,\n            previous_group_id=group.id,\n        )\n        GroupHash.objects.filter(group=group).update(group=None, group_tombstone_id=tombstone.id)\n\n        from sentry.utils.outcomes import track_outcome\n\n        a1 = CachedAttachment(name=\"a1\", data=b\"hello\")\n        a2 = CachedAttachment(name=\"a2\", data=b\"world\")\n\n        for i, event_id in enumerate([\"b\" * 32, \"c\" * 32]):\n            manager = EventManager(\n                make_event(message=\"foo\", event_id=event_id, fingerprint=[\"a\" * 32]),\n                project=self.project,\n            )\n            manager.normalize()\n            discarded_event = Event(\n                project_id=self.project.id, event_id=event_id, data=manager.get_data()\n            )\n\n            cache_key = cache_key_for_event(manager.get_data())\n            attachment_cache.set(cache_key, attachments=[a1, a2])\n\n            mock_track_outcome = mock.Mock(wraps=track_outcome)\n            with (\n                mock.patch(\"sentry.event_manager.track_outcome\", mock_track_outcome),\n                self.feature(\"organizations:event-attachments\"),\n                self.feature(\"organizations:grouptombstones-hit-counter\"),\n                self.tasks(),\n                pytest.raises(HashDiscarded),\n            ):\n                manager.save(self.project.id, cache_key=cache_key, has_attachments=True)\n\n            assert mock_track_outcome.call_count == 3\n\n            event_outcome = mock_track_outcome.mock_calls[0].kwargs\n            assert event_outcome[\"outcome\"] == Outcome.FILTERED\n            assert event_outcome[\"reason\"] == FilterStatKeys.DISCARDED_HASH\n            assert event_outcome[\"category\"] == DataCategory.ERROR\n            assert event_outcome[\"event_id\"] == event_id\n\n            for call in mock_track_outcome.mock_calls[1:]:\n                attachment_outcome = call.kwargs\n                assert attachment_outcome[\"category\"] == DataCategory.ATTACHMENT\n                assert attachment_outcome[\"quantity\"] == 5\n\n            expected_times_seen = 1 + i\n            tombstone.refresh_from_db()\n            assert tombstone.times_seen == expected_times_seen\n            assert tombstone.last_seen == discarded_event.datetime\n\n    def test_honors_crash_report_limit(self) -> None:\n        from sentry.utils.outcomes import track_outcome\n\n        mock_track_outcome = mock.Mock(wraps=track_outcome)\n\n        # Allow exactly one crash report\n        self.project.update_option(\"sentry:store_crash_reports\", 1)\n\n        manager = EventManager(\n            make_event(message=\"foo\", event_id=\"a\" * 32, fingerprint=[\"a\" * 32]),\n            project=self.project,\n        )\n        manager.normalize()\n\n        a1 = CachedAttachment(name=\"a1\", data=b\"hello\", type=\"event.minidump\")\n        a2 = CachedAttachment(name=\"a2\", data=b\"world\")\n        cache_key = cache_key_for_event(manager.get_data())\n        attachment_cache.set(cache_key, attachments=[a1, a2])\n\n        with mock.patch(\"sentry.event_manager.track_outcome\", mock_track_outcome):\n            with self.feature(\"organizations:event-attachments\"):\n                with self.tasks():\n                    manager.save(self.project.id, cache_key=cache_key, has_attachments=True)\n\n        # The first minidump should be accepted, since the limit is 1\n        assert mock_track_outcome.call_count == 3\n        for o in mock_track_outcome.mock_calls:\n            assert o.kwargs[\"outcome\"] == Outcome.ACCEPTED\n\n        mock_track_outcome.reset_mock()\n\n        manager = EventManager(\n            make_event(message=\"foo\", event_id=\"b\" * 32, fingerprint=[\"a\" * 32]),\n            project=self.project,\n        )\n        manager.normalize()\n\n        cache_key = cache_key_for_event(manager.get_data())\n        attachment_cache.set(cache_key, attachments=[a1, a2])\n\n        with mock.patch(\"sentry.event_manager.track_outcome\", mock_track_outcome):\n            with self.feature(\"organizations:event-attachments\"):\n                with self.tasks():\n                    event = manager.save(self.project.id, cache_key=cache_key, has_attachments=True)\n\n        assert event.data[\"metadata\"][\"stripped_crash\"] is True\n\n        assert mock_track_outcome.call_count == 3\n        o = mock_track_outcome.mock_calls[0]\n        assert o.kwargs[\"outcome\"] == Outcome.FILTERED\n        assert o.kwargs[\"category\"] == DataCategory.ATTACHMENT\n        assert o.kwargs[\"reason\"] == FilterStatKeys.CRASH_REPORT_LIMIT\n\n        for o in mock_track_outcome.mock_calls[1:]:\n            assert o.kwargs[\"outcome\"] == Outcome.ACCEPTED\n\n    def test_event_accepted_outcome(self) -> None:\n        manager = EventManager(make_event(message=\"foo\"))\n        manager.normalize()\n\n        mock_track_outcome = mock.Mock()\n        with mock.patch(\"sentry.event_manager.track_outcome\", mock_track_outcome):\n            manager.save(self.project.id)\n\n        assert_mock_called_once_with_partial(\n            mock_track_outcome, outcome=Outcome.ACCEPTED, category=DataCategory.ERROR\n        )\n\n    def test_attachment_accepted_outcomes(self) -> None:\n        manager = EventManager(make_event(message=\"foo\"), project=self.project)\n        manager.normalize()\n\n        a1 = CachedAttachment(name=\"a1\", data=b\"hello\")\n        a2 = CachedAttachment(name=\"a2\", data=b\"limited\", rate_limited=True)\n        a3 = CachedAttachment(name=\"a3\", data=b\"world\")\n\n        cache_key = cache_key_for_event(manager.get_data())\n        attachment_cache.set(cache_key, attachments=[a1, a2, a3])\n\n        mock_track_outcome = mock.Mock()\n        with mock.patch(\"sentry.event_manager.track_outcome\", mock_track_outcome):\n            with self.feature(\"organizations:event-attachments\"):\n                manager.save(self.project.id, cache_key=cache_key, has_attachments=True)\n\n        assert mock_track_outcome.call_count == 3\n\n        for o in mock_track_outcome.mock_calls:\n            assert o.kwargs[\"outcome\"] == Outcome.ACCEPTED\n\n        for o in mock_track_outcome.mock_calls[:2]:\n            assert o.kwargs[\"category\"] == DataCategory.ATTACHMENT\n            assert o.kwargs[\"quantity\"] == 5\n\n        final = mock_track_outcome.mock_calls[2]\n        assert final.kwargs[\"category\"] == DataCategory.ERROR\n\n    def test_attachment_filtered_outcomes(self) -> None:\n        manager = EventManager(make_event(message=\"foo\"), project=self.project)\n        manager.normalize()\n\n        # Disable storing all crash reports, which will drop the minidump but save the other\n        a1 = CachedAttachment(name=\"a1\", data=b\"minidump\", type=\"event.minidump\")\n        a2 = CachedAttachment(name=\"a2\", data=b\"limited\", rate_limited=True)\n        a3 = CachedAttachment(name=\"a3\", data=b\"world\")\n\n        cache_key = cache_key_for_event(manager.get_data())\n        attachment_cache.set(cache_key, attachments=[a1, a2, a3])\n\n        mock_track_outcome = mock.Mock()\n        with mock.patch(\"sentry.event_manager.track_outcome\", mock_track_outcome):\n            with self.feature(\"organizations:event-attachments\"):\n                manager.save(self.project.id, cache_key=cache_key, has_attachments=True)\n\n        assert mock_track_outcome.call_count == 3\n\n        # First outcome is the rejection of the minidump\n        o = mock_track_outcome.mock_calls[0]\n        assert o.kwargs[\"outcome\"] == Outcome.FILTERED\n        assert o.kwargs[\"category\"] == DataCategory.ATTACHMENT\n        assert o.kwargs[\"reason\"] == FilterStatKeys.CRASH_REPORT_LIMIT\n\n        # Second outcome is acceptance of the \"a3\" attachment\n        o = mock_track_outcome.mock_calls[1]\n        assert o.kwargs[\"outcome\"] == Outcome.ACCEPTED\n        assert o.kwargs[\"category\"] == DataCategory.ATTACHMENT\n        assert o.kwargs[\"quantity\"] == 5\n\n        # Last outcome is the event\n        o = mock_track_outcome.mock_calls[2]\n        assert o.kwargs[\"outcome\"] == Outcome.ACCEPTED\n        assert o.kwargs[\"category\"] == DataCategory.ERROR\n\n    def test_transaction_outcome_accepted(self) -> None:\n        \"\"\"\n        Without metrics extraction, we count the number of accepted transaction\n        events in the TRANSACTION data category. This maintains compatibility\n        with Sentry installations that do not have a metrics pipeline.\n        \"\"\"\n\n        timestamp = before_now(minutes=5).isoformat()\n        manager = EventManager(\n            make_event(\n                transaction=\"wait\",\n                contexts={\n                    \"trace\": {\n                        \"parent_span_id\": \"bce14471e0e9654d\",\n                        \"op\": \"foobar\",\n                        \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                        \"span_id\": \"bf5be759039ede9a\",\n                    }\n                },\n                spans=[],\n                timestamp=timestamp,\n                start_timestamp=timestamp,\n                type=\"transaction\",\n                platform=\"python\",\n            )\n        )\n        manager.normalize()\n\n        mock_track_outcome = mock.Mock()\n        with mock.patch(\"sentry.event_manager.track_outcome\", mock_track_outcome):\n            with self.feature({\"organizations:transaction-metrics-extraction\": False}):\n                manager.save(self.project.id)\n\n        assert_mock_called_once_with_partial(\n            mock_track_outcome, outcome=Outcome.ACCEPTED, category=DataCategory.TRANSACTION\n        )\n\n    def test_transaction_indexed_outcome_accepted(self) -> None:\n        \"\"\"\n        With metrics extraction, we count the number of accepted transaction\n        events in the TRANSACTION_INDEXED data category. The TRANSACTION data\n        category contains the number of metrics from\n        ``billing_metrics_consumer``.\n        \"\"\"\n\n        timestamp = before_now(minutes=5).isoformat()\n        manager = EventManager(\n            make_event(\n                transaction=\"wait\",\n                contexts={\n                    \"trace\": {\n                        \"parent_span_id\": \"bce14471e0e9654d\",\n                        \"op\": \"foobar\",\n                        \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                        \"span_id\": \"bf5be759039ede9a\",\n                    }\n                },\n                spans=[],\n                timestamp=timestamp,\n                start_timestamp=timestamp,\n                type=\"transaction\",\n                platform=\"python\",\n            )\n        )\n        manager.normalize()\n\n        mock_track_outcome = mock.Mock()\n        with mock.patch(\"sentry.event_manager.track_outcome\", mock_track_outcome):\n            with self.feature(\"organizations:transaction-metrics-extraction\"):\n                manager.save(self.project.id)\n\n        assert_mock_called_once_with_partial(\n            mock_track_outcome, outcome=Outcome.ACCEPTED, category=DataCategory.TRANSACTION_INDEXED\n        )\n\n    def test_invalid_checksum_gets_hashed(self) -> None:\n        checksum = \"invalid checksum hash\"\n        manager = EventManager(make_event(**{\"checksum\": checksum}))\n        manager.normalize()\n        event = manager.save(self.project.id)\n\n        hashes = [gh.hash for gh in GroupHash.objects.filter(group=event.group)]\n        assert len(hashes) == 1\n        assert hashes[0] == hash_from_values(checksum)\n\n    def test_legacy_attributes_moved(self) -> None:\n        event_params = make_event(\n            release=\"my-release\",\n            environment=\"my-environment\",\n            site=\"whatever\",\n            server_name=\"foo.com\",\n            event_id=uuid.uuid1().hex,\n        )\n        manager = EventManager(event_params)\n        event = manager.save(self.project.id)\n\n        # release and environment stay toplevel\n        assert event.data[\"release\"] == \"my-release\"\n        assert event.data[\"environment\"] == \"my-environment\"\n\n        # site is a legacy attribute that is just a tag\n        assert event.data.get(\"site\") is None\n        tags = dict(event.tags)\n        assert tags[\"site\"] == \"whatever\"\n        assert event.data.get(\"server_name\") is None\n        tags = dict(event.tags)\n        assert tags[\"server_name\"] == \"foo.com\"\n\n    @freeze_time()\n    def test_save_issueless_event(self) -> None:\n        timestamp = before_now(minutes=5).isoformat()\n        manager = EventManager(\n            make_event(\n                transaction=\"wait\",\n                contexts={\n                    \"trace\": {\n                        \"parent_span_id\": \"bce14471e0e9654d\",\n                        \"op\": \"foobar\",\n                        \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                        \"span_id\": \"bf5be759039ede9a\",\n                    }\n                },\n                spans=[],\n                timestamp=timestamp,\n                start_timestamp=timestamp,\n                type=\"transaction\",\n                platform=\"python\",\n            )\n        )\n\n        event = manager.save(self.project.id)\n\n        assert event.group is None\n        assert (\n            tsdb.backend.get_timeseries_sums(\n                TSDBModel.project,\n                [self.project.id],\n                event.datetime,\n                event.datetime,\n                tenant_ids={\"organization_id\": 123, \"referrer\": \"r\"},\n            )[self.project.id]\n            == 0\n        )\n\n    @pytest.mark.skip(reason=\"Flaky test\")\n    def test_category_match_in_app(self) -> None:\n        \"\"\"\n        Regression test to ensure that grouping in-app enhancements work in\n        principle.\n        \"\"\"\n        from sentry.grouping.enhancer import Enhancements\n\n        enhancements_str = Enhancements.from_rules_text(\n            \"\"\"\n            function:foo category=bar\n            function:foo2 category=bar\n            category:bar -app\n            \"\"\"\n        ).base64_string\n\n        grouping_config = {\"id\": DEFAULT_GROUPING_CONFIG, \"enhancements\": enhancements_str}\n\n        with patch(\n            \"sentry.grouping.ingest.hashing.get_grouping_config_dict_for_project\",\n            return_value=grouping_config,\n        ):\n            event_params = make_event(\n                platform=\"native\",\n                exception={\n                    \"values\": [\n                        {\n                            \"type\": \"Hello\",\n                            \"stacktrace\": {\n                                \"frames\": [\n                                    {\n                                        \"function\": \"foo\",\n                                        \"in_app\": True,\n                                    },\n                                    {\"function\": \"bar\"},\n                                ]\n                            },\n                        }\n                    ]\n                },\n            )\n\n            manager = EventManager(event_params)\n            manager.normalize()\n            event1 = manager.save(self.project.id)\n            assert (\n                event1.data[\"exception\"][\"values\"][0][\"stacktrace\"][\"frames\"][0][\"in_app\"] is False\n            )\n\n            event_params = make_event(\n                platform=\"native\",\n                exception={\n                    \"values\": [\n                        {\n                            \"type\": \"Hello\",\n                            \"stacktrace\": {\n                                \"frames\": [\n                                    {\n                                        \"function\": \"foo2\",\n                                        \"in_app\": True,\n                                    },\n                                    {\"function\": \"bar\"},\n                                ]\n                            },\n                        }\n                    ]\n                },\n            )\n\n            manager = EventManager(event_params)\n            manager.normalize()\n            event2 = manager.save(self.project.id)\n            assert (\n                event2.data[\"exception\"][\"values\"][0][\"stacktrace\"][\"frames\"][0][\"in_app\"] is False\n            )\n            assert event1.group_id == event2.group_id\n\n    def test_category_match_group(self) -> None:\n        \"\"\"\n        Regression test to ensure categories are applied consistently and don't\n        produce hash mismatches.\n        \"\"\"\n        from sentry.grouping.enhancer import Enhancements\n\n        enhancements_str = Enhancements.from_rules_text(\n            \"\"\"\n            function:foo category=foo_like\n            category:foo_like -group\n            \"\"\"\n        ).base64_string\n\n        grouping_config: GroupingConfig = {\n            \"id\": DEFAULT_GROUPING_CONFIG,\n            \"enhancements\": enhancements_str,\n        }\n\n        with patch(\n            \"sentry.grouping.ingest.hashing.get_grouping_config_dict_for_project\",\n            return_value=grouping_config,\n        ):\n            event_params = make_event(\n                platform=\"native\",\n                exception={\n                    \"values\": [\n                        {\n                            \"type\": \"Hello\",\n                            \"stacktrace\": {\n                                \"frames\": [\n                                    {\n                                        \"function\": \"foo\",\n                                    },\n                                    {\n                                        \"function\": \"bar\",\n                                    },\n                                ]\n                            },\n                        }\n                    ]\n                },\n            )\n\n            manager = EventManager(event_params)\n            manager.normalize()\n\n            event1 = manager.save(self.project.id)\n            event2 = Event(event1.project_id, event1.event_id, data=event1.data)\n\n            assert event1.get_hashes() == event2.get_hashes(load_grouping_config(grouping_config))\n\n    @override_options({\"performance.issues.all.problem-detection\": 1.0})\n    @override_options({\"performance.issues.n_plus_one_db.problem-creation\": 1.0})\n    def test_perf_issue_creation(self) -> None:\n        with mock.patch(\"sentry_sdk.tracing.Span.containing_transaction\"):\n            event = self.create_performance_issue(\n                event_data=make_event(**get_event(\"n-plus-one-in-django-index-view\"))\n            )\n            data = event.data\n            assert event.get_event_type() == \"transaction\"\n            assert event.transaction == \"/books/\"\n            assert data[\"span_grouping_config\"][\"id\"] == \"default:2022-10-27\"\n            span_hashes = [span[\"hash\"] for span in data[\"spans\"]]\n            assert span_hashes == [\n                \"0f43fb6f6e01ca52\",\n                \"3dc5dd68b38e1730\",\n                \"424c6ae1641f0f0e\",\n                \"d5da18d7274b34a1\",\n                \"ac72fc0a4f5fe381\",\n                \"ac1468d8e11a0553\",\n                \"d8681423cab4275f\",\n                \"e853d2eb7fb9ebb0\",\n                \"6a992d5529f459a4\",\n                \"b640a0ce465fa2a4\",\n                \"a3605e201eaf6c45\",\n                \"061710eb39a66089\",\n                \"c031296784b22ea9\",\n                \"d74ed7012596c3fb\",\n                \"d74ed7012596c3fb\",\n                \"d74ed7012596c3fb\",\n                \"d74ed7012596c3fb\",\n                \"d74ed7012596c3fb\",\n                \"d74ed7012596c3fb\",\n                \"d74ed7012596c3fb\",\n                \"d74ed7012596c3fb\",\n                \"d74ed7012596c3fb\",\n                \"d74ed7012596c3fb\",\n            ]\n            assert event.group\n            group = event.group\n            assert group is not None\n            assert group.title == \"N+1 Query\"\n            assert (\n                group.message\n                == \"/books/ N+1 Query SELECT `books_author`.`id`, `books_author`.`name` FROM `books_author` WHERE `books_author`.`id` = %s LIMIT 21\"\n            )\n            assert group.culprit == \"/books/\"\n            assert group.get_event_type() == \"transaction\"\n            description = \"SELECT `books_author`.`id`, `books_author`.`name` FROM `books_author` WHERE `books_author`.`id` = %s LIMIT 21\"\n            assert group.get_event_metadata() == {\n                \"location\": \"/books/\",\n                \"title\": \"N+1 Query\",\n                \"value\": description,\n                \"initial_priority\": PriorityLevel.LOW,\n            }\n            assert (\n                event.search_message\n                == \"/books/ N+1 Query SELECT `books_author`.`id`, `books_author`.`name` FROM `books_author` WHERE `books_author`.`id` = %s LIMIT 21\"\n            )\n            assert group.location() == \"/books/\"\n            assert group.level == 40\n            assert group.issue_category == GroupCategory.PERFORMANCE\n            assert group.issue_type == PerformanceNPlusOneGroupType\n            assert event.occurrence\n            assert event.occurrence.evidence_display == [\n                IssueEvidence(\n                    name=\"Offending Spans\",\n                    value=\"db - SELECT `books_author`.`id`, `books_author`.`name` \"\n                    \"FROM `books_author` WHERE `books_author`.`id` = %s LIMIT 21\",\n                    important=True,\n                )\n            ]\n            assert event.occurrence.evidence_data == {\n                \"transaction_name\": \"/books/\",\n                \"op\": \"db\",\n                \"parent_span_ids\": [\"8dd7a5869a4f4583\"],\n                \"parent_span\": \"django.view - index\",\n                \"cause_span_ids\": [\"9179e43ae844b174\"],\n                \"offender_span_ids\": [\n                    \"b8be6138369491dd\",\n                    \"b2d4826e7b618f1b\",\n                    \"b3fdeea42536dbf1\",\n                    \"b409e78a092e642f\",\n                    \"86d2ede57bbf48d4\",\n                    \"8e554c84cdc9731e\",\n                    \"94d6230f3f910e12\",\n                    \"a210b87a2191ceb6\",\n                    \"88a5ccaf25b9bd8f\",\n                    \"bb32cf50fc56b296\",\n                ],\n                \"repeating_spans\": \"db - SELECT `books_author`.`id`, `books_author`.`name` FROM `books_author` WHERE `books_author`.`id` = %s LIMIT 21\",\n                \"repeating_spans_compact\": \"SELECT `books_author`.`id`, `books_author`.`name` FROM `books_author` WHERE `books_author`.`id` = %s LIMIT 21\",\n                \"num_repeating_spans\": \"10\",\n            }\n\n    @override_options({\"performance.issues.all.problem-detection\": 1.0})\n    @override_options({\"performance.issues.n_plus_one_db.problem-creation\": 1.0})\n    def test_perf_issue_update(self) -> None:\n        with mock.patch(\"sentry_sdk.tracing.Span.containing_transaction\"):\n            event = self.create_performance_issue(\n                event_data=make_event(**get_event(\"n-plus-one-in-django-index-view\"))\n            )\n            group = event.group\n            assert group is not None\n            assert group.issue_category == GroupCategory.PERFORMANCE\n            assert group.issue_type == PerformanceNPlusOneGroupType\n            group.data[\"metadata\"] = {\n                \"location\": \"hi\",\n                \"title\": \"lol\",\n            }\n            group.culprit = \"wat\"\n            group.message = \"nope\"\n            group.save()\n            assert group.location() == \"hi\"\n            assert group.title == \"lol\"\n\n            with self.tasks():\n                self.create_performance_issue(\n                    event_data=make_event(**get_event(\"n-plus-one-in-django-index-view\"))\n                )\n\n            # Make sure the original group is updated via buffers\n            group.refresh_from_db()\n            assert group.title == \"N+1 Query\"\n\n            assert group.get_event_metadata() == {\n                \"location\": \"/books/\",\n                \"title\": \"N+1 Query\",\n                \"value\": \"SELECT `books_author`.`id`, `books_author`.`name` FROM `books_author` WHERE `books_author`.`id` = %s LIMIT 21\",\n                \"initial_priority\": PriorityLevel.LOW,\n            }\n            assert group.location() == \"/books/\"\n            assert group.message == \"nope\"\n            assert group.culprit == \"/books/\"\n\n    @override_options({\"performance.issues.all.problem-detection\": 1.0})\n    @override_options({\"performance.issues.n_plus_one_db.problem-creation\": 1.0})\n    def test_error_issue_no_associate_perf_event(self) -> None:\n        \"\"\"Test that you can't associate a performance event with an error issue\"\"\"\n        with mock.patch(\"sentry_sdk.tracing.Span.containing_transaction\"):\n            event = self.create_performance_issue(\n                event_data=make_event(**get_event(\"n-plus-one-in-django-index-view\"))\n            )\n            assert event.group is not None\n\n            # sneakily make the group type wrong\n            group = event.group\n            assert group is not None\n            group.type = ErrorGroupType.type_id\n            group.save()\n            event = self.create_performance_issue(\n                event_data=make_event(**get_event(\"n-plus-one-in-django-index-view\"))\n            )\n\n            assert event.group is None\n\n    @override_options({\"performance.issues.all.problem-detection\": 1.0})\n    @override_options({\"performance.issues.n_plus_one_db.problem-creation\": 1.0})\n    def test_perf_issue_no_associate_error_event(self) -> None:\n        \"\"\"Test that you can't associate an error event with a performance issue\"\"\"\n        with mock.patch(\"sentry_sdk.tracing.Span.containing_transaction\"):\n            manager = EventManager(make_event())\n            manager.normalize()\n            event = manager.save(self.project.id)\n            assert len(event.groups) == 1\n\n            # sneakily make the group type wrong\n            group = event.group\n            assert group is not None\n            group.type = PerformanceNPlusOneGroupType.type_id\n            group.save()\n            manager = EventManager(make_event())\n            manager.normalize()\n            event = manager.save(self.project.id)\n\n            assert not event.group\n\n    @override_options({\"performance.issues.all.problem-detection\": 1.0})\n    @override_options({\"performance.issues.n_plus_one_db.problem-creation\": 1.0})\n    def test_perf_issue_creation_ignored(self) -> None:\n        with mock.patch(\"sentry_sdk.tracing.Span.containing_transaction\"):\n            event = self.create_performance_issue(\n                event_data=make_event(**get_event(\"n-plus-one-in-django-index-view\")),\n                noise_limit=2,\n            )\n            assert event.get_event_type() == \"transaction\"\n            assert event.group is None\n\n    @override_options({\"performance.issues.all.problem-detection\": 1.0})\n    @override_options({\"performance.issues.n_plus_one_db.problem-creation\": 1.0})\n    def test_perf_issue_creation_over_ignored_threshold(self) -> None:\n        with mock.patch(\"sentry_sdk.tracing.Span.containing_transaction\"):\n            event_1 = self.create_performance_issue(\n                event_data=make_event(**get_event(\"n-plus-one-in-django-index-view\")), noise_limit=3\n            )\n            event_2 = self.create_performance_issue(\n                event_data=make_event(**get_event(\"n-plus-one-in-django-index-view\")), noise_limit=3\n            )\n            event_3 = self.create_performance_issue(\n                event_data=make_event(**get_event(\"n-plus-one-in-django-index-view\")), noise_limit=3\n            )\n            assert event_1.get_event_type() == \"transaction\"\n            assert event_2.get_event_type() == \"transaction\"\n            assert event_3.get_event_type() == \"transaction\"\n            # only the third occurrence of the hash should create the group\n            assert event_1.group is None\n            assert event_2.group is None\n            assert event_3.group is not None\n\n    @override_options(\n        {\n            \"performance.issues.slow_db_query.problem-creation\": 1.0,\n            \"performance_issue_creation_rate\": 1.0,\n            \"performance.issues.all.problem-detection\": 1.0,\n        }\n    )\n    def test_perf_issue_slow_db_issue_is_created(self) -> None:\n        def attempt_to_generate_slow_db_issue() -> Event:\n            return self.create_performance_issue(\n                event_data=make_event(**get_event(\"slow-db-spans\")),\n                issue_type=PerformanceSlowDBQueryGroupType,\n            )\n\n        last_event = attempt_to_generate_slow_db_issue()\n        assert last_event.group\n        assert last_event.group.type == PerformanceSlowDBQueryGroupType.type_id\n\n    @patch(\"sentry.event_manager.metrics.incr\")\n    def test_new_group_metrics_logging(self, mock_metrics_incr: MagicMock) -> None:\n        manager = EventManager(\n            make_event(platform=\"javascript\", sdk={\"name\": \"sentry.javascript.nextjs\"})\n        )\n        manager.normalize()\n        manager.save(self.project.id)\n\n        mock_metrics_incr.assert_any_call(\n            \"group.created\",\n            skip_internal=True,\n            tags={\n                \"platform\": \"javascript\",\n                \"sdk\": \"sentry.javascript.nextjs\",\n            },\n        )\n\n    @patch(\"sentry.event_manager.metrics.incr\")\n    def test_new_group_metrics_logging_no_platform_no_sdk(\n        self, mock_metrics_incr: MagicMock\n    ) -> None:\n        manager = EventManager(make_event(platform=None, sdk=None))\n        manager.normalize()\n        manager.save(self.project.id)\n\n        mock_metrics_incr.assert_any_call(\n            \"group.created\",\n            skip_internal=True,\n            tags={\n                \"platform\": \"other\",\n                \"sdk\": \"other\",\n            },\n        )\n\n    @patch(\"sentry.event_manager.metrics.incr\")\n    def test_new_group_metrics_logging_sdk_exist_but_null(\n        self, mock_metrics_incr: MagicMock\n    ) -> None:\n        manager = EventManager(make_event(platform=None, sdk={\"name\": None}))\n        manager.normalize()\n        manager.save(self.project.id)\n\n        mock_metrics_incr.assert_any_call(\n            \"group.created\",\n            skip_internal=True,\n            tags={\n                \"platform\": \"other\",\n                \"sdk\": \"other\",\n            },\n        )\n\n    def test_new_group_metrics_logging_with_frame_mix(self) -> None:\n        with patch(\"sentry.event_manager.metrics.incr\") as mock_metrics_incr:\n            manager = EventManager(\n                make_event(platform=\"javascript\", sdk={\"name\": \"sentry.javascript.nextjs\"})\n            )\n            manager.normalize()\n            # IRL, `normalize_stacktraces_for_grouping` adds frame mix metadata to the event, but we\n            # can't mock that because it's imported inside its calling function to avoid circular imports\n            manager._data[\"metadata\"] = {\"in_app_frame_mix\": \"in-app-only\"}\n            manager.save(self.project.id)\n\n            mock_metrics_incr.assert_any_call(\n                \"grouping.in_app_frame_mix\",\n                sample_rate=1.0,\n                tags={\n                    \"platform\": \"javascript\",\n                    \"frame_mix\": \"in-app-only\",\n                    \"sdk\": \"sentry.javascript.nextjs\",\n                },\n            )\n\n    def test_new_group_metrics_logging_without_frame_mix(self) -> None:\n        with patch(\"sentry.event_manager.metrics.incr\") as mock_metrics_incr:\n            manager = EventManager(make_event(platform=\"javascript\"))\n            event = manager.save(self.project.id)\n\n            assert event.get_event_metadata().get(\"in_app_frame_mix\") is None\n\n            metrics_logged = [call.args[0] for call in mock_metrics_incr.mock_calls]\n            assert \"grouping.in_app_frame_mix\" not in metrics_logged\n\n    def test_derive_client_error_sampling_rate_with_option_enabled(self) -> None:\n        \"\"\"Test that sample_rate is extracted from contexts when option is enabled.\"\"\"\n        with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):\n            event_data = make_event(\n                contexts={\"error_sampling\": {\"client_sample_rate\": 0.1}}, platform=\"python\"\n            )\n\n            manager = EventManager(event_data)\n            manager.normalize()\n            event = manager.save(self.project.id)\n\n            # Check that sample_rate was extracted and stored\n            assert event.data[\"sample_rate\"] == 0.1\n\n    def test_derive_client_error_sampling_rate_with_option_disabled(self) -> None:\n        \"\"\"Test that sample_rate is not extracted when option is disabled.\"\"\"\n        # Option disabled (project not in allowlist)\n        event_data = make_event(\n            contexts={\"error_sampling\": {\"client_sample_rate\": 0.1}}, platform=\"python\"\n        )\n\n        manager = EventManager(event_data)\n        manager.normalize()\n        event = manager.save(self.project.id)\n\n        # Check that sample_rate was not extracted\n        assert \"sample_rate\" not in event.data\n\n    def test_derive_client_error_sampling_rate_malformed_context(self) -> None:\n        \"\"\"Test that sample_rate extraction handles malformed error_sampling context gracefully.\"\"\"\n        with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):\n            # Test with error_sampling as a number instead of a dict\n            event_data = make_event(\n                contexts={\"error_sampling\": 0.1},\n                platform=\"python\",  # Should be a dict, not a number\n            )\n\n            manager = EventManager(event_data)\n            manager.normalize()\n            event = manager.save(self.project.id)\n\n            # Check that no sample_rate was added due to malformed context\n            assert \"sample_rate\" not in event.data\n\n    def test_derive_client_error_sampling_rate_invalid_range(self) -> None:\n        \"\"\"Test that sample_rate is not set when client_sample_rate is outside valid range (0-1).\"\"\"\n        with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):\n            # Test with sample rate > 1\n            event_data = make_event(\n                contexts={\"error_sampling\": {\"client_sample_rate\": 1.5}}, platform=\"python\"\n            )\n\n            manager = EventManager(event_data)\n            manager.normalize()\n            event = manager.save(self.project.id)\n\n            # Check that sample_rate was not set due to invalid range\n            assert \"sample_rate\" not in event.data\n\n            # Test with negative sample rate\n            event_data = make_event(\n                contexts={\"error_sampling\": {\"client_sample_rate\": -0.1}}, platform=\"python\"\n            )\n\n            manager = EventManager(event_data)\n            manager.normalize()\n            event = manager.save(self.project.id)\n\n            # Check that sample_rate was not set due to invalid range\n            assert \"sample_rate\" not in event.data\n\n    def test_times_seen_new_group_default_behavior(self) -> None:\n        \"\"\"Test that new groups start with times_seen=1 when no sample rate is provided\"\"\"\n        manager = EventManager(make_event(message=\"test message\"))\n        manager.normalize()\n\n        with self.tasks():\n            event = manager.save(self.project.id)\n\n        group = event.group\n        assert group is not None\n        assert group.times_seen == 1\n\n    def test_times_seen_existing_group_increment(self) -> None:\n        \"\"\"Test that existing groups have their times_seen incremented\"\"\"\n        # Create first event to establish the group\n        manager1 = EventManager(make_event(message=\"test message\", fingerprint=[\"group1\"]))\n        manager1.normalize()\n\n        with self.tasks():\n            event1 = manager1.save(self.project.id)\n\n        group = event1.group\n        assert group is not None\n        initial_times_seen = group.times_seen\n        assert initial_times_seen == 1\n\n        # Create second event for the same group\n        manager2 = EventManager(make_event(message=\"test message 2\", fingerprint=[\"group1\"]))\n        manager2.normalize()\n\n        with self.tasks():\n            event2 = manager2.save(self.project.id)\n\n        # Should be the same group\n        assert event2.group_id == event1.group_id\n\n        # Refresh group from database to get updated times_seen\n        group.refresh_from_db()\n        assert group.times_seen == initial_times_seen + 1\n\n    def test_times_seen_weighted_with_sample_rate_option_enabled(self) -> None:\n        \"\"\"Test that times_seen is weighted by 1/sample_rate when the project is in the allowlist\"\"\"\n\n        with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):\n            # Create event with a sample rate of 0.5 (50%)\n            event_data = make_event(\n                message=\"sampled event\", contexts={\"error_sampling\": {\"client_sample_rate\": 0.5}}\n            )\n\n            manager = EventManager(event_data)\n            manager.normalize()\n\n            with self.tasks():\n                event = manager.save(self.project.id)\n\n            group = event.group\n            assert group is not None\n            # With sample rate 0.5, times_seen should be 1/0.5 = 2\n            assert group.times_seen == 2\n\n    def test_times_seen_weighted_with_sample_rate_option_disabled(self) -> None:\n        \"\"\"Test that times_seen is not weighted when the project is not in the allowlist\"\"\"\n\n        # Create event with a sample rate of 0.5 (50%) but project not in allowlist\n        event_data = make_event(\n            message=\"sampled event\", contexts={\"error_sampling\": {\"client_sample_rate\": 0.5}}\n        )\n\n        manager = EventManager(event_data)\n        manager.normalize()\n\n        with self.tasks():\n            event = manager.save(self.project.id)\n\n        group = event.group\n        assert group is not None\n        # With the project not in allowlist, times_seen should remain 1 regardless of sample rate\n        assert group.times_seen == 1\n\n    def test_times_seen_weighted_existing_group_with_sample_rate(self) -> None:\n        \"\"\"Test that existing groups are incremented by weighted amount when project is in allowlist\"\"\"\n\n        # Create first event to establish the group\n        manager1 = EventManager(make_event(message=\"test message\", fingerprint=[\"group1\"]))\n        manager1.normalize()\n\n        with self.tasks():\n            event1 = manager1.save(self.project.id)\n\n        group = event1.group\n        assert group is not None\n        initial_times_seen = group.times_seen\n        assert initial_times_seen == 1\n\n        with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):\n            # Create second event for the same group with sample rate 0.25 (25%)\n            event_data = make_event(\n                message=\"test message 2\",\n                fingerprint=[\"group1\"],\n                contexts={\"error_sampling\": {\"client_sample_rate\": 0.25}},\n            )\n\n            manager2 = EventManager(event_data)\n            manager2.normalize()\n\n            with self.tasks():\n                event2 = manager2.save(self.project.id)\n\n            # Should be the same group\n            assert event2.group_id == event1.group_id\n\n            # Refresh group from database to get updated times_seen\n            group.refresh_from_db()\n            # Should be incremented by 1/0.25 = 4\n            assert group.times_seen == initial_times_seen + 4\n\n    def test_times_seen_no_sample_rate_meta(self) -> None:\n        \"\"\"Test that times_seen defaults to 1 when no sample rate meta exists\"\"\"\n        with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):\n            # Create event with no error_sampling context\n            manager = EventManager(make_event(fingerprint=[\"no_context\"]))\n            manager.normalize()\n\n            with self.tasks():\n                event = manager.save(self.project.id)\n            assert event.group is not None\n            assert event.group.times_seen == 1\n\n            # Create event with empty error_sampling context\n            manager = EventManager(\n                make_event(fingerprint=[\"empty_context\"], contexts={\"error_sampling\": {}})\n            )\n            manager.normalize()\n\n            with self.tasks():\n                event = manager.save(self.project.id)\n            assert event.group is not None\n            assert event.group.times_seen == 1\n\n            # Create event with null client_sample_rate\n            manager = EventManager(\n                make_event(\n                    fingerprint=[\"null_client_sample_rate\"],\n                    contexts={\"error_sampling\": {\"client_sample_rate\": None}},\n                )\n            )\n            manager.normalize()\n\n            with self.tasks():\n                event = manager.save(self.project.id)\n            assert event.group is not None\n            assert event.group.times_seen == 1\n\n    def test_times_seen_invalid_sample_rate(self) -> None:\n        \"\"\"Test times_seen calculation with invalid sample rates (null, 0, negative, > 1)\"\"\"\n        with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):\n            # Test null sample rate\n            manager = EventManager(make_event(fingerprint=[\"null_sample_rate\"]))\n            manager.normalize()\n\n            with self.tasks():\n                event = manager.save(self.project.id)\n            assert event.group is not None\n            assert event.group.times_seen == 1\n\n            # Test sample rate of 0 (should result in times_seen = 1)\n            manager = EventManager(\n                make_event(\n                    fingerprint=[\"zero_sample_rate\"],\n                    contexts={\"error_sampling\": {\"client_sample_rate\": 0}},\n                )\n            )\n            manager.normalize()\n\n            with self.tasks():\n                event = manager.save(self.project.id)\n            assert event.group is not None\n            assert event.group.times_seen == 1\n\n            # Test negative sample rate (should result in times_seen = 1)\n            manager = EventManager(\n                make_event(\n                    fingerprint=[\"negative_sample_rate\"],\n                    contexts={\"error_sampling\": {\"client_sample_rate\": -0.5}},\n                )\n            )\n            manager.normalize()\n\n            with self.tasks():\n                event = manager.save(self.project.id)\n            assert event.group is not None\n            assert event.group.times_seen == 1\n\n            # Test sample rate > 1 (should result in times_seen = 1)\n            manager = EventManager(\n                make_event(\n                    fingerprint=[\"high_sample_rate\"],\n                    contexts={\"error_sampling\": {\"client_sample_rate\": 1.5}},\n                )\n            )\n            manager.normalize()\n\n            with self.tasks():\n                event = manager.save(self.project.id)\n            assert event.group is not None\n            assert event.group.times_seen == 1\n\n\nclass ReleaseIssueTest(TestCase):\n    def setUp(self) -> None:\n        self.project = self.create_project()\n        self.release = Release.get_or_create(self.project, \"1.0\")\n        self.environment1 = Environment.get_or_create(self.project, \"prod\")\n        self.environment2 = Environment.get_or_create(self.project, \"staging\")\n        self.timestamp = float(int(time() - 300))\n\n    def make_release_event(\n        self,\n        release_version: str = \"1.0\",\n        environment_name: str | None = \"prod\",\n        project_id: int = 1,\n        **kwargs: Any,\n    ) -> Event:\n        event_params = make_event(\n            release=release_version, environment=environment_name, event_id=uuid.uuid1().hex\n        )\n        event_params.update(kwargs)\n        manager = EventManager(event_params)\n        with self.tasks():\n            event = manager.save(project_id)\n        return event\n\n    def convert_timestamp(self, timestamp: float) -> datetime:\n        return datetime.fromtimestamp(timestamp, tz=UTC)\n\n    def assert_release_project_environment(\n        self, event: Event, new_issues_count: int, first_seen: float, last_seen: float\n    ) -> None:\n        release = Release.objects.get(\n            organization=event.project.organization.id, version=event.get_tag(\"sentry:release\")\n        )\n        release_project_envs = ReleaseProjectEnvironment.objects.filter(\n            release=release, project=event.project, environment=event.get_environment()\n        )\n        assert len(release_project_envs) == 1\n\n        release_project_env = release_project_envs[0]\n        assert release_project_env.new_issues_count == new_issues_count\n        assert release_project_env.first_seen == self.convert_timestamp(first_seen)\n        assert release_project_env.last_seen == self.convert_timestamp(last_seen)\n\n    def test_different_groups(self) -> None:\n        event1 = self.make_release_event(\n            release_version=self.release.version,\n            environment_name=self.environment1.name,\n            project_id=self.project.id,\n            checksum=\"a\" * 32,\n            timestamp=self.timestamp,\n        )\n        self.assert_release_project_environment(\n            event=event1, new_issues_count=1, last_seen=self.timestamp, first_seen=self.timestamp\n        )\n\n        event2 = self.make_release_event(\n            release_version=self.release.version,\n            environment_name=self.environment1.name,\n            project_id=self.project.id,\n            checksum=\"b\" * 32,\n            timestamp=self.timestamp + 100,\n        )\n        self.assert_release_project_environment(\n            event=event2,\n            new_issues_count=2,\n            last_seen=self.timestamp + 100,\n            first_seen=self.timestamp,\n        )\n\n    def test_same_group(self) -> None:\n        event1 = self.make_release_event(\n            release_version=self.release.version,\n            environment_name=self.environment1.name,\n            project_id=self.project.id,\n            checksum=\"a\" * 32,\n            timestamp=self.timestamp,\n        )\n        self.assert_release_project_environment(\n            event=event1, new_issues_count=1, last_seen=self.timestamp, first_seen=self.timestamp\n        )\n        event2 = self.make_release_event(\n            release_version=self.release.version,\n            environment_name=self.environment1.name,\n            project_id=self.project.id,\n            checksum=\"a\" * 32,\n            timestamp=self.timestamp + 100,\n        )\n        self.assert_release_project_environment(\n            event=event2,\n            new_issues_count=1,\n            last_seen=self.timestamp + 100,\n            first_seen=self.timestamp,\n        )\n\n    def test_same_group_different_environment(self) -> None:\n        event1 = self.make_release_event(\n            release_version=self.release.version,\n            environment_name=self.environment1.name,\n            project_id=self.project.id,\n            checksum=\"a\" * 32,\n            timestamp=self.timestamp,\n        )\n        self.assert_release_project_environment(\n            event=event1, new_issues_count=1, last_seen=self.timestamp, first_seen=self.timestamp\n        )\n        event2 = self.make_release_event(\n            release_version=self.release.version,\n            environment_name=self.environment2.name,\n            project_id=self.project.id,\n            checksum=\"a\" * 32,\n            timestamp=self.timestamp + 100,\n        )\n        self.assert_release_project_environment(\n            event=event1, new_issues_count=1, last_seen=self.timestamp, first_seen=self.timestamp\n        )\n        self.assert_release_project_environment(\n            event=event2,\n            new_issues_count=1,\n            last_seen=self.timestamp + 100,\n            first_seen=self.timestamp + 100,\n        )\n\n\n@apply_feature_flag_on_cls(\"organizations:dynamic-sampling\")\nclass DSLatestReleaseBoostTest(TestCase):\n    def setUp(self) -> None:\n        self.environment1 = Environment.get_or_create(self.project, \"prod\")\n        self.environment2 = Environment.get_or_create(self.project, \"staging\")\n        self.timestamp = float(int(time() - 300))\n        self.redis_client = get_redis_client_for_ds()\n\n    def make_transaction_event(self, **kwargs: Any) -> dict[str, Any]:\n        result = {\n            \"transaction\": \"wait\",\n            \"contexts\": {\n                \"trace\": {\n                    \"parent_span_id\": \"bce14471e0e9654d\",\n                    \"op\": \"foobar\",\n                    \"trace_id\": \"a0fa8803753e40fd8124b21eeb2986b5\",\n                    \"span_id\": \"bf5be759039ede9a\",\n                }\n            },\n            \"spans\": [],\n            \"timestamp\": self.timestamp + 0.23,\n            \"start_timestamp\": \"2019-06-14T14:01:40Z\",\n            \"type\": \"transaction\",\n        }\n        result.update(kwargs)\n        return result\n\n    def make_release_transaction(\n        self,\n        release_version: str = \"1.0\",\n        environment_name: str | None = \"prod\",\n        project_id: int = 1,\n        **kwargs: Any,\n    ) -> Event:\n        transaction = (\n            self.make_transaction_event(\n                release=release_version, environment=environment_name, event_id=uuid.uuid1().hex\n            )\n            if environment_name is not None\n            else self.make_transaction_event(release=release_version, event_id=uuid.uuid1().hex)\n        )\n        transaction.update(kwargs)\n        manager = EventManager(transaction)\n        with self.tasks():\n            event = manager.save(project_id)\n        return event\n\n    @freeze_time(\"2022-11-03 10:00:00\")\n    def test_boost_release_with_non_observed_release(self) -> None:\n        ts = timezone.now().timestamp()\n\n        project = self.create_project(platform=\"python\")\n        release_1 = Release.get_or_create(project=project, version=\"1.0\", date_added=timezone.now())\n        release_2 = Release.get_or_create(\n            project=project, version=\"2.0\", date_added=timezone.now() + timedelta(hours=1)\n        )\n        release_3 = Release.get_or_create(\n            project=project, version=\"3.0\", date_added=timezone.now() + timedelta(hours=2)\n        )\n\n        for release, environment in (\n            (release_1, None),\n            (release_2, \"prod\"),\n            (release_3, \"dev\"),\n        ):\n            self.make_release_transaction(\n                release_version=release.version,\n                environment_name=environment,\n                project_id=project.id,\n                checksum=\"a\" * 32,\n                timestamp=self.timestamp,\n            )\n\n            env_postfix = f\":e:{environment}\" if environment is not None else \"\"\n            assert self.redis_client.get(f\"ds::p:{project.id}:r:{release.id}{env_postfix}\") == \"1\"\n\n        assert self.redis_client.hgetall(f\"ds::p:{project.id}:boosted_releases\") == {\n            f\"ds::r:{release_1.id}\": str(ts),\n            f\"ds::r:{release_2.id}:e:prod\": str(ts),\n            f\"ds::r:{release_3.id}:e:dev\": str(ts),\n        }\n        assert ProjectBoostedReleases(project_id=project.id).get_extended_boosted_releases() == [\n            ExtendedBoostedRelease(\n                id=release_1.id,\n                timestamp=ts,\n                environment=None,\n                cache_key=f\"ds::r:{release_1.id}\",\n                version=release_1.version,\n                platform=Platform(project.platform),\n            ),\n            ExtendedBoostedRelease(\n                id=release_2.id,\n                timestamp=ts,\n                environment=\"prod\",\n                cache_key=f\"ds::r:{release_2.id}:e:prod\",\n                version=release_2.version,\n                platform=Platform(project.platform),\n            ),\n            ExtendedBoostedRelease(\n                id=release_3.id,\n                timestamp=ts,\n                environment=\"dev\",\n                cache_key=f\"ds::r:{release_3.id}:e:dev\",\n                version=release_3.version,\n                platform=Platform(project.platform),\n            ),\n        ]\n\n    @freeze_time(\"2022-11-03 10:00:00\")\n    def test_boost_release_boosts_only_latest_release(self) -> None:\n        ts = timezone.now().timestamp()\n\n        project = self.create_project(platform=\"python\")\n        release_1 = Release.get_or_create(project=project, version=\"1.0\", date_added=timezone.now())\n        release_2 = Release.get_or_create(\n            project=project,\n            version=\"2.0\",\n            # We must make sure the new release_2.date_added > release_1.date_added.\n            date_added=timezone.now() + timedelta(hours=1),\n        )\n\n        # We add a transaction for latest release release_2.\n        self.make_release_transaction(\n            release_version=release_2.version,\n            environment_name=self.environment1.name,\n            project_id=project.id,\n            checksum=\"a\" * 32,\n            timestamp=self.timestamp,\n        )\n\n        # We add a transaction for release_1 which is not anymore the latest release, therefore we should skip this.\n        self.make_release_transaction(\n            release_version=release_1.version,\n            environment_name=self.environment1.name,\n            project_id=project.id,\n            checksum=\"a\" * 32,\n            timestamp=self.timestamp,\n        )\n\n        assert (\n            self.redis_client.get(f\"ds::p:{project.id}:r:{release_2.id}:e:{self.environment1.name}\")\n            == \"1\"\n        )\n        assert self.redis_client.hgetall(f\"ds::p:{project.id}:boosted_releases\") == {\n            f\"ds::r:{release_2.id}:e:{self.environment1.name}\": str(ts),\n        }\n        assert ProjectBoostedReleases(project_id=project.id).get_extended_boosted_releases() == [\n            ExtendedBoostedRelease(\n                id=release_2.id,\n                timestamp=ts,\n                environment=self.environment1.name,\n                cache_key=f\"ds::r:{release_2.id}:e:{self.environment1.name}\",\n                version=release_2.version,\n                platform=Platform(project.platform),\n            )\n        ]\n\n    @freeze_time(\"2022-11-03 10:00:00\")\n    def test_boost_release_with_observed_release_and_different_environment(self) -> None:\n        project = self.create_project(platform=\"python\")\n        release = Release.get_or_create(project=project, version=\"1.0\", date_added=timezone.now())\n\n        self.make_release_transaction(\n            release_version=release.version,\n            environment_name=self.environment1.name,\n            project_id=project.id,\n            checksum=\"a\" * 32,\n            timestamp=self.timestamp,\n        )\n\n        ts_1 = time()\n\n        assert (\n            self.redis_client.get(f\"ds::p:{project.id}:r:{release.id}:e:{self.environment1.name}\")\n            == \"1\"\n        )\n        assert self.redis_client.hgetall(f\"ds::p:{project.id}:boosted_releases\") == {\n            f\"ds::r:{release.id}:e:{self.environment1.name}\": str(ts_1)\n        }\n        assert ProjectBoostedReleases(project_id=project.id).get_extended_boosted_releases() == [\n            ExtendedBoostedRelease(\n                id=release.id,\n                timestamp=ts_1,\n                environment=self.environment1.name,\n                cache_key=f\"ds::r:{release.id}:e:{self.environment1.name}\",\n                version=release.version,\n                platform=Platform(project.platform),\n            )\n        ]\n\n        # We simulate that a new transaction with same release but with a different environment value comes after\n        # 30 minutes to show that we expect the entry for that release-env to be added to the boosted releases.\n        with freeze_time(\"2022-11-03 10:30:00\"):\n            self.make_release_transaction(\n                release_version=release.version,\n                environment_name=self.environment2.name,\n                project_id=project.id,\n                checksum=\"b\" * 32,\n                timestamp=self.timestamp,\n            )\n\n            ts_2 = time()\n\n            assert (\n                self.redis_client.get(\n                    f\"ds::p:{project.id}:r:{release.id}:e:{self.environment2.name}\"\n                )\n                == \"1\"\n            )\n            assert self.redis_client.hgetall(f\"ds::p:{project.id}:boosted_releases\") == {\n                f\"ds::r:{release.id}:e:{self.environment1.name}\": str(ts_1),\n                f\"ds::r:{release.id}:e:{self.environment2.name}\": str(ts_2),\n            }\n            assert ProjectBoostedReleases(\n                project_id=project.id\n            ).get_extended_boosted_releases() == [\n                ExtendedBoostedRelease(\n                    id=release.id,\n                    timestamp=ts_1,\n                    environment=self.environment1.name,\n                    cache_key=f\"ds::r:{release.id}:e:{self.environment1.name}\",\n                    version=release.version,\n                    platform=Platform(project.platform),\n                ),\n                ExtendedBoostedRelease(\n                    id=release.id,\n                    timestamp=ts_2,\n                    environment=self.environment2.name,\n                    cache_key=f\"ds::r:{release.id}:e:{self.environment2.name}\",\n                    version=release.version,\n                    platform=Platform(project.platform),\n                ),\n            ]\n\n        # We also test the case in which no environment is set, which can be the case as per\n        # https://docs.sentry.io/platforms/javascript/configuration/options/#environment.\n        with freeze_time(\"2022-11-03 11:00:00\"):\n            self.make_release_transaction(\n                release_version=release.version,\n                environment_name=None,\n                project_id=project.id,\n                checksum=\"b\" * 32,\n                timestamp=self.timestamp,\n            )\n\n            ts_3 = time()\n\n            assert self.redis_client.get(f\"ds::p:{project.id}:r:{release.id}\") == \"1\"\n            assert self.redis_client.hgetall(f\"ds::p:{project.id}:boosted_releases\") == {\n                f\"ds::r:{release.id}:e:{self.environment1.name}\": str(ts_1),\n                f\"ds::r:{release.id}:e:{self.environment2.name}\": str(ts_2),\n                f\"ds::r:{release.id}\": str(ts_3),\n            }\n            assert ProjectBoostedReleases(\n                project_id=project.id\n            ).get_extended_boosted_releases() == [\n                ExtendedBoostedRelease(\n                    id=release.id,\n                    timestamp=ts_1,\n                    environment=self.environment1.name,\n                    cache_key=f\"ds::r:{release.id}:e:{self.environment1.name}\",\n                    version=release.version,\n                    platform=Platform(project.platform),\n                ),\n                ExtendedBoostedRelease(\n                    id=release.id,\n                    timestamp=ts_2,\n                    environment=self.environment2.name,\n                    cache_key=f\"ds::r:{release.id}:e:{self.environment2.name}\",\n                    version=release.version,\n                    platform=Platform(project.platform),\n                ),\n                ExtendedBoostedRelease(\n                    id=release.id,\n                    timestamp=ts_3,\n                    environment=None,\n                    cache_key=f\"ds::r:{release.id}\",\n                    version=release.version,\n                    platform=Platform(project.platform),\n                ),\n            ]\n\n    @freeze_time(\"2022-11-03 10:00:00\")\n    def test_release_not_boosted_with_observed_release_and_same_environment(self) -> None:\n        project = self.create_project(platform=\"python\")\n        release = Release.get_or_create(project=project, version=\"1.0\", date_added=timezone.now())\n\n        for environment in (self.environment1.name, self.environment2.name):\n            self.redis_client.set(\n                f\"ds::p:{project.id}:r:{release.id}:e:{environment}\", 1, 60 * 60 * 24\n            )\n            self.make_release_transaction(\n                release_version=release.version,\n                environment_name=environment,\n                project_id=project.id,\n                checksum=\"b\" * 32,\n                timestamp=self.timestamp,\n            )\n\n        assert self.redis_client.hgetall(f\"ds::p:{project.id}:boosted_releases\") == {}\n        assert ProjectBoostedReleases(project_id=project.id).get_extended_boosted_releases() == []\n\n    @freeze_time(\"2022-11-03 10:00:00\")\n    def test_release_not_boosted_with_deleted_release_after_event_received(self) -> None:\n        ts = timezone.now().timestamp()\n\n        project = self.create_project(platform=\"python\")\n        release_1 = Release.get_or_create(project=project, version=\"1.0\", date_added=timezone.now())\n        release_2 = Release.get_or_create(\n            project=project, version=\"2.0\", date_added=timezone.now() + timedelta(hours=1)\n        )\n\n        self.make_release_transaction(\n            release_version=release_1.version,\n            environment_name=None,\n            project_id=project.id,\n            checksum=\"a\" * 32,\n            timestamp=self.timestamp,\n        )\n        assert self.redis_client.get(f\"ds::p:{project.id}:r:{release_1.id}\") == \"1\"\n\n        self.make_release_transaction(\n            release_version=release_2.version,\n            environment_name=None,\n            project_id=project.id,\n            checksum=\"a\" * 32,\n            timestamp=self.timestamp,\n        )\n        assert self.redis_client.get(f\"ds::p:{project.id}:r:{release_2.id}\") == \"1\"\n\n        # We simulate that the release_2 is deleted after the boost has been inserted.\n        release_2_id = release_2.id\n        release_2.delete()\n\n        # We expect the boosted release to be kept in Redis, if not queried by the ProjectBoostedReleases.\n        assert self.redis_client.hgetall(f\"ds::p:{project.id}:boosted_releases\") == {\n            f\"ds::r:{release_1.id}\": str(ts),\n            f\"ds::r:{release_2_id}\": str(ts),\n        }\n        # We expect to not see the release 2 because it will not be in the database anymore, thus we mark it as\n        # expired.\n        assert ProjectBoostedReleases(project_id=project.id).get_extended_boosted_releases() == [\n            ExtendedBoostedRelease(\n                id=release_1.id,\n                timestamp=ts,\n                environment=None,\n                cache_key=f\"ds::r:{release_1.id}\",\n                version=release_1.version,\n                platform=Platform(project.platform),\n            ),\n        ]\n\n    @freeze_time(\"2022-11-03 10:00:00\")\n    def test_get_boosted_releases_with_old_and_new_cache_keys(self) -> None:\n        ts = timezone.now().timestamp()\n\n        project = self.create_project(platform=\"python\")\n\n        # Old cache key\n        release_1 = Release.get_or_create(project=project, version=\"1.0\", date_added=timezone.now())\n        self.redis_client.hset(\n            f\"ds::p:{project.id}:boosted_releases\",\n            f\"{release_1.id}\",\n            ts,\n        )\n\n        # New cache key\n        release_2 = Release.get_or_create(\n            project=project, version=\"2.0\", date_added=timezone.now() + timedelta(hours=1)\n        )\n        self.redis_client.hset(\n            f\"ds::p:{project.id}:boosted_releases\",\n            f\"ds::r:{release_2.id}\",\n            ts,\n        )\n        self.redis_client.hset(\n            f\"ds::p:{project.id}:boosted_releases\",\n            f\"ds::r:{release_2.id}:e:{self.environment1.name}\",\n            ts,\n        )\n        self.redis_client.hset(\n            f\"ds::p:{project.id}:boosted_releases\",\n            f\"ds::r:{release_2.id}:e:{self.environment2.name}\",\n            ts,\n        )\n\n        assert ProjectBoostedReleases(project_id=project.id).get_extended_boosted_releases() == [\n            ExtendedBoostedRelease(\n                id=release_1.id,\n                timestamp=ts,\n                environment=None,\n                # This item has the old cache key.\n                cache_key=f\"{release_1.id}\",\n                version=release_1.version,\n                platform=Platform(project.platform),\n            ),\n            ExtendedBoostedRelease(\n                id=release_2.id,\n                timestamp=ts,\n                environment=None,\n                cache_key=f\"ds::r:{release_2.id}\",\n                version=release_2.version,\n                platform=Platform(project.platform),\n            ),\n            ExtendedBoostedRelease(\n                id=release_2.id,\n                timestamp=ts,\n                environment=self.environment1.name,\n                cache_key=f\"ds::r:{release_2.id}:e:{self.environment1.name}\",\n                version=release_2.version,\n                platform=Platform(project.platform),\n            ),\n            ExtendedBoostedRelease(\n                id=release_2.id,\n                timestamp=ts,\n                environment=self.environment2.name,\n                cache_key=f\"ds::r:{release_2.id}:e:{self.environment2.name}\",\n                version=release_2.version,\n                platform=Platform(project.platform),\n            ),\n        ]\n\n    @freeze_time(\"2022-11-03 10:00:00\")\n    def test_expired_boosted_releases_are_removed(self) -> None:\n        ts = timezone.now().timestamp()\n\n        # We want to test with multiple platforms.\n        for platform in (\"python\", \"java\", None):\n            project = self.create_project(platform=platform)\n\n            for index, (release_version, environment) in enumerate(\n                (\n                    (f\"1.0-{platform}\", self.environment1.name),\n                    (f\"2.0-{platform}\", self.environment2.name),\n                )\n            ):\n                release = Release.get_or_create(\n                    project=project,\n                    version=release_version,\n                    date_added=timezone.now() + timedelta(hours=index),\n                )\n                self.redis_client.set(\n                    f\"ds::p:{project.id}:r:{release.id}:e:{environment}\", 1, 60 * 60 * 24\n                )\n                self.redis_client.hset(\n                    f\"ds::p:{project.id}:boosted_releases\",\n                    f\"ds::r:{release.id}:e:{environment}\",\n                    # We set the creation time in order to expire it by 1 second.\n                    ts - Platform(platform).time_to_adoption - 1,\n                )\n\n            # We add a new boosted release that is not expired.\n            release_3 = Release.get_or_create(\n                project=project,\n                version=f\"3.0-{platform}\",\n                date_added=timezone.now() + timedelta(hours=2),\n            )\n            self.make_release_transaction(\n                release_version=release_3.version,\n                environment_name=self.environment1.name,\n                project_id=project.id,\n                checksum=\"b\" * 32,\n                timestamp=self.timestamp,\n            )\n\n            assert (\n                self.redis_client.get(\n                    f\"ds::p:{project.id}:r:{release_3.id}:e:{self.environment1.name}\"\n                )\n                == \"1\"\n            )\n            assert self.redis_client.hgetall(f\"ds::p:{project.id}:boosted_releases\") == {\n                f\"ds::r:{release_3.id}:e:{self.environment1.name}\": str(ts)\n            }\n            assert ProjectBoostedReleases(\n                project_id=project.id\n            ).get_extended_boosted_releases() == [\n                ExtendedBoostedRelease(\n                    id=release_3.id,\n                    timestamp=ts,\n                    environment=self.environment1.name,\n                    cache_key=f\"ds::r:{release_3.id}:e:{self.environment1.name}\",\n                    version=release_3.version,\n                    platform=Platform(project.platform),\n                )\n            ]\n\n    @mock.patch(\n        \"sentry.dynamic_sampling.rules.helpers.latest_releases.schedule_invalidate_project_config\"\n    )\n    def test_project_config_invalidation_is_triggered_when_new_release_is_observed(\n        self, mocked_invalidate: mock.MagicMock\n    ) -> None:\n        self.make_release_transaction(\n            release_version=self.release.version,\n            environment_name=self.environment1.name,\n            project_id=self.project.id,\n            checksum=\"a\" * 32,\n            timestamp=self.timestamp,\n        )\n        assert any(\n            o.kwargs[\"trigger\"] == \"dynamic_sampling:boost_release\"\n            for o in mocked_invalidate.mock_calls\n        )\n\n    @freeze_time(\"2022-11-03 10:00:00\")\n    @mock.patch(\"sentry.dynamic_sampling.rules.helpers.latest_releases.BOOSTED_RELEASES_LIMIT\", 2)\n    def test_least_recently_boosted_release_is_removed_if_limit_is_exceeded(self) -> None:\n        ts = timezone.now().timestamp()\n\n        project = self.create_project(platform=\"python\")\n        release_1 = Release.get_or_create(\n            project=project,\n            version=\"1.0\",\n            date_added=timezone.now(),\n        )\n        release_2 = Release.get_or_create(\n            project=project,\n            version=\"2.0\",\n            date_added=timezone.now() + timedelta(hours=1),\n        )\n\n        # We boost with increasing timestamps, so that we know that the smallest will be evicted.\n        for release, boost_time in ((release_1, ts - 2), (release_2, ts - 1)):\n            self.redis_client.set(\n                f\"ds::p:{project.id}:r:{release.id}\",\n                1,\n                60 * 60 * 24,\n            )\n            self.redis_client.hset(\n                f\"ds::p:{project.id}:boosted_releases\",\n                f\"ds::r:{release.id}\",\n                boost_time,\n            )\n\n        release_3 = Release.get_or_create(\n            project=project,\n            version=\"3.0\",\n            date_added=timezone.now() + timedelta(hours=2),\n        )\n        self.make_release_transaction(\n            release_version=release_3.version,\n            environment_name=self.environment1.name,\n            project_id=project.id,\n            checksum=\"b\" * 32,\n            timestamp=self.timestamp,\n        )\n\n        assert (\n            self.redis_client.get(f\"ds::p:{project.id}:r:{release_3.id}:e:{self.environment1.name}\")\n            == \"1\"\n        )\n        assert self.redis_client.hgetall(f\"ds::p:{project.id}:boosted_releases\") == {\n            f\"ds::r:{release_2.id}\": str(ts - 1),\n            f\"ds::r:{release_3.id}:e:{self.environment1.name}\": str(ts),\n        }\n        assert ProjectBoostedReleases(project_id=project.id).get_extended_boosted_releases() == [\n            ExtendedBoostedRelease(\n                id=release_2.id,\n                timestamp=ts - 1,\n                environment=None,\n                cache_key=f\"ds::r:{release_2.id}\",\n                version=release_2.version,\n                platform=Platform(project.platform),\n            ),\n            ExtendedBoostedRelease(\n                id=release_3.id,\n                timestamp=ts,\n                environment=self.environment1.name,\n                cache_key=f\"ds::r:{release_3.id}:e:{self.environment1.name}\",\n                version=release_3.version,\n                platform=Platform(project.platform),\n            ),\n        ]\n\n    @freeze_time()\n    @mock.patch(\"sentry.dynamic_sampling.rules.helpers.latest_releases.BOOSTED_RELEASES_LIMIT\", 2)\n    def test_removed_boost_not_added_again_if_limit_is_exceeded(self) -> None:\n        ts = timezone.now().timestamp()\n\n        project = self.create_project(platform=\"python\")\n        release_1 = Release.get_or_create(project=project, version=\"1.0\", date_added=timezone.now())\n\n        # We want to test that if we have the same release, but we send different environments that go over the\n        # limit, and we evict an environment, but then we send a transaction with the evicted environment.\n        #\n        # As an example suppose the following history of transactions received in the form (release, env) -> None:\n        # (1, production) -> (1, staging) -> (1, None) -> (1, production)\n        #\n        # Once we receive the first two, we have reached maximum capacity. Then we receive (1, None) and evict boost\n        # for (1, production) which results in the following boosts (1, staging), (1, None). After that we receive\n        # (1, production) again but in this case we don't want to remove (1, staging) because we will end up in an\n        # infinite loop. Instead, we expect to mark (1, production) as observed and only un-observe it if it does\n        # not receive transactions within the next 24 hours.\n        environments_sequence = [\n            self.environment1.name,\n            self.environment2.name,\n            None,\n            self.environment1.name,\n        ]\n        for environment in environments_sequence:\n            self.make_release_transaction(\n                release_version=release_1.version,\n                environment_name=environment,\n                project_id=project.id,\n                checksum=\"b\" * 32,\n                timestamp=self.timestamp,\n            )\n\n        # We assert that all environments have been observed.\n        assert (\n            self.redis_client.get(f\"ds::p:{project.id}:r:{release_1.id}:e:{self.environment1.name}\")\n            == \"1\"\n        )\n        assert (\n            self.redis_client.get(f\"ds::p:{project.id}:r:{release_1.id}:e:{self.environment2.name}\")\n            == \"1\"\n        )\n        assert self.redis_client.get(f\"ds::p:{project.id}:r:{release_1.id}\") == \"1\"\n\n        # We assert that only the last 2 unseen (release, env) pairs are boosted.\n        assert self.redis_client.hgetall(f\"ds::p:{project.id}:boosted_releases\") == {\n            f\"ds::r:{release_1.id}:e:{self.environment2.name}\": str(ts),\n            f\"ds::r:{release_1.id}\": str(ts),\n        }\n        assert ProjectBoostedReleases(project_id=project.id).get_extended_boosted_releases() == [\n            ExtendedBoostedRelease(\n                id=release_1.id,\n                timestamp=ts,\n                environment=self.environment2.name,\n                cache_key=f\"ds::r:{release_1.id}:e:{self.environment2.name}\",\n                version=release_1.version,\n                platform=Platform(project.platform),\n            ),\n            ExtendedBoostedRelease(\n                id=release_1.id,\n                timestamp=ts,\n                environment=None,\n                cache_key=f\"ds::r:{release_1.id}\",\n                version=release_1.version,\n                platform=Platform(project.platform),\n            ),\n        ]\n\n\nclass TestSaveGroupHashAndGroup(TransactionTestCase):\n    def test(self) -> None:\n        perf_data = load_data(\"transaction-n-plus-one\", timestamp=before_now(minutes=10))\n        event = _get_event_instance(perf_data, project_id=self.project.id)\n        group_hash = \"some_group\"\n        group, created, _ = save_grouphash_and_group(self.project, event, group_hash)\n        assert created\n        group_2, created, _ = save_grouphash_and_group(self.project, event, group_hash)\n        assert group.id == group_2.id\n        assert not created\n        assert Group.objects.filter(grouphash__hash=group_hash).count() == 1\n        group_3, created, _ = save_grouphash_and_group(self.project, event, \"new_hash\")\n        assert created\n        assert group_2.id != group_3.id\n        assert Group.objects.filter(grouphash__hash=group_hash).count() == 1\n\n\nexample_transaction_event = {\n    \"type\": \"transaction\",\n    \"timestamp\": datetime.now().isoformat(),\n    \"start_timestamp\": (datetime.now() - timedelta(seconds=1)).isoformat(),\n    \"spans\": [],\n    \"contexts\": {\n        \"trace\": {\n            \"parent_span_id\": \"8988cec7cc0779c1\",\n            \"type\": \"trace\",\n            \"op\": \"foobar\",\n            \"trace_id\": \"a7d67cf796774551a95be6543cacd459\",\n            \"span_id\": \"babaae0d4b7512d9\",\n            \"status\": \"ok\",\n        }\n    },\n}\n\n\nexample_error_event = {\n    \"event_id\": \"80e3496eff734ab0ac993167aaa0d1cd\",\n    \"release\": \"5.222.5\",\n    \"type\": \"error\",\n    \"level\": \"fatal\",\n    \"platform\": \"cocoa\",\n    \"tags\": {\"level\": \"fatal\"},\n    \"environment\": \"test-app\",\n    \"sdk\": {\n        \"name\": \"sentry.cocoa\",\n        \"version\": \"8.2.0\",\n        \"integrations\": [\n            \"Crash\",\n            \"PerformanceTracking\",\n            \"MetricKit\",\n            \"WatchdogTerminationTracking\",\n            \"ViewHierarchy\",\n            \"NetworkTracking\",\n            \"ANRTracking\",\n            \"AutoBreadcrumbTracking\",\n            \"FramesTracking\",\n            \"AppStartTracking\",\n            \"Screenshot\",\n            \"FileIOTracking\",\n            \"UIEventTracking\",\n            \"AutoSessionTracking\",\n            \"CoreDataTracking\",\n            \"PreWarmedAppStartTracing\",\n        ],\n    },\n    \"user\": {\n        \"id\": \"803F5C87-0F8B-41C7-8499-27BD71A92738\",\n        \"ip_address\": \"192.168.0.1\",\n        \"geo\": {\"country_code\": \"US\", \"region\": \"United States\"},\n    },\n    \"logger\": \"my.logger.name\",\n}\n\n\n@pytest.mark.parametrize(\n    \"event_data,expected_type\",\n    [\n        pytest.param(\n            example_transaction_event,\n            \"transactions\",\n            id=\"transactions\",\n        ),\n        pytest.param(\n            example_error_event,\n            \"errors\",\n            id=\"errors\",\n        ),\n    ],\n)\n@django_db_all\ndef test_cogs_event_manager(\n    default_project: int, event_data: Mapping[str, Any], expected_type: str\n) -> None:\n    storage: MemoryMessageStorage[KafkaPayload] = MemoryMessageStorage()\n    broker = LocalBroker(storage)\n    topic = Topic(\"shared-resources-usage\")\n    broker.create_topic(topic, 1)\n    producer = broker.get_producer()\n\n    with (\n        override_options(\n            {\"shared_resources_accounting_enabled\": [settings.COGS_EVENT_STORE_LABEL]}\n        ),\n        usage_accountant_backend(producer),\n    ):\n        raw_event_params = make_event(**event_data)\n\n        manager = EventManager(raw_event_params)\n        manager.normalize()\n        normalized_data = dict(manager.get_data())\n        _ = manager.save(default_project)\n\n        expected_len = len(json.dumps(normalized_data))\n\n    msg1 = broker.consume(Partition(topic, 0), 0)\n    assert msg1 is not None\n    payload = msg1.payload\n    assert payload is not None\n    formatted = json.loads(payload.value.decode(\"utf-8\"))\n    assert formatted[\"shared_resource_id\"] == settings.COGS_EVENT_STORE_LABEL\n    assert formatted[\"app_feature\"] == expected_type\n    assert formatted[\"usage_unit\"] == \"bytes\"\n    # We cannot assert for exact length because manager save method adds some extra fields. So we\n    # assert that the length is at least greater than the expected length.\n    assert formatted[\"amount\"] >= expected_len\n",
    "file_path": "/Users/wangyue/Code/CodeReviewData/ReviewDataset/sentry-greptile/tests/sentry/event_manager/test_event_manager.py",
    "line_count": 4050,
    "encoding": "utf-8",
    "error": null
}

Message 17 [Assistant]:
--------------------------------------------------------------------------------
Content:
让我搜索`test_derive_client_error_sampling_rate_with_option_enabled`测试：
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "test_derive_client_error_sampling_rate_with_option_enabled",
      "is_regex": false,
      "case_sensitive": true,
      "context_lines": 50
    },
    "id": "call_00_LhE1jH96YgLkTznwpNxSB7Wi",
    "type": "tool_call"
  }
]

Message 18 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_LhE1jH96YgLkTznwpNxSB7Wi
Result:
{
    "matches": [
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2879,
            "matched_line": "    def test_derive_client_error_sampling_rate_with_option_enabled(self) -> None:",
            "context_start_line": 2829,
            "context_end_line": 2929,
            "context": [
                "2829:         )",
                "2830: ",
                "2831:     @patch(\"sentry.event_manager.metrics.incr\")",
                "2832:     def test_new_group_metrics_logging_sdk_exist_but_null(",
                "2833:         self, mock_metrics_incr: MagicMock",
                "2834:     ) -> None:",
                "2835:         manager = EventManager(make_event(platform=None, sdk={\"name\": None}))",
                "2836:         manager.normalize()",
                "2837:         manager.save(self.project.id)",
                "2838: ",
                "2839:         mock_metrics_incr.assert_any_call(",
                "2840:             \"group.created\",",
                "2841:             skip_internal=True,",
                "2842:             tags={",
                "2843:                 \"platform\": \"other\",",
                "2844:                 \"sdk\": \"other\",",
                "2845:             },",
                "2846:         )",
                "2847: ",
                "2848:     def test_new_group_metrics_logging_with_frame_mix(self) -> None:",
                "2849:         with patch(\"sentry.event_manager.metrics.incr\") as mock_metrics_incr:",
                "2850:             manager = EventManager(",
                "2851:                 make_event(platform=\"javascript\", sdk={\"name\": \"sentry.javascript.nextjs\"})",
                "2852:             )",
                "2853:             manager.normalize()",
                "2854:             # IRL, `normalize_stacktraces_for_grouping` adds frame mix metadata to the event, but we",
                "2855:             # can't mock that because it's imported inside its calling function to avoid circular imports",
                "2856:             manager._data[\"metadata\"] = {\"in_app_frame_mix\": \"in-app-only\"}",
                "2857:             manager.save(self.project.id)",
                "2858: ",
                "2859:             mock_metrics_incr.assert_any_call(",
                "2860:                 \"grouping.in_app_frame_mix\",",
                "2861:                 sample_rate=1.0,",
                "2862:                 tags={",
                "2863:                     \"platform\": \"javascript\",",
                "2864:                     \"frame_mix\": \"in-app-only\",",
                "2865:                     \"sdk\": \"sentry.javascript.nextjs\",",
                "2866:                 },",
                "2867:             )",
                "2868: ",
                "2869:     def test_new_group_metrics_logging_without_frame_mix(self) -> None:",
                "2870:         with patch(\"sentry.event_manager.metrics.incr\") as mock_metrics_incr:",
                "2871:             manager = EventManager(make_event(platform=\"javascript\"))",
                "2872:             event = manager.save(self.project.id)",
                "2873: ",
                "2874:             assert event.get_event_metadata().get(\"in_app_frame_mix\") is None",
                "2875: ",
                "2876:             metrics_logged = [call.args[0] for call in mock_metrics_incr.mock_calls]",
                "2877:             assert \"grouping.in_app_frame_mix\" not in metrics_logged",
                "2878: ",
                "2879:     def test_derive_client_error_sampling_rate_with_option_enabled(self) -> None:",
                "2880:         \"\"\"Test that sample_rate is extracted from contexts when option is enabled.\"\"\"",
                "2881:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2882:             event_data = make_event(",
                "2883:                 contexts={\"error_sampling\": {\"client_sample_rate\": 0.1}}, platform=\"python\"",
                "2884:             )",
                "2885: ",
                "2886:             manager = EventManager(event_data)",
                "2887:             manager.normalize()",
                "2888:             event = manager.save(self.project.id)",
                "2889: ",
                "2890:             # Check that sample_rate was extracted and stored",
                "2891:             assert event.data[\"sample_rate\"] == 0.1",
                "2892: ",
                "2893:     def test_derive_client_error_sampling_rate_with_option_disabled(self) -> None:",
                "2894:         \"\"\"Test that sample_rate is not extracted when option is disabled.\"\"\"",
                "2895:         # Option disabled (project not in allowlist)",
                "2896:         event_data = make_event(",
                "2897:             contexts={\"error_sampling\": {\"client_sample_rate\": 0.1}}, platform=\"python\"",
                "2898:         )",
                "2899: ",
                "2900:         manager = EventManager(event_data)",
                "2901:         manager.normalize()",
                "2902:         event = manager.save(self.project.id)",
                "2903: ",
                "2904:         # Check that sample_rate was not extracted",
                "2905:         assert \"sample_rate\" not in event.data",
                "2906: ",
                "2907:     def test_derive_client_error_sampling_rate_malformed_context(self) -> None:",
                "2908:         \"\"\"Test that sample_rate extraction handles malformed error_sampling context gracefully.\"\"\"",
                "2909:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2910:             # Test with error_sampling as a number instead of a dict",
                "2911:             event_data = make_event(",
                "2912:                 contexts={\"error_sampling\": 0.1},",
                "2913:                 platform=\"python\",  # Should be a dict, not a number",
                "2914:             )",
                "2915: ",
                "2916:             manager = EventManager(event_data)",
                "2917:             manager.normalize()",
                "2918:             event = manager.save(self.project.id)",
                "2919: ",
                "2920:             # Check that no sample_rate was added due to malformed context",
                "2921:             assert \"sample_rate\" not in event.data",
                "2922: ",
                "2923:     def test_derive_client_error_sampling_rate_invalid_range(self) -> None:",
                "2924:         \"\"\"Test that sample_rate is not set when client_sample_rate is outside valid range (0-1).\"\"\"",
                "2925:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2926:             # Test with sample rate > 1",
                "2927:             event_data = make_event(",
                "2928:                 contexts={\"error_sampling\": {\"client_sample_rate\": 1.5}}, platform=\"python\"",
                "2929:             )"
            ]
        }
    ],
    "total": 1
}

Message 19 [Assistant]:
--------------------------------------------------------------------------------
Content:
现在让我查看`test_derive_client_error_sampling_rate_malformed_context`测试：
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "test_derive_client_error_sampling_rate_malformed_context",
      "is_regex": false,
      "case_sensitive": true,
      "context_lines": 30
    },
    "id": "call_00_sH8mEkcgbOrYBOeA2SVX0v3z",
    "type": "tool_call"
  }
]

Message 20 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_sH8mEkcgbOrYBOeA2SVX0v3z
Result:
{
    "matches": [
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2907,
            "matched_line": "    def test_derive_client_error_sampling_rate_malformed_context(self) -> None:",
            "context_start_line": 2877,
            "context_end_line": 2937,
            "context": [
                "2877:             assert \"grouping.in_app_frame_mix\" not in metrics_logged",
                "2878: ",
                "2879:     def test_derive_client_error_sampling_rate_with_option_enabled(self) -> None:",
                "2880:         \"\"\"Test that sample_rate is extracted from contexts when option is enabled.\"\"\"",
                "2881:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2882:             event_data = make_event(",
                "2883:                 contexts={\"error_sampling\": {\"client_sample_rate\": 0.1}}, platform=\"python\"",
                "2884:             )",
                "2885: ",
                "2886:             manager = EventManager(event_data)",
                "2887:             manager.normalize()",
                "2888:             event = manager.save(self.project.id)",
                "2889: ",
                "2890:             # Check that sample_rate was extracted and stored",
                "2891:             assert event.data[\"sample_rate\"] == 0.1",
                "2892: ",
                "2893:     def test_derive_client_error_sampling_rate_with_option_disabled(self) -> None:",
                "2894:         \"\"\"Test that sample_rate is not extracted when option is disabled.\"\"\"",
                "2895:         # Option disabled (project not in allowlist)",
                "2896:         event_data = make_event(",
                "2897:             contexts={\"error_sampling\": {\"client_sample_rate\": 0.1}}, platform=\"python\"",
                "2898:         )",
                "2899: ",
                "2900:         manager = EventManager(event_data)",
                "2901:         manager.normalize()",
                "2902:         event = manager.save(self.project.id)",
                "2903: ",
                "2904:         # Check that sample_rate was not extracted",
                "2905:         assert \"sample_rate\" not in event.data",
                "2906: ",
                "2907:     def test_derive_client_error_sampling_rate_malformed_context(self) -> None:",
                "2908:         \"\"\"Test that sample_rate extraction handles malformed error_sampling context gracefully.\"\"\"",
                "2909:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2910:             # Test with error_sampling as a number instead of a dict",
                "2911:             event_data = make_event(",
                "2912:                 contexts={\"error_sampling\": 0.1},",
                "2913:                 platform=\"python\",  # Should be a dict, not a number",
                "2914:             )",
                "2915: ",
                "2916:             manager = EventManager(event_data)",
                "2917:             manager.normalize()",
                "2918:             event = manager.save(self.project.id)",
                "2919: ",
                "2920:             # Check that no sample_rate was added due to malformed context",
                "2921:             assert \"sample_rate\" not in event.data",
                "2922: ",
                "2923:     def test_derive_client_error_sampling_rate_invalid_range(self) -> None:",
                "2924:         \"\"\"Test that sample_rate is not set when client_sample_rate is outside valid range (0-1).\"\"\"",
                "2925:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2926:             # Test with sample rate > 1",
                "2927:             event_data = make_event(",
                "2928:                 contexts={\"error_sampling\": {\"client_sample_rate\": 1.5}}, platform=\"python\"",
                "2929:             )",
                "2930: ",
                "2931:             manager = EventManager(event_data)",
                "2932:             manager.normalize()",
                "2933:             event = manager.save(self.project.id)",
                "2934: ",
                "2935:             # Check that sample_rate was not set due to invalid range",
                "2936:             assert \"sample_rate\" not in event.data",
                "2937: "
            ]
        }
    ],
    "total": 1
}

Message 21 [Assistant]:
--------------------------------------------------------------------------------
Content:
现在让我查看`test_derive_client_error_sampling_rate_invalid_range`测试：
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "test_derive_client_error_sampling_rate_invalid_range",
      "is_regex": false,
      "case_sensitive": true,
      "context_lines": 30
    },
    "id": "call_00_NflRYliyzSvMLpAqwmomDGSk",
    "type": "tool_call"
  }
]

Message 22 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_NflRYliyzSvMLpAqwmomDGSk
Result:
{
    "matches": [
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 2923,
            "matched_line": "    def test_derive_client_error_sampling_rate_invalid_range(self) -> None:",
            "context_start_line": 2893,
            "context_end_line": 2953,
            "context": [
                "2893:     def test_derive_client_error_sampling_rate_with_option_disabled(self) -> None:",
                "2894:         \"\"\"Test that sample_rate is not extracted when option is disabled.\"\"\"",
                "2895:         # Option disabled (project not in allowlist)",
                "2896:         event_data = make_event(",
                "2897:             contexts={\"error_sampling\": {\"client_sample_rate\": 0.1}}, platform=\"python\"",
                "2898:         )",
                "2899: ",
                "2900:         manager = EventManager(event_data)",
                "2901:         manager.normalize()",
                "2902:         event = manager.save(self.project.id)",
                "2903: ",
                "2904:         # Check that sample_rate was not extracted",
                "2905:         assert \"sample_rate\" not in event.data",
                "2906: ",
                "2907:     def test_derive_client_error_sampling_rate_malformed_context(self) -> None:",
                "2908:         \"\"\"Test that sample_rate extraction handles malformed error_sampling context gracefully.\"\"\"",
                "2909:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2910:             # Test with error_sampling as a number instead of a dict",
                "2911:             event_data = make_event(",
                "2912:                 contexts={\"error_sampling\": 0.1},",
                "2913:                 platform=\"python\",  # Should be a dict, not a number",
                "2914:             )",
                "2915: ",
                "2916:             manager = EventManager(event_data)",
                "2917:             manager.normalize()",
                "2918:             event = manager.save(self.project.id)",
                "2919: ",
                "2920:             # Check that no sample_rate was added due to malformed context",
                "2921:             assert \"sample_rate\" not in event.data",
                "2922: ",
                "2923:     def test_derive_client_error_sampling_rate_invalid_range(self) -> None:",
                "2924:         \"\"\"Test that sample_rate is not set when client_sample_rate is outside valid range (0-1).\"\"\"",
                "2925:         with self.options({\"issues.client_error_sampling.project_allowlist\": [self.project.id]}):",
                "2926:             # Test with sample rate > 1",
                "2927:             event_data = make_event(",
                "2928:                 contexts={\"error_sampling\": {\"client_sample_rate\": 1.5}}, platform=\"python\"",
                "2929:             )",
                "2930: ",
                "2931:             manager = EventManager(event_data)",
                "2932:             manager.normalize()",
                "2933:             event = manager.save(self.project.id)",
                "2934: ",
                "2935:             # Check that sample_rate was not set due to invalid range",
                "2936:             assert \"sample_rate\" not in event.data",
                "2937: ",
                "2938:             # Test with negative sample rate",
                "2939:             event_data = make_event(",
                "2940:                 contexts={\"error_sampling\": {\"client_sample_rate\": -0.1}}, platform=\"python\"",
                "2941:             )",
                "2942: ",
                "2943:             manager = EventManager(event_data)",
                "2944:             manager.normalize()",
                "2945:             event = manager.save(self.project.id)",
                "2946: ",
                "2947:             # Check that sample_rate was not set due to invalid range",
                "2948:             assert \"sample_rate\" not in event.data",
                "2949: ",
                "2950:     def test_times_seen_new_group_default_behavior(self) -> None:",
                "2951:         \"\"\"Test that new groups start with times_seen=1 when no sample rate is provided\"\"\"",
                "2952:         manager = EventManager(make_event(message=\"test message\"))",
                "2953:         manager.normalize()"
            ]
        }
    ],
    "total": 1
}

Message 23 [Assistant]:
--------------------------------------------------------------------------------
Content:
```json
{
    "risk_type": "business_intent",
    "file_path": "src/sentry/testutils/factories.py",
    "line_number": [344, 358],
    "description": "函数逻辑存在业务意图问题：当 `client_sample_rate` 存在但无法转换为浮点数时（例如字符串 \"invalid\" 或空字符串），异常被静默忽略，`sample_rate` 字段不会被设置。这可能与预期行为不符——要么应设置默认值，要么应记录警告。此外，如果 `client_sample_rate` 为 0 或 False 等假值，也会被跳过，这可能错误地忽略了有效的零采样率。",
    "confidence": 0.8,
    "severity": "warning",
    "suggestion": "建议修改逻辑：1) 明确处理转换异常，可以设置默认值或记录警告；2) 正确处理假值（如0、False），确保有效的零采样率不被忽略。"
}
```


Expert Analysis 3:
================================================================================
Risk Type: unknown
File: src/sentry/testutils/factories.py
Line: (1049, 1049)
Description: 在 `store_event` 方法中调用 `_set_sample_rate_from_error_sampling(normalized_data)` 会直接修改传入的 `normalized_data` 字典。由于 `normalized_data` 来自 `manager.get_data()`，而 `EventManager` 可能内部重用或缓存数据，这种副作用可能影响其他依赖相同数据的地方。测试工厂的修改可能无意中影响其他测试用例。
--------------------------------------------------------------------------------

Analysis Result:
{
  "risk_type": "lifecycle",
  "file_path": "src/sentry/testutils/factories.py",
  "line_number": [
    1049,
    1049
  ],
  "description": "在 `store_event` 方法中调用 `_set_sample_rate_from_error_sampling(normalized_data)` 会直接修改传入的 `normalized_data` 字典。由于 `normalized_data` 来自 `manager.get_data()`，而 `EventManager` 可能内部重用或缓存数据，这种副作用可能影响其他依赖相同数据的地方。测试工厂的修改可能无意中影响其他测试用例。",
  "confidence": 0.8,
  "severity": "warning",
  "suggestion": "考虑在调用 `_set_sample_rate_from_error_sampling` 之前对 `normalized_data` 进行深拷贝，以避免副作用。例如：`normalized_data_copy = copy.deepcopy(normalized_data)` 然后传递副本。"
}

Risk Item:
{
  "risk_type": "lifecycle",
  "file_path": "src/sentry/testutils/factories.py",
  "line_number": [
    1049,
    1049
  ],
  "description": "在 `store_event` 方法中调用 `_set_sample_rate_from_error_sampling(normalized_data)` 会直接修改传入的 `normalized_data` 字典。由于 `normalized_data` 来自 `manager.get_data()`，而 `EventManager` 可能内部重用或缓存数据，这种副作用可能影响其他依赖相同数据的地方。测试工厂的修改可能无意中影响其他测试用例。",
  "confidence": 0.7,
  "severity": "warning",
  "suggestion": null
}

Conversation History (23 messages):
================================================================================

Message 1 [Assistant]:
--------------------------------------------------------------------------------
Content:
我将分析这个风险项。首先，我需要了解 `_set_sample_rate_from_error_sampling` 函数的实现，以及 `EventManager` 的 `get_data()` 方法是否返回内部数据的引用。

让我先查看相关函数的实现：
Tool Calls:
[
  {
    "name": "cpg_symbol_search",
    "args": {
      "query": "_set_sample_rate_from_error_sampling",
      "include_callsites": true
    },
    "id": "call_00_KCLGnQFFhjopIdKaDQOl80yL",
    "type": "tool_call"
  }
]

Message 2 [Tool]:
--------------------------------------------------------------------------------
Tool: cpg_symbol_search
Tool Call ID: call_00_KCLGnQFFhjopIdKaDQOl80yL
Result:
{
    "rev": "head",
    "query": "_set_sample_rate_from_error_sampling",
    "symbols": [
        {
            "symbol_id": "420fa78ece20dc14eee724aec395460d3171f9faf94855bc7b288cd416bb9be5:13708-14284",
            "name": "_set_sample_rate_from_error_sampling",
            "kind": "function",
            "lang": "python",
            "location": {
                "file_path": "/Users/wangyue/Code/CodeReviewData/ReviewDataset/sentry-greptile/src/sentry/testutils/factories.py",
                "start_line": 344,
                "start_col": 1,
                "end_line": 357,
                "end_col": 17
            }
        }
    ],
    "callsites": [
        {
            "node_id": "420fa78ece20dc14eee724aec395460d3171f9faf94855bc7b288cd416bb9be5:39564-39617",
            "kind": "callsite",
            "location": {
                "file_path": "/Users/wangyue/Code/CodeReviewData/ReviewDataset/sentry-greptile/src/sentry/testutils/factories.py",
                "start_line": 1049,
                "start_col": 9,
                "end_line": 1049,
                "end_col": 62
            }
        }
    ],
    "_budget": {
        "attrs_dropped": true,
        "symbols_total": 1,
        "symbols_returned": 1,
        "symbols_truncated": false,
        "callsites_total": 1,
        "callsites_returned": 1,
        "callsites_truncated": false
    },
    "error": null,
    "_payload": {
        "approx_chars": 923,
        "approx_bytes_utf8": 923,
        "keys": [
            "_budget",
            "callsites",
            "error",
            "query",
            "rev",
            "symbols"
        ]
    }
}

Message 3 [Assistant]:
--------------------------------------------------------------------------------
Content:
现在让我查看 `_set_sample_rate_from_error_sampling` 函数的实现：
Tool Calls:
[
  {
    "name": "read_file",
    "args": {
      "file_path": "src/sentry/testutils/factories.py",
      "encoding": "utf-8"
    },
    "id": "call_00_7ZQXDzrCVsbu4O6VMtbZEcrv",
    "type": "tool_call"
  }
]

Message 4 [Tool]:
--------------------------------------------------------------------------------
Tool: read_file
Tool Call ID: call_00_7ZQXDzrCVsbu4O6VMtbZEcrv
Result:
{
    "content": "from __future__ import annotations\n\nimport contextlib\nimport copy\nimport io\nimport os\nimport random\nimport zipfile\nfrom base64 import b64encode\nfrom binascii import hexlify\nfrom collections.abc import Mapping, MutableMapping, Sequence\nfrom datetime import UTC, datetime\nfrom enum import Enum\nfrom hashlib import sha1\nfrom importlib import import_module\nfrom typing import Any\nfrom unittest import mock\nfrom uuid import uuid4\n\nimport orjson\nimport petname\nfrom django.conf import settings\nfrom django.contrib.auth.models import AnonymousUser\nfrom django.core.files.base import ContentFile\nfrom django.db import router, transaction\nfrom django.test.utils import override_settings\nfrom django.utils import timezone\nfrom django.utils.text import slugify\n\nfrom sentry.auth.access import RpcBackedAccess\nfrom sentry.auth.services.auth.model import RpcAuthState, RpcMemberSsoState\nfrom sentry.constants import SentryAppInstallationStatus, SentryAppStatus\nfrom sentry.event_manager import EventManager\nfrom sentry.eventstore.models import Event\nfrom sentry.hybridcloud.models.outbox import RegionOutbox, outbox_context\nfrom sentry.hybridcloud.models.webhookpayload import WebhookPayload\nfrom sentry.hybridcloud.outbox.category import OutboxCategory, OutboxScope\nfrom sentry.incidents.grouptype import MetricIssue\nfrom sentry.incidents.logic import (\n    create_alert_rule,\n    create_alert_rule_trigger,\n    create_alert_rule_trigger_action,\n    query_datasets_to_type,\n)\nfrom sentry.incidents.models.alert_rule import (\n    AlertRuleDetectionType,\n    AlertRuleThresholdType,\n    AlertRuleTriggerAction,\n)\nfrom sentry.incidents.models.incident import (\n    Incident,\n    IncidentActivity,\n    IncidentProject,\n    IncidentTrigger,\n    IncidentType,\n    TriggerStatus,\n)\nfrom sentry.integrations.models.doc_integration import DocIntegration\nfrom sentry.integrations.models.doc_integration_avatar import DocIntegrationAvatar\nfrom sentry.integrations.models.external_actor import ExternalActor\nfrom sentry.integrations.models.external_issue import ExternalIssue\nfrom sentry.integrations.models.integration import Integration\nfrom sentry.integrations.models.integration_external_project import IntegrationExternalProject\nfrom sentry.integrations.models.integration_feature import (\n    Feature,\n    IntegrationFeature,\n    IntegrationTypes,\n)\nfrom sentry.integrations.models.organization_integration import OrganizationIntegration\nfrom sentry.integrations.models.repository_project_path_config import RepositoryProjectPathConfig\nfrom sentry.integrations.types import ExternalProviders\nfrom sentry.issues.grouptype import get_group_type_by_type_id\nfrom sentry.models.activity import Activity\nfrom sentry.models.apikey import ApiKey\nfrom sentry.models.apitoken import ApiToken\nfrom sentry.models.artifactbundle import ArtifactBundle\nfrom sentry.models.authidentity import AuthIdentity\nfrom sentry.models.authprovider import AuthProvider\nfrom sentry.models.commit import Commit\nfrom sentry.models.commitauthor import CommitAuthor\nfrom sentry.models.commitfilechange import CommitFileChange\nfrom sentry.models.dashboard import Dashboard\nfrom sentry.models.dashboard_widget import (\n    DashboardWidget,\n    DashboardWidgetDisplayTypes,\n    DashboardWidgetQuery,\n)\nfrom sentry.models.debugfile import ProjectDebugFile\nfrom sentry.models.environment import Environment\nfrom sentry.models.files.control_file import ControlFile\nfrom sentry.models.files.file import File\nfrom sentry.models.group import Group\nfrom sentry.models.grouphistory import GroupHistory\nfrom sentry.models.grouplink import GroupLink\nfrom sentry.models.groupopenperiod import GroupOpenPeriod\nfrom sentry.models.grouprelease import GroupRelease\nfrom sentry.models.organization import Organization\nfrom sentry.models.organizationmapping import OrganizationMapping\nfrom sentry.models.organizationmember import OrganizationMember\nfrom sentry.models.organizationmemberinvite import OrganizationMemberInvite\nfrom sentry.models.organizationmemberteam import OrganizationMemberTeam\nfrom sentry.models.organizationslugreservation import OrganizationSlugReservation\nfrom sentry.models.orgauthtoken import OrgAuthToken\nfrom sentry.models.project import Project\nfrom sentry.models.projectbookmark import ProjectBookmark\nfrom sentry.models.projectcodeowners import ProjectCodeOwners\nfrom sentry.models.projecttemplate import ProjectTemplate\nfrom sentry.models.release import Release, ReleaseStatus\nfrom sentry.models.releasecommit import ReleaseCommit\nfrom sentry.models.releaseenvironment import ReleaseEnvironment\nfrom sentry.models.releasefile import ReleaseFile, update_artifact_index\nfrom sentry.models.releaseprojectenvironment import ReleaseProjectEnvironment\nfrom sentry.models.repository import Repository\nfrom sentry.models.rule import Rule\nfrom sentry.models.rulesnooze import RuleSnooze\nfrom sentry.models.savedsearch import SavedSearch\nfrom sentry.models.team import Team\nfrom sentry.models.userreport import UserReport\nfrom sentry.notifications.models.notificationaction import (\n    ActionService,\n    ActionTarget,\n    ActionTrigger,\n    NotificationAction,\n)\nfrom sentry.notifications.models.notificationsettingprovider import NotificationSettingProvider\nfrom sentry.organizations.services.organization import RpcOrganization, RpcUserOrganizationContext\nfrom sentry.performance_issues.performance_problem import PerformanceProblem\nfrom sentry.sentry_apps.installations import (\n    SentryAppInstallationCreator,\n    SentryAppInstallationTokenCreator,\n)\nfrom sentry.sentry_apps.logic import SentryAppCreator\nfrom sentry.sentry_apps.models.platformexternalissue import PlatformExternalIssue\nfrom sentry.sentry_apps.models.sentry_app import SentryApp\nfrom sentry.sentry_apps.models.sentry_app_avatar import SentryAppAvatar\nfrom sentry.sentry_apps.models.sentry_app_installation import SentryAppInstallation\nfrom sentry.sentry_apps.models.sentry_app_installation_for_provider import (\n    SentryAppInstallationForProvider,\n)\nfrom sentry.sentry_apps.models.servicehook import ServiceHook\nfrom sentry.sentry_apps.services.hook import hook_service\nfrom sentry.sentry_apps.token_exchange.grant_exchanger import GrantExchanger\nfrom sentry.signals import project_created\nfrom sentry.silo.base import SiloMode\nfrom sentry.snuba.dataset import Dataset\nfrom sentry.snuba.models import QuerySubscriptionDataSourceHandler\nfrom sentry.tempest.models import MessageType as TempestMessageType\nfrom sentry.tempest.models import TempestCredentials\nfrom sentry.testutils.outbox import outbox_runner\nfrom sentry.testutils.silo import assume_test_silo_mode\nfrom sentry.types.activity import ActivityType\nfrom sentry.types.actor import Actor\nfrom sentry.types.region import Region, get_local_region, get_region_by_name\nfrom sentry.types.token import AuthTokenType\nfrom sentry.uptime.models import (\n    IntervalSecondsLiteral,\n    ProjectUptimeSubscription,\n    UptimeStatus,\n    UptimeSubscription,\n    UptimeSubscriptionRegion,\n)\nfrom sentry.uptime.types import UptimeMonitorMode\nfrom sentry.users.models.identity import Identity, IdentityProvider, IdentityStatus\nfrom sentry.users.models.user import User\nfrom sentry.users.models.user_avatar import UserAvatar\nfrom sentry.users.models.user_option import UserOption\nfrom sentry.users.models.useremail import UserEmail\nfrom sentry.users.models.userpermission import UserPermission\nfrom sentry.users.models.userrole import UserRole\nfrom sentry.users.services.user import RpcUser\nfrom sentry.utils import loremipsum\nfrom sentry.workflow_engine.models import (\n    Action,\n    ActionAlertRuleTriggerAction,\n    AlertRuleDetector,\n    AlertRuleWorkflow,\n    DataCondition,\n    DataConditionGroup,\n    DataConditionGroupAction,\n    DataSource,\n    DataSourceDetector,\n    Detector,\n    DetectorState,\n    DetectorWorkflow,\n    IncidentGroupOpenPeriod,\n    Workflow,\n    WorkflowDataConditionGroup,\n)\nfrom sentry.workflow_engine.registry import data_source_type_registry\nfrom social_auth.models import UserSocialAuth\n\n\nclass EventType(Enum):\n    ERROR = \"error\"\n    DEFAULT = \"default\"\n\n\ndef get_fixture_path(*parts: str) -> str:\n    path = os.path.realpath(__file__)\n    for _ in range(4):  # src/sentry/testutils/{__file__}\n        path = os.path.dirname(path)\n    return os.path.join(path, \"fixtures\", *parts)\n\n\ndef make_sentence(words=None):\n    if words is None:\n        words = int(random.weibullvariate(8, 3))\n    return \" \".join(random.choice(loremipsum.words) for _ in range(words))\n\n\ndef make_word(words=None):\n    if words is None:\n        words = int(random.weibullvariate(8, 3))\n    return random.choice(loremipsum.words)\n\n\nDEFAULT_EVENT_DATA = {\n    \"extra\": {\n        \"loadavg\": [0.97607421875, 0.88330078125, 0.833984375],\n        \"sys.argv\": [\n            \"/Users/dcramer/.virtualenvs/sentry/bin/raven\",\n            \"test\",\n            \"https://ebc35f33e151401f9deac549978bda11:f3403f81e12e4c24942d505f086b2cad@sentry.io/1\",\n        ],\n        \"user\": \"dcramer\",\n    },\n    \"modules\": {\"raven\": \"3.1.13\"},\n    \"request\": {\n        \"cookies\": {},\n        \"data\": {},\n        \"env\": {},\n        \"headers\": {},\n        \"method\": \"GET\",\n        \"query_string\": \"\",\n        \"url\": \"http://example.com\",\n    },\n    \"stacktrace\": {\n        \"frames\": [\n            {\n                \"abs_path\": \"www/src/sentry/models/foo.py\",\n                \"context_line\": \"                        string_max_length=self.string_max_length)\",\n                \"filename\": \"sentry/models/foo.py\",\n                \"function\": \"build_msg\",\n                \"in_app\": True,\n                \"lineno\": 29,\n                \"module\": \"raven.base\",\n                \"post_context\": [\n                    \"                },\",\n                    \"            })\",\n                    \"\",\n                    \"        if 'stacktrace' in data:\",\n                    \"            if self.include_paths:\",\n                ],\n                \"pre_context\": [\n                    \"\",\n                    \"            data.update({\",\n                    \"                'stacktrace': {\",\n                    \"                    'frames': get_stack_info(frames,\",\n                    \"                        list_max_length=self.list_max_length,\",\n                ],\n                \"vars\": {\n                    \"culprit\": \"raven.scripts.runner\",\n                    \"date\": \"datetime.datetime(2013, 2, 14, 20, 6, 33, 479471)\",\n                    \"event_id\": \"598fb19363e745ec8be665e6ba88b1b2\",\n                    \"event_type\": \"raven.events.Message\",\n                    \"frames\": \"<generator object iter_stack_frames at 0x103fef050>\",\n                    \"handler\": \"<raven.events.Message object at 0x103feb710>\",\n                    \"k\": \"logentry\",\n                    \"public_key\": None,\n                    \"result\": {\n                        \"logentry\": \"{'message': 'This is a test message generated using ``raven test``', 'params': []}\"\n                    },\n                    \"self\": \"<raven.base.Client object at 0x104397f10>\",\n                    \"stack\": True,\n                    \"tags\": None,\n                    \"time_spent\": None,\n                },\n            },\n            {\n                \"abs_path\": \"/Users/dcramer/.virtualenvs/sentry/lib/python2.7/site-packages/raven/base.py\",\n                \"context_line\": \"                        string_max_length=self.string_max_length)\",\n                \"filename\": \"raven/base.py\",\n                \"function\": \"build_msg\",\n                \"in_app\": False,\n                \"lineno\": 290,\n                \"module\": \"raven.base\",\n                \"post_context\": [\n                    \"                },\",\n                    \"            })\",\n                    \"\",\n                    \"        if 'stacktrace' in data:\",\n                    \"            if self.include_paths:\",\n                ],\n                \"pre_context\": [\n                    \"\",\n                    \"            data.update({\",\n                    \"                'stacktrace': {\",\n                    \"                    'frames': get_stack_info(frames,\",\n                    \"                        list_max_length=self.list_max_length,\",\n                ],\n                \"vars\": {\n                    \"culprit\": \"raven.scripts.runner\",\n                    \"date\": \"datetime.datetime(2013, 2, 14, 20, 6, 33, 479471)\",\n                    \"event_id\": \"598fb19363e745ec8be665e6ba88b1b2\",\n                    \"event_type\": \"raven.events.Message\",\n                    \"frames\": \"<generator object iter_stack_frames at 0x103fef050>\",\n                    \"handler\": \"<raven.events.Message object at 0x103feb710>\",\n                    \"k\": \"logentry\",\n                    \"public_key\": None,\n                    \"result\": {\n                        \"logentry\": \"{'message': 'This is a test message generated using ``raven test``', 'params': []}\"\n                    },\n                    \"self\": \"<raven.base.Client object at 0x104397f10>\",\n                    \"stack\": True,\n                    \"tags\": None,\n                    \"time_spent\": None,\n                },\n            },\n        ]\n    },\n    \"tags\": [],\n    \"platform\": \"python\",\n}\n\ndefault_detector_config_data = {\n    MetricIssue.slug: {\"threshold_period\": 1, \"detection_type\": \"static\"}\n}\n\n\ndef _patch_artifact_manifest(path, org=None, release=None, project=None, extra_files=None):\n    with open(path, \"rb\") as fp:\n        manifest = orjson.loads(fp.read())\n    if org:\n        manifest[\"org\"] = org\n    if release:\n        manifest[\"release\"] = release\n    if project:\n        manifest[\"project\"] = project\n    for path in extra_files or {}:\n        manifest[\"files\"][path] = {\"url\": path}\n    return orjson.dumps(manifest).decode()\n\n\ndef _set_sample_rate_from_error_sampling(normalized_data: MutableMapping[str, Any]) -> None:\n    \"\"\"Set 'sample_rate' on normalized_data if contexts.error_sampling.client_sample_rate is present and valid.\"\"\"\n    client_sample_rate = None\n    try:\n        client_sample_rate = (\n            normalized_data.get(\"contexts\", {}).get(\"error_sampling\", {}).get(\"client_sample_rate\")\n        )\n    except Exception:\n        pass\n    if client_sample_rate:\n        try:\n            normalized_data[\"sample_rate\"] = float(client_sample_rate)\n        except Exception:\n            pass\n\n\n# TODO(dcramer): consider moving to something more scalable like factoryboy\nclass Factories:\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_organization(name=None, owner=None, region: Region | str | None = None, **kwargs):\n        if not name:\n            name = petname.generate(2, \" \", letters=10).title()\n\n        with contextlib.ExitStack() as ctx:\n            if region is None or SiloMode.get_current_mode() == SiloMode.MONOLITH:\n                region_name = get_local_region().name\n            else:\n                if isinstance(region, Region):\n                    region_name = region.name\n                else:\n                    region_obj = get_region_by_name(region)  # Verify it exists\n                    region_name = region_obj.name\n\n                ctx.enter_context(\n                    override_settings(SILO_MODE=SiloMode.REGION, SENTRY_REGION=region_name)\n                )\n\n            with outbox_context(flush=False):\n                org = Organization.objects.create(name=name, **kwargs)\n\n            with assume_test_silo_mode(SiloMode.CONTROL):\n                # Organization mapping creation relies on having a matching org slug reservation\n                OrganizationSlugReservation(\n                    organization_id=org.id,\n                    region_name=region_name,\n                    user_id=owner.id if owner else -1,\n                    slug=org.slug,\n                ).save(unsafe_write=True)\n\n            # Manually replicate org data after adding an org slug reservation\n            org.handle_async_replication(org.id)\n\n            # Flush remaining organization update outboxes accumulated by org create\n            RegionOutbox(\n                shard_identifier=org.id,\n                shard_scope=OutboxScope.ORGANIZATION_SCOPE,\n                category=OutboxCategory.ORGANIZATION_UPDATE,\n            ).drain_shard()\n\n        if owner:\n            Factories.create_member(organization=org, user_id=owner.id, role=\"owner\")\n        return org\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_org_mapping(org=None, **kwds):\n        if org:\n            kwds.setdefault(\"organization_id\", org.id)\n            kwds.setdefault(\"slug\", org.slug)\n            kwds.setdefault(\"name\", org.name)\n            kwds.setdefault(\"idempotency_key\", uuid4().hex)\n            kwds.setdefault(\"region_name\", \"na\")\n        return OrganizationMapping.objects.create(**kwds)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_member(teams=None, team_roles=None, **kwargs):\n        kwargs.setdefault(\"role\", \"member\")\n        teamRole = kwargs.pop(\"teamRole\", None)\n\n        # user_id will have precedence over user\n        user = kwargs.pop(\"user\", None)\n        user_id = kwargs.pop(\"user_id\", None)\n        if not user_id and user:\n            user_id = user.id\n        kwargs[\"user_id\"] = user_id\n\n        # inviter_id will have precedence over inviter\n        inviter = kwargs.pop(\"inviter\", None)\n        inviter_id = kwargs.pop(\"inviter_id\", None)\n        if not inviter_id and inviter:\n            inviter_id = inviter.id\n        kwargs[\"inviter_id\"] = inviter_id\n\n        om = OrganizationMember.objects.create(**kwargs)\n\n        if team_roles:\n            for team, role in team_roles:\n                Factories.create_team_membership(team=team, member=om, role=role)\n        elif teams:\n            for team in teams:\n                Factories.create_team_membership(team=team, member=om, role=teamRole)\n        return om\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_member_invite(\n        organization: Organization | None = None,\n        email: str | None = None,\n        **kwargs,\n    ) -> OrganizationMemberInvite:\n        if organization is None:\n            organization = Factories.create_organization()\n        if email is None:\n            email = f\"{petname.generate().title()}@email.com\"\n        om = OrganizationMember.objects.create(organization=organization)\n        return OrganizationMemberInvite.objects.create(\n            organization=organization, organization_member_id=om.id, email=email, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_team_membership(team, member=None, user=None, role=None):\n        if member is None:\n            member, created = OrganizationMember.objects.get_or_create(\n                user_id=user.id if user else None,\n                organization=team.organization,\n                defaults={\"role\": \"member\"},\n            )\n\n        return OrganizationMemberTeam.objects.create(\n            team=team, organizationmember=member, is_active=True, role=role\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_api_key(organization, **kwargs) -> ApiKey:\n        return ApiKey.objects.create(organization_id=organization.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_auth_provider(**kwargs):\n        return AuthProvider.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_auth_identity(**kwargs):\n        return AuthIdentity.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_auth_token(user, scope_list: list[str] | None = None, **kwargs) -> ApiToken:\n        if scope_list is None:\n            scope_list = []\n        return ApiToken.objects.create(\n            user=user,\n            scope_list=scope_list,\n            token_type=AuthTokenType.USER,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_org_auth_token(*args, **kwargs) -> OrgAuthToken:\n        return OrgAuthToken.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_team(organization, **kwargs):\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n        if not kwargs.get(\"slug\"):\n            kwargs[\"slug\"] = slugify(str(kwargs[\"name\"]))\n        members = kwargs.pop(\"members\", None)\n\n        team = Team.objects.create(organization=organization, **kwargs)\n        if members:\n            for user in members:\n                Factories.create_team_membership(team=team, user=user)\n        return team\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_environment(project, **kwargs):\n        name = kwargs.get(\"name\", petname.generate(3, \" \", letters=10)[:64])\n\n        organization = kwargs.get(\"organization\")\n        organization_id = organization.id if organization else project.organization_id\n\n        env = Environment.objects.create(organization_id=organization_id, name=name)\n        env.add_project(project, is_hidden=kwargs.get(\"is_hidden\"))\n        return env\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project(\n        organization=None, teams=None, fire_project_created=False, **kwargs\n    ) -> Project:\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n        if not kwargs.get(\"slug\"):\n            kwargs[\"slug\"] = slugify(str(kwargs[\"name\"]))\n        if not organization and teams:\n            organization = teams[0].organization\n\n        with transaction.atomic(router.db_for_write(Project)):\n            project = Project.objects.create(organization=organization, **kwargs)\n            if teams:\n                for team in teams:\n                    project.add_team(team)\n            if fire_project_created:\n                project_created.send(\n                    project=project, user=AnonymousUser(), default_rules=True, sender=Factories\n                )\n        return project\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_template(project=None, organization=None, **kwargs) -> ProjectTemplate:\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n\n        with transaction.atomic(router.db_for_write(Project)):\n            project_template = ProjectTemplate.objects.create(organization=organization, **kwargs)\n\n        return project_template\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_bookmark(project, user):\n        return ProjectBookmark.objects.create(project_id=project.id, user_id=user.id)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_rule(\n        project,\n        action_data=None,\n        allow_no_action_data=False,\n        condition_data=None,\n        name=\"Test Alert\",\n        action_match=\"all\",\n        filter_match=\"all\",\n        frequency=30,\n        **kwargs,\n    ):\n        actions = None\n        if not allow_no_action_data:\n            action_data = action_data or [\n                {\n                    \"id\": \"sentry.rules.actions.notify_event.NotifyEventAction\",\n                    \"name\": \"Send a notification (for all legacy integrations)\",\n                },\n                {\n                    \"id\": \"sentry.rules.actions.notify_event_service.NotifyEventServiceAction\",\n                    \"service\": \"mail\",\n                    \"name\": \"Send a notification via mail\",\n                },\n            ]\n            actions = action_data\n        condition_data = condition_data or [\n            {\n                \"id\": \"sentry.rules.conditions.first_seen_event.FirstSeenEventCondition\",\n                \"name\": \"A new issue is created\",\n            },\n            {\n                \"id\": \"sentry.rules.conditions.every_event.EveryEventCondition\",\n                \"name\": \"The event occurs\",\n            },\n        ]\n        data = {\n            \"conditions\": condition_data,\n            \"action_match\": action_match,\n            \"filter_match\": filter_match,\n            \"frequency\": frequency,\n        }\n        if actions:\n            data[\"actions\"] = actions\n\n        return Rule.objects.create(\n            label=name,\n            project=project,\n            data=data,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_slack_project_rule(project, integration_id, channel_id=None, channel_name=None):\n        action_data = [\n            {\n                \"id\": \"sentry.rules.actions.notify_event.SlackNotifyServiceAction\",\n                \"name\": \"Send a Slack notification\",\n                \"workspace\": integration_id,\n                \"channel_id\": channel_id or \"123453\",\n                \"channel\": channel_name or \"#general\",\n            }\n        ]\n        return Factories.create_project_rule(project, action_data)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_key(project):\n        return project.key_set.get_or_create()[0]\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_tempest_credentials(\n        project: Project,\n        created_by: User | None = None,\n        client_id: str | None = None,\n        client_secret: str | None = None,\n        message: str = \"\",\n        message_type: str | None = None,\n        latest_fetched_item_id: str | None = None,\n    ):\n        if client_id is None:\n            client_id = str(uuid4())\n        if client_secret is None:\n            client_secret = str(uuid4())\n        if message_type is None:\n            message_type = TempestMessageType.ERROR\n\n        return TempestCredentials.objects.create(\n            project=project,\n            created_by_id=created_by.id if created_by else None,\n            client_id=client_id,\n            client_secret=client_secret,\n            message=message,\n            message_type=message_type,\n            latest_fetched_item_id=latest_fetched_item_id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release(\n        project: Project,\n        user: User | None = None,\n        version: str | None = None,\n        date_added: datetime | None = None,\n        additional_projects: Sequence[Project] | None = None,\n        environments: Sequence[Environment] | None = None,\n        date_released: datetime | None = None,\n        adopted: datetime | None = None,\n        unadopted: datetime | None = None,\n        status: int | None = ReleaseStatus.OPEN,\n    ):\n        if version is None:\n            version = hexlify(os.urandom(20)).decode()\n\n        if date_added is None:\n            date_added = timezone.now()\n\n        if additional_projects is None:\n            additional_projects = []\n\n        release = Release.objects.create(\n            version=version,\n            organization_id=project.organization_id,\n            date_added=date_added,\n            date_released=date_released,\n            status=status,\n        )\n\n        release.add_project(project)\n        for additional_project in additional_projects:\n            release.add_project(additional_project)\n\n        for environment in environments or []:\n            ReleaseEnvironment.objects.create(\n                organization=project.organization, release=release, environment=environment\n            )\n            for project in [project, *additional_projects]:\n                ReleaseProjectEnvironment.objects.create(\n                    project=project,\n                    release=release,\n                    environment=environment,\n                    adopted=adopted,\n                    unadopted=unadopted,\n                )\n\n        Activity.objects.create(\n            type=ActivityType.RELEASE.value,\n            project=project,\n            ident=Activity.get_version_ident(version),\n            user_id=user.id if user else None,\n            data={\"version\": version},\n        )\n\n        # add commits\n        if user:\n            author = Factories.create_commit_author(project=project, user=user)\n            repo = Factories.create_repo(project, name=f\"organization-{project.slug}\")\n            commit = Factories.create_commit(\n                project=project,\n                repo=repo,\n                author=author,\n                release=release,\n                key=\"deadbeef\",\n                message=\"placeholder commit message\",\n            )\n\n            release.update(authors=[str(author.id)], commit_count=1, last_commit_id=commit.id)\n\n        return release\n\n    @staticmethod\n    def create_group_release(project: Project, group: Group, release: Release) -> GroupRelease:\n        return GroupRelease.objects.create(\n            project_id=project.id,\n            group_id=group.id,\n            release_id=release.id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release_file(release_id, file=None, name=None, dist_id=None):\n        if file is None:\n            file = Factories.create_file(\n                name=\"log.txt\",\n                size=32,\n                headers={\"Content-Type\": \"text/plain\"},\n                checksum=\"dc1e3f3e411979d336c3057cce64294f3420f93a\",\n            )\n\n        if name is None:\n            name = file.name\n\n        organization_id = Release.objects.get(pk=release_id).organization.id\n\n        return ReleaseFile.objects.create(\n            organization_id=organization_id,\n            release_id=release_id,\n            name=name,\n            file=file,\n            dist_id=dist_id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_artifact_bundle_zip(\n        org=None, release=None, project=None, extra_files=None, fixture_path=\"artifact_bundle\"\n    ):\n        bundle = io.BytesIO()\n        bundle_dir = get_fixture_path(fixture_path)\n        with zipfile.ZipFile(bundle, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n            for path, content in (extra_files or {}).items():\n                zipf.writestr(path, content)\n            for path, _, files in os.walk(bundle_dir):\n                for filename in files:\n                    fullpath = os.path.join(path, filename)\n                    relpath = os.path.relpath(fullpath, bundle_dir)\n                    if filename == \"manifest.json\":\n                        manifest = _patch_artifact_manifest(\n                            fullpath, org, release, project, extra_files\n                        )\n                        zipf.writestr(relpath, manifest)\n                    else:\n                        zipf.write(fullpath, relpath)\n\n        return bundle.getvalue()\n\n    @classmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release_archive(cls, org, release: str, project=None, dist=None):\n        bundle = cls.create_artifact_bundle_zip(org, release, project)\n        file = File.objects.create(name=\"release-artifacts.zip\")\n        file.putfile(ContentFile(bundle))\n        release_obj = Release.objects.get(organization__slug=org, version=release)\n        return update_artifact_index(release_obj, dist, file)\n\n    @classmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_artifact_bundle(\n        cls,\n        org,\n        bundle_id=None,\n        artifact_count=0,\n        fixture_path=\"artifact_bundle_debug_ids\",\n        date_uploaded=None,\n        date_last_modified=None,\n    ):\n        if date_uploaded is None:\n            date_uploaded = timezone.now()\n\n        bundle = cls.create_artifact_bundle_zip(org.slug, fixture_path=fixture_path)\n        file_ = File.objects.create(name=\"artifact-bundle.zip\")\n        file_.putfile(ContentFile(bundle))\n        # The 'artifact_count' should correspond to the 'bundle' contents but for the purpose of tests we can also\n        # mock it with an arbitrary value.\n        artifact_bundle = ArtifactBundle.objects.create(\n            organization_id=org.id,\n            bundle_id=bundle_id or uuid4(),\n            file=file_,\n            artifact_count=artifact_count,\n            date_uploaded=date_uploaded,\n            date_last_modified=date_last_modified,\n        )\n        return artifact_bundle\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_code_mapping(project, repo=None, organization_integration=None, **kwargs):\n        kwargs.setdefault(\"stack_root\", \"\")\n        kwargs.setdefault(\"source_root\", \"\")\n        kwargs.setdefault(\"default_branch\", \"master\")\n\n        if not repo:\n            repo = Factories.create_repo(project=project)\n        return RepositoryProjectPathConfig.objects.create(\n            project=project,\n            repository=repo,\n            organization_integration_id=organization_integration.id,\n            integration_id=organization_integration.integration_id,\n            organization_id=organization_integration.organization_id,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_repo(\n        project, name=None, provider=None, integration_id=None, url=None, external_id=None\n    ):\n        repo, _ = Repository.objects.get_or_create(\n            organization_id=project.organization_id,\n            name=name\n            or \"{}-{}\".format(petname.generate(2, \"\", letters=10), random.randint(1000, 9999)),\n            provider=provider,\n            integration_id=integration_id,\n            url=url,\n            external_id=external_id,\n        )\n        return repo\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit(\n        repo, project=None, author=None, release=None, message=None, key=None, date_added=None\n    ):\n        commit = Commit.objects.get_or_create(\n            organization_id=repo.organization_id,\n            repository_id=repo.id,\n            key=key or sha1(uuid4().hex.encode(\"utf-8\")).hexdigest(),\n            defaults={\n                \"message\": message or make_sentence(),\n                \"author\": author\n                or Factories.create_commit_author(organization_id=repo.organization_id),\n                \"date_added\": date_added or timezone.now(),\n            },\n        )[0]\n\n        if release:\n            assert project\n            ReleaseCommit.objects.create(\n                organization_id=repo.organization_id,\n                project_id=project.id,\n                release=release,\n                commit=commit,\n                order=1,\n            )\n\n        Factories.create_commit_file_change(commit=commit, filename=\"/models/foo.py\")\n        Factories.create_commit_file_change(commit=commit, filename=\"/worsematch/foo.py\")\n        Factories.create_commit_file_change(commit=commit, filename=\"/models/other.py\")\n\n        return commit\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit_author(organization_id=None, project=None, user=None, email=None):\n        if email:\n            user_email = email\n        else:\n            user_email = user.email if user else f\"{make_word()}@example.com\"\n        return CommitAuthor.objects.get_or_create(\n            organization_id=organization_id or project.organization_id,\n            email=user_email,\n            defaults={\"name\": user.name if user else make_word()},\n        )[0]\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit_file_change(commit, filename):\n        return CommitFileChange.objects.get_or_create(\n            organization_id=commit.organization_id, commit=commit, filename=filename, type=\"M\"\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user(\n        email=None, is_superuser=False, is_staff=False, is_active=True, **kwargs\n    ) -> User:\n        if email is None:\n            email = uuid4().hex + \"@example.com\"\n\n        kwargs.setdefault(\"username\", email)\n\n        user = User(\n            email=email, is_superuser=is_superuser, is_staff=is_staff, is_active=is_active, **kwargs\n        )\n        if kwargs.get(\"password\") is None:\n            user.set_password(\"admin\")\n        user.save()\n\n        # UserEmail is created by a signal\n        assert UserEmail.objects.filter(user=user, email=email).update(is_verified=True)\n\n        return user\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_useremail(user, email=None, **kwargs):\n        if not email:\n            email = uuid4().hex + \"@example.com\"\n\n        kwargs.setdefault(\"is_verified\", True)\n\n        useremail = UserEmail(user=user, email=email, **kwargs)\n        useremail.save()\n\n        return useremail\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_avatar(*args, **kwargs):\n        return UserAvatar.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_role(*args, **kwargs):\n        return UserRole.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_usersocialauth(\n        user: User,\n        provider: str | None = None,\n        uid: str | None = None,\n        extra_data: dict[str, Any] | None = None,\n    ):\n        if not provider:\n            provider = \"asana\"\n        if not uid:\n            uid = \"abc-123\"\n        usa = UserSocialAuth(user=user, provider=provider, uid=uid, extra_data=extra_data)\n        usa.save()\n        return usa\n\n    @staticmethod\n    def inject_performance_problems(jobs, _):\n        for job in jobs:\n            job[\"performance_problems\"] = []\n            for f in job[\"data\"][\"fingerprint\"]:\n                f_data = f.split(\"-\", 1)\n                if len(f_data) < 2:\n                    raise ValueError(\n                        \"Invalid performance fingerprint data. Format must be 'group_type-fingerprint'.\"\n                    )\n                group_type = get_group_type_by_type_id(int(f_data[0]))\n                perf_fingerprint = f_data[1]\n\n                job[\"performance_problems\"].append(\n                    PerformanceProblem(\n                        fingerprint=perf_fingerprint,\n                        op=\"db\",\n                        desc=\"\",\n                        type=group_type,\n                        parent_span_ids=None,\n                        cause_span_ids=None,\n                        offender_span_ids=[],\n                        evidence_data={},\n                        evidence_display=[],\n                    )\n                )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def store_event(\n        data,\n        project_id: int,\n        assert_no_errors: bool = True,\n        default_event_type: EventType | None = None,\n        sent_at: datetime | None = None,\n    ) -> Event:\n        \"\"\"\n        Like `create_event`, but closer to how events are actually\n        ingested. Prefer to use this method over `create_event`\n        \"\"\"\n\n        # this creates a basic message event\n        if default_event_type == EventType.DEFAULT:\n            data.update({\"stacktrace\": copy.deepcopy(DEFAULT_EVENT_DATA[\"stacktrace\"])})\n\n        # this creates an error event\n        elif default_event_type == EventType.ERROR:\n            data.update({\"exception\": [{\"value\": \"BadError\"}]})\n\n        manager = EventManager(data, sent_at=sent_at)\n        manager.normalize()\n        if assert_no_errors:\n            errors = manager.get_data().get(\"errors\")\n            assert not errors, errors\n\n        normalized_data = manager.get_data()\n\n        _set_sample_rate_from_error_sampling(normalized_data)\n\n        event = None\n\n        # When fingerprint is present on transaction, inject performance problems\n        if (\n            normalized_data.get(\"type\") == \"transaction\"\n            and normalized_data.get(\"fingerprint\") is not None\n        ):\n            with mock.patch(\n                \"sentry.event_manager._detect_performance_problems\",\n                Factories.inject_performance_problems,\n            ):\n                event = manager.save(project_id)\n\n        else:\n            event = manager.save(project_id)\n\n        if event.groups:\n            for group in event.groups:\n                group.save()\n\n        if event.group:\n            event.group.save()\n\n        return event\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_group(project, create_open_period=True, **kwargs):\n        from sentry.models.group import GroupStatus\n        from sentry.models.groupopenperiod import GroupOpenPeriod\n        from sentry.testutils.helpers.datetime import before_now\n        from sentry.types.group import GroupSubStatus\n\n        kwargs.setdefault(\"message\", \"Hello world\")\n        kwargs.setdefault(\"data\", {})\n        if \"type\" not in kwargs[\"data\"]:\n            kwargs[\"data\"].update({\"type\": \"default\", \"metadata\": {\"title\": kwargs[\"message\"]}})\n        if \"short_id\" not in kwargs:\n            kwargs[\"short_id\"] = project.next_short_id()\n        if \"metadata\" in kwargs:\n            metadata = kwargs.pop(\"metadata\")\n            kwargs[\"data\"].setdefault(\"metadata\", {}).update(metadata)\n        if \"status\" not in kwargs:\n            kwargs[\"status\"] = GroupStatus.UNRESOLVED\n            kwargs[\"substatus\"] = GroupSubStatus.NEW\n\n        group = Group.objects.create(project=project, **kwargs)\n        if create_open_period:\n            open_period = GroupOpenPeriod.objects.create(\n                group=group,\n                project=project,\n                date_started=group.first_seen or before_now(minutes=5),\n            )\n            if group.status == GroupStatus.RESOLVED:\n                open_period.update(\n                    date_ended=group.resolved_at if group.resolved_at else timezone.now()\n                )\n\n        return group\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_file(**kwargs):\n        return File.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_file_from_path(path, name=None, **kwargs):\n        if name is None:\n            name = os.path.basename(path)\n\n        file = Factories.create_file(name=name, **kwargs)\n        with open(path) as f:\n            file.putfile(f)\n        return file\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dif_file(\n        project,\n        debug_id=None,\n        object_name=None,\n        features=None,\n        data=None,\n        file=None,\n        cpu_name=None,\n        code_id=None,\n        **kwargs,\n    ):\n        if debug_id is None:\n            debug_id = str(uuid4())\n\n        if object_name is None:\n            object_name = \"%s.dSYM\" % debug_id\n\n        if features is not None:\n            if data is None:\n                data = {}\n            data[\"features\"] = features\n\n        if file is None:\n            file = Factories.create_file(\n                name=object_name,\n                size=42,\n                headers={\"Content-Type\": \"application/x-mach-binary\"},\n                checksum=\"dc1e3f3e411979d336c3057cce64294f3420f93a\",\n            )\n\n        return ProjectDebugFile.objects.create(\n            debug_id=debug_id,\n            code_id=code_id,\n            project_id=project.id,\n            object_name=object_name,\n            cpu_name=cpu_name or \"x86_64\",\n            file=file,\n            checksum=file.checksum,\n            data=data,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dif_from_path(path, object_name=None, **kwargs):\n        if object_name is None:\n            object_name = os.path.basename(path)\n\n        headers = {\"Content-Type\": \"application/x-mach-binary\"}\n        file = Factories.create_file_from_path(path, name=object_name, headers=headers)\n        return Factories.create_dif_file(file=file, object_name=object_name, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def add_user_permission(user, permission):\n        UserPermission.objects.create(user=user, permission=permission)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app(**kwargs):\n        published = kwargs.pop(\"published\", False)\n        args = Factories._sentry_app_kwargs(**kwargs)\n        user = args.pop(\"user\", None)\n        app = SentryAppCreator(is_internal=False, **args).run(user=user, request=None)\n\n        if published:\n            app.update(status=SentryAppStatus.PUBLISHED)\n\n        return app\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_avatar(*args, **kwargs):\n        return SentryAppAvatar.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_internal_integration(**kwargs) -> SentryApp:\n        args = Factories._sentry_app_kwargs(**kwargs)\n        args[\"verify_install\"] = False\n        user = args.pop(\"user\", None)\n        app = SentryAppCreator(is_internal=True, **args).run(\n            user=user, request=None, skip_default_auth_token=True\n        )\n        return app\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_internal_integration_token(\n        user,\n        internal_integration: SentryApp | None = None,\n        install: SentryAppInstallation | None = None,\n        request=None,\n    ) -> ApiToken:\n        if internal_integration and install:\n            raise ValueError(\"Only one of internal_integration or install arg can be provided\")\n        elif internal_integration is None and install is None:\n            raise ValueError(\"Must pass in either internal_integration or install arg\")\n\n        if internal_integration is not None and install is None:\n            # Fetch install from provided or created internal integration\n            with assume_test_silo_mode(SiloMode.CONTROL):\n                install = SentryAppInstallation.objects.get(\n                    sentry_app=internal_integration.id,\n                    organization_id=internal_integration.owner_id,\n                )\n        elif install is None:\n            raise AssertionError(\"unreachable\")\n\n        return SentryAppInstallationTokenCreator(sentry_app_installation=install).run(\n            user=user, request=request\n        )\n\n    @staticmethod\n    def _sentry_app_kwargs(**kwargs):\n        _kwargs = {\n            \"user\": kwargs.get(\"user\", Factories.create_user()),\n            \"name\": kwargs.get(\"name\", petname.generate(2, \" \", letters=10).title()),\n            \"organization_id\": kwargs.get(\n                \"organization_id\", kwargs.pop(\"organization\", Factories.create_organization()).id\n            ),\n            \"author\": kwargs.get(\"author\", \"A Company\"),\n            \"scopes\": kwargs.get(\"scopes\", ()),\n            \"verify_install\": kwargs.get(\"verify_install\", True),\n            \"webhook_url\": kwargs.get(\"webhook_url\", \"https://example.com/webhook\"),\n            \"events\": [],\n            \"schema\": {},\n        }\n\n        _kwargs.update(**kwargs)\n        return _kwargs\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_sentry_app_installation(\n        organization=None,\n        slug=None,\n        user=None,\n        status=None,\n        prevent_token_exchange=False,\n    ):\n        if not organization:\n            organization = Factories.create_organization()\n\n        Factories.create_project(organization=organization)\n\n        with assume_test_silo_mode(SiloMode.CONTROL):\n            install = SentryAppInstallationCreator(\n                slug=(slug or Factories.create_sentry_app(organization=organization).slug),\n                organization_id=organization.id,\n            ).run(\n                user=(user or Factories.create_user()),\n                request=None,\n            )\n\n            install.status = SentryAppInstallationStatus.INSTALLED if status is None else status\n            install.save()\n            if not prevent_token_exchange and (\n                install.sentry_app.status != SentryAppStatus.INTERNAL\n            ):\n                assert install.api_grant is not None\n                assert install.sentry_app.application is not None\n                assert install.sentry_app.proxy_user is not None\n                GrantExchanger(\n                    install=install,\n                    code=install.api_grant.code,\n                    client_id=install.sentry_app.application.client_id,\n                    user=install.sentry_app.proxy_user,\n                ).run()\n                install = SentryAppInstallation.objects.get(id=install.id)\n        return install\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_installation_for_provider(\n        sentry_app_id: int,\n        organization_id: int,\n        provider: str,\n    ) -> SentryAppInstallationForProvider:\n        installation = SentryAppInstallation.objects.get(\n            sentry_app_id=sentry_app_id, organization_id=organization_id\n        )\n        return SentryAppInstallationForProvider.objects.create(\n            organization_id=organization_id,\n            provider=provider,\n            sentry_app_installation=installation,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_stacktrace_link_schema():\n        return {\"type\": \"stacktrace-link\", \"uri\": \"/redirect/\"}\n\n    @staticmethod\n    def create_issue_link_schema():\n        return {\n            \"type\": \"issue-link\",\n            \"link\": {\n                \"uri\": \"/sentry/issues/link\",\n                \"required_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    }\n                ],\n            },\n            \"create\": {\n                \"uri\": \"/sentry/issues/create\",\n                \"required_fields\": [\n                    {\"type\": \"text\", \"name\": \"title\", \"label\": \"Title\"},\n                    {\"type\": \"text\", \"name\": \"summary\", \"label\": \"Summary\"},\n                ],\n                \"optional_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"points\",\n                        \"label\": \"Points\",\n                        \"options\": [[\"1\", \"1\"], [\"2\", \"2\"], [\"3\", \"3\"], [\"5\", \"5\"], [\"8\", \"8\"]],\n                    },\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    },\n                ],\n            },\n        }\n\n    @staticmethod\n    def create_alert_rule_action_schema():\n        return {\n            \"type\": \"alert-rule-action\",\n            \"title\": \"Create Task with App\",\n            \"settings\": {\n                \"type\": \"alert-rule-settings\",\n                \"uri\": \"/sentry/alert-rule\",\n                \"required_fields\": [\n                    {\"type\": \"text\", \"name\": \"title\", \"label\": \"Title\"},\n                    {\"type\": \"text\", \"name\": \"summary\", \"label\": \"Summary\"},\n                ],\n                \"optional_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"points\",\n                        \"label\": \"Points\",\n                        \"options\": [[\"1\", \"1\"], [\"2\", \"2\"], [\"3\", \"3\"], [\"5\", \"5\"], [\"8\", \"8\"]],\n                    },\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    },\n                ],\n            },\n        }\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_service_hook(\n        actor=None, org=None, project=None, events=None, url=None, project_ids=None, **kwargs\n    ):\n        if project:\n            if project_ids is not None:\n                raise ValueError(\"Cannot provide both project and project_ids\")\n            project_ids = [project.id]\n        if not actor:\n            actor = Factories.create_user()\n        if not org:\n            if project:\n                org = project.organization\n            else:\n                org = Factories.create_organization(owner=actor)\n        if project_ids is None:  # empty list for project_ids is valid and means no project filter\n            project_ids = [Factories.create_project(organization=org).id]\n        if events is None:\n            events = [\"event.created\"]\n        if not url:\n            url = \"https://example.com/sentry/webhook\"\n\n        app_id = kwargs.pop(\"application_id\", None)\n        if app_id is None and \"application\" in kwargs:\n            app_id = kwargs[\"application\"].id\n        installation_id = kwargs.pop(\"installation_id\", None)\n        if installation_id is None and \"installation\" in kwargs:\n            installation_id = kwargs[\"installation\"].id\n        hook_id = hook_service.create_service_hook(\n            application_id=app_id,\n            actor_id=actor.id,\n            installation_id=installation_id,\n            organization_id=org.id,\n            project_ids=project_ids,\n            events=events,\n            url=url,\n        ).id\n        return ServiceHook.objects.get(id=hook_id)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_feature(feature=None, sentry_app=None, description=None):\n        if not sentry_app:\n            sentry_app = Factories.create_sentry_app()\n\n        integration_feature = IntegrationFeature.objects.create(\n            target_id=sentry_app.id,\n            target_type=IntegrationTypes.SENTRY_APP.value,\n            feature=feature or Feature.API,\n        )\n\n        if description:\n            integration_feature.update(user_description=description)\n\n        return integration_feature\n\n    @staticmethod\n    def _doc_integration_kwargs(**kwargs):\n        _kwargs = {\n            \"name\": kwargs.get(\"name\", petname.generate(2, \" \", letters=10).title()),\n            \"author\": kwargs.get(\"author\", \"me\"),\n            \"description\": kwargs.get(\"description\", \"hi im a description\"),\n            \"url\": kwargs.get(\"url\", \"https://sentry.io\"),\n            \"popularity\": kwargs.get(\"popularity\", 1),\n            \"is_draft\": kwargs.get(\"is_draft\", True),\n            \"metadata\": kwargs.get(\"metadata\", {}),\n        }\n        _kwargs[\"slug\"] = slugify(_kwargs[\"name\"])\n        _kwargs.update(**kwargs)\n        return _kwargs\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration(features=None, has_avatar: bool = False, **kwargs) -> DocIntegration:\n        doc = DocIntegration.objects.create(**Factories._doc_integration_kwargs(**kwargs))\n        if features:\n            Factories.create_doc_integration_features(features=features, doc_integration=doc)\n        if has_avatar:\n            Factories.create_doc_integration_avatar(doc_integration=doc)\n        return doc\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration_features(\n        features=None, doc_integration=None\n    ) -> list[IntegrationFeature]:\n        if not features:\n            features = [Feature.API]\n        if not doc_integration:\n            doc_integration = Factories.create_doc_integration()\n        return IntegrationFeature.objects.bulk_create(\n            [\n                IntegrationFeature(\n                    target_id=doc_integration.id,\n                    target_type=IntegrationTypes.DOC_INTEGRATION.value,\n                    feature=feature,\n                )\n                for feature in features\n            ]\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration_avatar(doc_integration=None, **kwargs) -> DocIntegrationAvatar:\n        if not doc_integration:\n            doc_integration = Factories.create_doc_integration()\n        photo = ControlFile.objects.create(name=\"test.png\", type=\"avatar.file\")\n        photo.putfile(io.BytesIO(b\"imaginethiswasphotobytes\"))\n\n        return DocIntegrationAvatar.objects.create(\n            doc_integration=doc_integration, avatar_type=0, control_file_id=photo.id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_userreport(\n        project: Project, event_id: str | None = None, **kwargs: Any\n    ) -> UserReport:\n        event = Factories.store_event(\n            data={\n                \"timestamp\": datetime.now(UTC).isoformat(),\n                \"event_id\": event_id or \"a\" * 32,\n                \"message\": \"testing\",\n            },\n            project_id=project.id,\n        )\n        assert event.group is not None\n\n        return UserReport.objects.create(\n            group_id=event.group.id,\n            event_id=event.event_id,\n            project_id=project.id,\n            name=\"Jane Bloggs\",\n            email=\"jane@example.com\",\n            comments=\"the application crashed\",\n            **kwargs,\n        )\n\n    @staticmethod\n    def create_session():\n        engine = import_module(settings.SESSION_ENGINE)\n\n        session = engine.SessionStore()\n        session.save()\n        return session\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_platform_external_issue(\n        group=None, service_type=None, display_name=None, web_url=None\n    ):\n        return PlatformExternalIssue.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            service_type=service_type,\n            display_name=display_name,\n            web_url=web_url,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_integration_external_issue(group=None, integration=None, key=None, **kwargs):\n        external_issue = ExternalIssue.objects.create(\n            organization_id=group.organization.id, integration_id=integration.id, key=key, **kwargs\n        )\n\n        GroupLink.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            linked_type=GroupLink.LinkedType.issue,\n            linked_id=external_issue.id,\n            relationship=GroupLink.Relationship.references,\n        )\n\n        return external_issue\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_integration_external_project(\n        organization_id: int, integration_id: int, *args: Any, **kwargs: Any\n    ) -> IntegrationExternalProject:\n        oi = OrganizationIntegration.objects.get(\n            organization_id=organization_id, integration_id=integration_id\n        )\n        return IntegrationExternalProject.objects.create(\n            organization_integration_id=oi.id, *args, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident(\n        organization,\n        projects,\n        detection_uuid=None,\n        status=1,\n        title=None,\n        query=\"test query\",\n        date_started=None,\n        date_detected=None,\n        date_closed=None,\n        alert_rule=None,\n        subscription=None,\n    ):\n        if not title:\n            title = petname.generate(2, \" \", letters=10).title()\n        if alert_rule is None:\n            alert_rule = Factories.create_alert_rule(\n                organization, projects, query=query, time_window=1\n            )\n\n        incident = Incident.objects.create(\n            organization=organization,\n            detection_uuid=detection_uuid,\n            status=status,\n            title=title,\n            alert_rule=alert_rule,\n            date_started=date_started or timezone.now(),\n            date_detected=date_detected or timezone.now(),\n            date_closed=timezone.now() if date_closed is not None else date_closed,\n            type=IncidentType.ALERT_TRIGGERED.value,\n            subscription=subscription,\n        )\n        for project in projects:\n            IncidentProject.objects.create(incident=incident, project=project)\n\n        return incident\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_activity(incident, type, comment=None, user_id=None):\n        return IncidentActivity.objects.create(\n            incident=incident, type=type, comment=comment, user_id=user_id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule(\n        organization,\n        projects,\n        name=None,\n        owner=None,\n        query=\"level:error\",\n        aggregate=\"count()\",\n        time_window=10,\n        threshold_period=1,\n        environment=None,\n        date_added=None,\n        query_type=None,\n        dataset=Dataset.Events,\n        threshold_type=AlertRuleThresholdType.ABOVE,\n        resolve_threshold=None,\n        user=None,\n        event_types=None,\n        comparison_delta=None,\n        description=None,\n        sensitivity=None,\n        seasonality=None,\n        detection_type=AlertRuleDetectionType.STATIC,\n    ):\n        if not name:\n            name = petname.generate(2, \" \", letters=10).title()\n\n        if query_type is None:\n            query_type = query_datasets_to_type[dataset]\n\n        alert_rule = create_alert_rule(\n            organization,\n            projects,\n            name,\n            query,\n            aggregate,\n            time_window,\n            threshold_type,\n            threshold_period,\n            owner=owner,\n            resolve_threshold=resolve_threshold,\n            query_type=query_type,\n            dataset=dataset,\n            environment=environment,\n            user=user,\n            event_types=event_types,\n            comparison_delta=comparison_delta,\n            description=description,\n            sensitivity=sensitivity,\n            seasonality=seasonality,\n            detection_type=detection_type,\n        )\n\n        if date_added is not None:\n            alert_rule.update(date_added=date_added)\n\n        return alert_rule\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_trigger(alert_rule, label=None, alert_threshold=100):\n        if not label:\n            label = petname.generate(2, \" \", letters=10).title()\n\n        return create_alert_rule_trigger(alert_rule, label, alert_threshold)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_trigger(incident, alert_rule_trigger, status=None):\n        if status is None:\n            status = TriggerStatus.ACTIVE.value\n\n        return IncidentTrigger.objects.create(\n            alert_rule_trigger=alert_rule_trigger, incident=incident, status=status\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_trigger_action(\n        trigger,\n        type=AlertRuleTriggerAction.Type.EMAIL,\n        target_type=AlertRuleTriggerAction.TargetType.USER,\n        target_identifier=None,\n        integration=None,\n        sentry_app=None,\n        sentry_app_config=None,\n    ):\n        return create_alert_rule_trigger_action(\n            trigger,\n            type,\n            target_type,\n            target_identifier,\n            integration.id if integration else None,\n            sentry_app.id if sentry_app else None,\n            sentry_app_config=sentry_app_config,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_external_user(user: User, **kwargs: Any) -> ExternalActor:\n        kwargs.setdefault(\"provider\", ExternalProviders.GITHUB.value)\n        kwargs.setdefault(\"external_name\", \"\")\n\n        return ExternalActor.objects.create(user_id=user.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_external_team(team: Team, **kwargs: Any) -> ExternalActor:\n        kwargs.setdefault(\"provider\", ExternalProviders.GITHUB.value)\n        kwargs.setdefault(\"external_name\", \"@getsentry/ecosystem\")\n\n        return ExternalActor.objects.create(team_id=team.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_codeowners(project, code_mapping, **kwargs):\n        kwargs.setdefault(\"raw\", \"\")\n\n        return ProjectCodeOwners.objects.create(\n            project=project, repository_project_path_config=code_mapping, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_slack_integration(\n        organization: Organization, external_id: str, **kwargs: Any\n    ) -> Integration:\n        integration = Integration.objects.create(\n            provider=\"slack\",\n            name=\"Team A\",\n            external_id=external_id,\n            metadata={\n                \"access_token\": \"xoxp-xxxxxxxxx-xxxxxxxxxx-xxxxxxxxxxxx\",\n                \"installation_type\": \"born_as_bot\",\n            },\n        )\n        integration.add_organization(organization)\n        return integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_integration(\n        organization: Organization,\n        external_id: str,\n        oi_params: Mapping[str, Any] | None = None,\n        **integration_params: Any,\n    ) -> Integration:\n        integration = Integration.objects.create(external_id=external_id, **integration_params)\n        with outbox_runner():\n            organization_integration = integration.add_organization(organization)\n            assert organization_integration is not None\n        organization_integration.update(**(oi_params or {}))\n\n        return integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_provider_integration(**integration_params: Any) -> Integration:\n        return Integration.objects.create(**integration_params)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_provider_integration_for(\n        organization: Organization | RpcOrganization,\n        user: User | RpcUser | None,\n        **integration_params: Any,\n    ) -> tuple[Integration, OrganizationIntegration]:\n        integration = Integration.objects.create(**integration_params)\n        org_integration = integration.add_organization(organization, user)\n        assert org_integration is not None\n        return integration, org_integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity_integration(\n        user: User | RpcUser,\n        organization: Organization | RpcOrganization,\n        integration_params: Mapping[Any, Any],\n        identity_params: Mapping[Any, Any],\n    ) -> tuple[Integration, OrganizationIntegration, Identity, IdentityProvider]:\n        # Avoid common pitfalls in tests\n        assert \"provider\" in integration_params\n        assert \"external_id\" in integration_params\n        assert \"external_id\" in identity_params\n\n        integration = Factories.create_provider_integration(**integration_params)\n        identity_provider = Factories.create_identity_provider(integration=integration)\n        identity = Factories.create_identity(\n            user=user, identity_provider=identity_provider, **identity_params\n        )\n        organization_integration = integration.add_organization(\n            organization_id=organization.id, user=user, default_auth_id=identity.id\n        )\n        assert organization_integration is not None\n        return integration, organization_integration, identity, identity_provider\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_organization_integration(**integration_params: Any) -> OrganizationIntegration:\n        return OrganizationIntegration.objects.create(**integration_params)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity_provider(\n        integration: Integration | None = None,\n        config: dict[str, Any] | None = None,\n        **kwargs: Any,\n    ) -> IdentityProvider:\n        if integration is not None:\n            integration_values = dict(\n                type=integration.provider,\n                external_id=integration.external_id,\n            )\n            if any((key in kwargs) for key in integration_values):\n                raise ValueError(\n                    \"Values from integration should not be in kwargs: \"\n                    + repr(list(integration_values.keys()))\n                )\n            kwargs.update(integration_values)\n\n        return IdentityProvider.objects.create(config=config or {}, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity(\n        user: User | RpcUser, identity_provider: IdentityProvider, external_id: str, **kwargs: Any\n    ) -> Identity:\n        return Identity.objects.create(\n            external_id=external_id,\n            idp=identity_provider,\n            user_id=user.id,\n            status=IdentityStatus.VALID,\n            scopes=[],\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_group_history(\n        group: Group,\n        status: int,\n        release: Release | None = None,\n        user_id: int | None = None,\n        team_id: int | None = None,\n        prev_history: GroupHistory | None = None,\n        date_added: datetime | None = None,\n    ) -> GroupHistory:\n        prev_history_date = None\n        if prev_history:\n            prev_history_date = prev_history.date_added\n\n        kwargs = {}\n        if date_added:\n            kwargs[\"date_added\"] = date_added\n        return GroupHistory.objects.create(\n            organization=group.organization,\n            group=group,\n            project=group.project,\n            release=release,\n            user_id=user_id,\n            team_id=team_id,\n            status=status,\n            prev_history=prev_history,\n            prev_history_date=prev_history_date,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_comment(issue, project, user, text=\"hello world\"):\n        data = {\"text\": text}\n        return Activity.objects.create(\n            project=project,\n            group=issue,\n            type=ActivityType.NOTE.value,\n            user_id=user.id,\n            data=data,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_saved_search(name: str, **kwargs):\n        if \"owner\" in kwargs:\n            owner = kwargs.pop(\"owner\")\n            kwargs[\"owner_id\"] = owner.id if not isinstance(owner, int) else owner\n        return SavedSearch.objects.create(name=name, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_notification_action(\n        organization: Organization | None = None,\n        projects: list[Project] | None = None,\n        **kwargs,\n    ):\n        if not organization:\n            organization = Factories.create_organization()\n\n        if not projects:\n            projects = []\n\n        action_kwargs = {\n            \"organization\": organization,\n            \"type\": ActionService.SENTRY_NOTIFICATION,\n            \"target_type\": ActionTarget.USER,\n            \"target_identifier\": \"1\",\n            \"target_display\": \"Sentry User\",\n            \"trigger_type\": ActionTrigger.AUDIT_LOG,\n            **kwargs,\n        }\n\n        action = NotificationAction.objects.create(**action_kwargs)\n        action.projects.add(*projects)\n        action.save()\n\n        return action\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_notification_settings_provider(*args, **kwargs) -> NotificationSettingProvider:\n        return NotificationSettingProvider.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_option(*args, **kwargs) -> UserOption:\n        return UserOption.objects.create(*args, **kwargs)\n\n    @staticmethod\n    def create_basic_auth_header(username: str, password: str = \"\") -> bytes:\n        return b\"Basic \" + b64encode(f\"{username}:{password}\".encode())\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def snooze_rule(**kwargs):\n        return RuleSnooze.objects.create(**kwargs)\n\n    @staticmethod\n    def create_request_access(\n        sso_state: RpcMemberSsoState | None = None,\n        permissions: list | None = None,\n        org_context: RpcUserOrganizationContext | None = None,\n        scopes_upper_bound: frozenset | None = frozenset(),\n    ) -> RpcBackedAccess:\n        if not sso_state:\n            sso_state = RpcMemberSsoState()\n        if not permissions:\n            permissions = []\n        if not org_context:\n            org_context = RpcUserOrganizationContext()\n\n        auth_state = RpcAuthState(sso_state=sso_state, permissions=permissions)\n        return RpcBackedAccess(\n            rpc_user_organization_context=org_context,\n            auth_state=auth_state,\n            scopes_upper_bound=scopes_upper_bound,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_webhook_payload(mailbox_name: str, region_name: str, **kwargs) -> WebhookPayload:\n        payload_kwargs = {\n            \"request_method\": \"POST\",\n            \"request_path\": \"/extensions/github/webhook/\",\n            \"request_headers\": '{\"Content-Type\": \"application/json\"}',\n            \"request_body\": \"{}\",\n            **kwargs,\n        }\n        return WebhookPayload.objects.create(\n            mailbox_name=mailbox_name, region_name=region_name, **payload_kwargs\n        )\n\n    @staticmethod\n    def create_uptime_subscription(\n        type: str,\n        subscription_id: str | None,\n        status: UptimeSubscription.Status,\n        url: str | None,\n        url_domain: str,\n        url_domain_suffix: str,\n        host_provider_id: str,\n        host_provider_name: str,\n        interval_seconds: IntervalSecondsLiteral,\n        timeout_ms: int,\n        method,\n        headers,\n        body,\n        date_updated: datetime,\n        uptime_status: UptimeStatus,\n        uptime_status_update_date: datetime,\n        trace_sampling: bool = False,\n    ):\n        if url is None:\n            url = petname.generate().title()\n            url = f\"http://{url}.com\"\n\n        return UptimeSubscription.objects.create(\n            type=type,\n            subscription_id=subscription_id,\n            status=status.value,\n            url=url,\n            url_domain=url_domain,\n            url_domain_suffix=url_domain_suffix,\n            host_provider_id=host_provider_id,\n            host_provider_name=host_provider_name,\n            interval_seconds=interval_seconds,\n            timeout_ms=timeout_ms,\n            date_updated=date_updated,\n            method=method,\n            headers=headers,\n            body=body,\n            trace_sampling=trace_sampling,\n            uptime_status=uptime_status,\n            uptime_status_update_date=uptime_status_update_date,\n        )\n\n    @staticmethod\n    def create_project_uptime_subscription(\n        project: Project,\n        env: Environment | None,\n        uptime_subscription: UptimeSubscription,\n        status: int,\n        mode: UptimeMonitorMode,\n        name: str | None,\n        owner: Actor | None,\n        id: int | None,\n    ):\n        if name is None:\n            name = petname.generate().title()\n        owner_team_id = None\n        owner_user_id = None\n        if owner:\n            if owner.is_team:\n                owner_team_id = owner.id\n            elif owner.is_user:\n                owner_user_id = owner.id\n\n        return ProjectUptimeSubscription.objects.create(\n            uptime_subscription=uptime_subscription,\n            project=project,\n            environment=env,\n            status=status,\n            mode=mode,\n            name=name,\n            owner_team_id=owner_team_id,\n            owner_user_id=owner_user_id,\n            pk=id,\n        )\n\n    @staticmethod\n    def create_uptime_subscription_region(\n        subscription: UptimeSubscription,\n        region_slug: str,\n        mode: UptimeSubscriptionRegion.RegionMode,\n    ) -> UptimeSubscriptionRegion:\n        return UptimeSubscriptionRegion.objects.create(\n            uptime_subscription=subscription,\n            region_slug=region_slug,\n            mode=mode,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard(\n        organization: Organization | None = None,\n        title: str | None = None,\n        created_by: User | None = None,\n        **kwargs,\n    ):\n        if organization is None:\n            organization = Factories.create_organization()\n        if created_by is None:\n            created_by = Factories.create_user()\n            Factories.create_member(organization=organization, user=created_by, role=\"owner\")\n        if title is None:\n            title = petname.generate(2, \" \", letters=10).title()\n        return Dashboard.objects.create(\n            organization=organization, title=title, created_by_id=created_by.id, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard_widget(\n        order: int,\n        dashboard: Dashboard | None = None,\n        title: str | None = None,\n        display_type: int | None = None,\n        **kwargs,\n    ):\n        if dashboard is None:\n            dashboard = Factories.create_dashboard()\n        if display_type is None:\n            display_type = DashboardWidgetDisplayTypes.AREA_CHART\n        if title is None:\n            title = petname.generate(2, \" \", letters=10).title()\n\n        return DashboardWidget.objects.create(\n            dashboard=dashboard, title=title, display_type=display_type, order=order, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard_widget_query(\n        order: int,\n        widget: DashboardWidget | None = None,\n        name: str | None = None,\n        **kwargs,\n    ):\n        if widget is None:\n            widget = Factories.create_dashboard_widget(order=order)\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        return DashboardWidgetQuery.objects.create(widget=widget, name=name, order=order, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_workflow(\n        name: str | None = None,\n        organization: Organization | None = None,\n        config: dict[str, Any] | None = None,\n        **kwargs,\n    ) -> Workflow:\n        if organization is None:\n            organization = Factories.create_organization()\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        if config is None:\n            config = {}\n        return Workflow.objects.create(\n            organization=organization, name=name, config=config, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition_group(\n        organization: Organization | None = None,\n        **kwargs,\n    ) -> DataConditionGroup:\n        if organization is None:\n            organization = Factories.create_organization()\n        return DataConditionGroup.objects.create(organization=organization, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_workflow_data_condition_group(\n        workflow: Workflow | None = None,\n        condition_group: DataConditionGroup | None = None,\n        **kwargs,\n    ) -> WorkflowDataConditionGroup:\n        if workflow is None:\n            workflow = Factories.create_workflow()\n\n        if not condition_group:\n            condition_group = Factories.create_data_condition_group()\n\n        return WorkflowDataConditionGroup.objects.create(\n            workflow=workflow, condition_group=condition_group\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition(\n        condition_group: DataConditionGroup | None = None, **kwargs\n    ) -> DataCondition:\n        if condition_group is None:\n            condition_group = Factories.create_data_condition_group()\n        return DataCondition.objects.create(condition_group=condition_group, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_source(\n        organization: Organization | None = None,\n        source_id: str | None = None,\n        type: str | None = None,\n        **kwargs,\n    ) -> DataSource:\n        if organization is None:\n            organization = Factories.create_organization()\n        if source_id is None:\n            source_id = str(random.randint(1, 10000))\n        if type is None:\n            type = data_source_type_registry.get_key(QuerySubscriptionDataSourceHandler)\n        return DataSource.objects.create(organization=organization, source_id=source_id, type=type)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector(\n        name: str | None = None,\n        config: dict | None = None,\n        **kwargs,\n    ) -> Detector:\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        if config is None:\n            config = default_detector_config_data.get(kwargs[\"type\"], {})\n\n        return Detector.objects.create(\n            name=name,\n            config=config,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector_state(\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> DetectorState:\n        if detector is None:\n            detector = Factories.create_detector()\n\n        return DetectorState.objects.create(detector=detector, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_source_detector(\n        data_source: DataSource | None = None,\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> DataSourceDetector:\n        if data_source is None:\n            data_source = Factories.create_data_source()\n        if detector is None:\n            detector = Factories.create_detector()\n        return DataSourceDetector.objects.create(data_source=data_source, detector=detector)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_action(\n        config: dict[str, Any] | None = None,\n        type: Action.Type | None = None,\n        data: dict[str, Any] | None = None,\n        **kwargs,\n    ) -> Action:\n        if config is None and type is None and data is None:\n            # Default to a slack action with nice defaults so someone can just do\n            # self.create_action() and have a sane default\n            config = {\n                \"target_identifier\": \"1\",\n                \"target_display\": \"Sentry User\",\n                \"target_type\": ActionTarget.SPECIFIC,\n            }\n\n            data = {\"notes\": \"bufos are great\", \"tags\": \"bufo-bot\"}\n\n        if config is None:\n            config = {}\n\n        if data is None:\n            data = {}\n\n        if type is None:\n            type = Action.Type.SLACK\n\n        return Action.objects.create(type=type, config=config, data=data, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector_workflow(\n        detector: Detector | None = None,\n        workflow: Workflow | None = None,\n        **kwargs,\n    ) -> DetectorWorkflow:\n        if detector is None:\n            detector = Factories.create_detector()\n        if workflow is None:\n            workflow = Factories.create_workflow()\n        return DetectorWorkflow.objects.create(detector=detector, workflow=workflow, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_workflow(\n        alert_rule_id: int | None = None,\n        rule_id: int | None = None,\n        workflow: Workflow | None = None,\n        **kwargs,\n    ) -> AlertRuleWorkflow:\n        if rule_id is None and alert_rule_id is None:\n            raise ValueError(\"Either rule_id or alert_rule_id must be provided\")\n\n        if rule_id is not None and alert_rule_id is not None:\n            raise ValueError(\"Only one of rule_id or alert_rule_id can be provided\")\n\n        if workflow is None:\n            workflow = Factories.create_workflow()\n\n        return AlertRuleWorkflow.objects.create(\n            alert_rule_id=alert_rule_id, rule_id=rule_id, workflow=workflow, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_group_open_period(\n        incident: Incident,\n        group_open_period: GroupOpenPeriod,\n        **kwargs,\n    ) -> IncidentGroupOpenPeriod:\n        return IncidentGroupOpenPeriod.objects.create(\n            incident_id=incident.id,\n            incident_identifier=incident.identifier,\n            group_open_period=group_open_period,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_detector(\n        alert_rule_id: int | None = None,\n        rule_id: int | None = None,\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> AlertRuleDetector:\n        if rule_id is None and alert_rule_id is None:\n            raise ValueError(\"Either rule_id or alert_rule_id must be provided\")\n\n        if rule_id is not None and alert_rule_id is not None:\n            raise ValueError(\"Only one of rule_id or alert_rule_id can be provided\")\n\n        if detector is None:\n            detector = Factories.create_detector()\n\n        return AlertRuleDetector.objects.create(\n            alert_rule_id=alert_rule_id, rule_id=rule_id, detector=detector, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_action_alert_rule_trigger_action(\n        alert_rule_trigger_action_id: int,\n        action: Action | None = None,\n        **kwargs,\n    ) -> ActionAlertRuleTriggerAction:\n        if action is None:\n            action = Factories.create_action()\n\n        return ActionAlertRuleTriggerAction.objects.create(\n            action=action, alert_rule_trigger_action_id=alert_rule_trigger_action_id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition_group_action(\n        action: Action | None = None,\n        condition_group: DataConditionGroup | None = None,\n        **kwargs,\n    ) -> DataConditionGroupAction:\n        if action is None:\n            action = Factories.create_action()\n        if condition_group is None:\n            condition_group = Factories.create_data_condition_group()\n        return DataConditionGroupAction.objects.create(\n            action=action, condition_group=condition_group, **kwargs\n        )\n",
    "file_path": "/Users/wangyue/Code/CodeReviewData/ReviewDataset/sentry-greptile/src/sentry/testutils/factories.py",
    "line_count": 2375,
    "encoding": "utf-8",
    "error": null
}

Message 5 [Assistant]:
--------------------------------------------------------------------------------
Content:
文件内容太长，让我直接查看 `_set_sample_rate_from_error_sampling` 函数的实现：
Tool Calls:
[
  {
    "name": "read_file",
    "args": {
      "file_path": "src/sentry/testutils/factories.py",
      "encoding": "utf-8"
    },
    "id": "call_00_bqOR5CWR3mXYn3vVNwKOCPzT",
    "type": "tool_call"
  }
]

Message 6 [Tool]:
--------------------------------------------------------------------------------
Tool: read_file
Tool Call ID: call_00_bqOR5CWR3mXYn3vVNwKOCPzT
Result:
{
    "content": "from __future__ import annotations\n\nimport contextlib\nimport copy\nimport io\nimport os\nimport random\nimport zipfile\nfrom base64 import b64encode\nfrom binascii import hexlify\nfrom collections.abc import Mapping, MutableMapping, Sequence\nfrom datetime import UTC, datetime\nfrom enum import Enum\nfrom hashlib import sha1\nfrom importlib import import_module\nfrom typing import Any\nfrom unittest import mock\nfrom uuid import uuid4\n\nimport orjson\nimport petname\nfrom django.conf import settings\nfrom django.contrib.auth.models import AnonymousUser\nfrom django.core.files.base import ContentFile\nfrom django.db import router, transaction\nfrom django.test.utils import override_settings\nfrom django.utils import timezone\nfrom django.utils.text import slugify\n\nfrom sentry.auth.access import RpcBackedAccess\nfrom sentry.auth.services.auth.model import RpcAuthState, RpcMemberSsoState\nfrom sentry.constants import SentryAppInstallationStatus, SentryAppStatus\nfrom sentry.event_manager import EventManager\nfrom sentry.eventstore.models import Event\nfrom sentry.hybridcloud.models.outbox import RegionOutbox, outbox_context\nfrom sentry.hybridcloud.models.webhookpayload import WebhookPayload\nfrom sentry.hybridcloud.outbox.category import OutboxCategory, OutboxScope\nfrom sentry.incidents.grouptype import MetricIssue\nfrom sentry.incidents.logic import (\n    create_alert_rule,\n    create_alert_rule_trigger,\n    create_alert_rule_trigger_action,\n    query_datasets_to_type,\n)\nfrom sentry.incidents.models.alert_rule import (\n    AlertRuleDetectionType,\n    AlertRuleThresholdType,\n    AlertRuleTriggerAction,\n)\nfrom sentry.incidents.models.incident import (\n    Incident,\n    IncidentActivity,\n    IncidentProject,\n    IncidentTrigger,\n    IncidentType,\n    TriggerStatus,\n)\nfrom sentry.integrations.models.doc_integration import DocIntegration\nfrom sentry.integrations.models.doc_integration_avatar import DocIntegrationAvatar\nfrom sentry.integrations.models.external_actor import ExternalActor\nfrom sentry.integrations.models.external_issue import ExternalIssue\nfrom sentry.integrations.models.integration import Integration\nfrom sentry.integrations.models.integration_external_project import IntegrationExternalProject\nfrom sentry.integrations.models.integration_feature import (\n    Feature,\n    IntegrationFeature,\n    IntegrationTypes,\n)\nfrom sentry.integrations.models.organization_integration import OrganizationIntegration\nfrom sentry.integrations.models.repository_project_path_config import RepositoryProjectPathConfig\nfrom sentry.integrations.types import ExternalProviders\nfrom sentry.issues.grouptype import get_group_type_by_type_id\nfrom sentry.models.activity import Activity\nfrom sentry.models.apikey import ApiKey\nfrom sentry.models.apitoken import ApiToken\nfrom sentry.models.artifactbundle import ArtifactBundle\nfrom sentry.models.authidentity import AuthIdentity\nfrom sentry.models.authprovider import AuthProvider\nfrom sentry.models.commit import Commit\nfrom sentry.models.commitauthor import CommitAuthor\nfrom sentry.models.commitfilechange import CommitFileChange\nfrom sentry.models.dashboard import Dashboard\nfrom sentry.models.dashboard_widget import (\n    DashboardWidget,\n    DashboardWidgetDisplayTypes,\n    DashboardWidgetQuery,\n)\nfrom sentry.models.debugfile import ProjectDebugFile\nfrom sentry.models.environment import Environment\nfrom sentry.models.files.control_file import ControlFile\nfrom sentry.models.files.file import File\nfrom sentry.models.group import Group\nfrom sentry.models.grouphistory import GroupHistory\nfrom sentry.models.grouplink import GroupLink\nfrom sentry.models.groupopenperiod import GroupOpenPeriod\nfrom sentry.models.grouprelease import GroupRelease\nfrom sentry.models.organization import Organization\nfrom sentry.models.organizationmapping import OrganizationMapping\nfrom sentry.models.organizationmember import OrganizationMember\nfrom sentry.models.organizationmemberinvite import OrganizationMemberInvite\nfrom sentry.models.organizationmemberteam import OrganizationMemberTeam\nfrom sentry.models.organizationslugreservation import OrganizationSlugReservation\nfrom sentry.models.orgauthtoken import OrgAuthToken\nfrom sentry.models.project import Project\nfrom sentry.models.projectbookmark import ProjectBookmark\nfrom sentry.models.projectcodeowners import ProjectCodeOwners\nfrom sentry.models.projecttemplate import ProjectTemplate\nfrom sentry.models.release import Release, ReleaseStatus\nfrom sentry.models.releasecommit import ReleaseCommit\nfrom sentry.models.releaseenvironment import ReleaseEnvironment\nfrom sentry.models.releasefile import ReleaseFile, update_artifact_index\nfrom sentry.models.releaseprojectenvironment import ReleaseProjectEnvironment\nfrom sentry.models.repository import Repository\nfrom sentry.models.rule import Rule\nfrom sentry.models.rulesnooze import RuleSnooze\nfrom sentry.models.savedsearch import SavedSearch\nfrom sentry.models.team import Team\nfrom sentry.models.userreport import UserReport\nfrom sentry.notifications.models.notificationaction import (\n    ActionService,\n    ActionTarget,\n    ActionTrigger,\n    NotificationAction,\n)\nfrom sentry.notifications.models.notificationsettingprovider import NotificationSettingProvider\nfrom sentry.organizations.services.organization import RpcOrganization, RpcUserOrganizationContext\nfrom sentry.performance_issues.performance_problem import PerformanceProblem\nfrom sentry.sentry_apps.installations import (\n    SentryAppInstallationCreator,\n    SentryAppInstallationTokenCreator,\n)\nfrom sentry.sentry_apps.logic import SentryAppCreator\nfrom sentry.sentry_apps.models.platformexternalissue import PlatformExternalIssue\nfrom sentry.sentry_apps.models.sentry_app import SentryApp\nfrom sentry.sentry_apps.models.sentry_app_avatar import SentryAppAvatar\nfrom sentry.sentry_apps.models.sentry_app_installation import SentryAppInstallation\nfrom sentry.sentry_apps.models.sentry_app_installation_for_provider import (\n    SentryAppInstallationForProvider,\n)\nfrom sentry.sentry_apps.models.servicehook import ServiceHook\nfrom sentry.sentry_apps.services.hook import hook_service\nfrom sentry.sentry_apps.token_exchange.grant_exchanger import GrantExchanger\nfrom sentry.signals import project_created\nfrom sentry.silo.base import SiloMode\nfrom sentry.snuba.dataset import Dataset\nfrom sentry.snuba.models import QuerySubscriptionDataSourceHandler\nfrom sentry.tempest.models import MessageType as TempestMessageType\nfrom sentry.tempest.models import TempestCredentials\nfrom sentry.testutils.outbox import outbox_runner\nfrom sentry.testutils.silo import assume_test_silo_mode\nfrom sentry.types.activity import ActivityType\nfrom sentry.types.actor import Actor\nfrom sentry.types.region import Region, get_local_region, get_region_by_name\nfrom sentry.types.token import AuthTokenType\nfrom sentry.uptime.models import (\n    IntervalSecondsLiteral,\n    ProjectUptimeSubscription,\n    UptimeStatus,\n    UptimeSubscription,\n    UptimeSubscriptionRegion,\n)\nfrom sentry.uptime.types import UptimeMonitorMode\nfrom sentry.users.models.identity import Identity, IdentityProvider, IdentityStatus\nfrom sentry.users.models.user import User\nfrom sentry.users.models.user_avatar import UserAvatar\nfrom sentry.users.models.user_option import UserOption\nfrom sentry.users.models.useremail import UserEmail\nfrom sentry.users.models.userpermission import UserPermission\nfrom sentry.users.models.userrole import UserRole\nfrom sentry.users.services.user import RpcUser\nfrom sentry.utils import loremipsum\nfrom sentry.workflow_engine.models import (\n    Action,\n    ActionAlertRuleTriggerAction,\n    AlertRuleDetector,\n    AlertRuleWorkflow,\n    DataCondition,\n    DataConditionGroup,\n    DataConditionGroupAction,\n    DataSource,\n    DataSourceDetector,\n    Detector,\n    DetectorState,\n    DetectorWorkflow,\n    IncidentGroupOpenPeriod,\n    Workflow,\n    WorkflowDataConditionGroup,\n)\nfrom sentry.workflow_engine.registry import data_source_type_registry\nfrom social_auth.models import UserSocialAuth\n\n\nclass EventType(Enum):\n    ERROR = \"error\"\n    DEFAULT = \"default\"\n\n\ndef get_fixture_path(*parts: str) -> str:\n    path = os.path.realpath(__file__)\n    for _ in range(4):  # src/sentry/testutils/{__file__}\n        path = os.path.dirname(path)\n    return os.path.join(path, \"fixtures\", *parts)\n\n\ndef make_sentence(words=None):\n    if words is None:\n        words = int(random.weibullvariate(8, 3))\n    return \" \".join(random.choice(loremipsum.words) for _ in range(words))\n\n\ndef make_word(words=None):\n    if words is None:\n        words = int(random.weibullvariate(8, 3))\n    return random.choice(loremipsum.words)\n\n\nDEFAULT_EVENT_DATA = {\n    \"extra\": {\n        \"loadavg\": [0.97607421875, 0.88330078125, 0.833984375],\n        \"sys.argv\": [\n            \"/Users/dcramer/.virtualenvs/sentry/bin/raven\",\n            \"test\",\n            \"https://ebc35f33e151401f9deac549978bda11:f3403f81e12e4c24942d505f086b2cad@sentry.io/1\",\n        ],\n        \"user\": \"dcramer\",\n    },\n    \"modules\": {\"raven\": \"3.1.13\"},\n    \"request\": {\n        \"cookies\": {},\n        \"data\": {},\n        \"env\": {},\n        \"headers\": {},\n        \"method\": \"GET\",\n        \"query_string\": \"\",\n        \"url\": \"http://example.com\",\n    },\n    \"stacktrace\": {\n        \"frames\": [\n            {\n                \"abs_path\": \"www/src/sentry/models/foo.py\",\n                \"context_line\": \"                        string_max_length=self.string_max_length)\",\n                \"filename\": \"sentry/models/foo.py\",\n                \"function\": \"build_msg\",\n                \"in_app\": True,\n                \"lineno\": 29,\n                \"module\": \"raven.base\",\n                \"post_context\": [\n                    \"                },\",\n                    \"            })\",\n                    \"\",\n                    \"        if 'stacktrace' in data:\",\n                    \"            if self.include_paths:\",\n                ],\n                \"pre_context\": [\n                    \"\",\n                    \"            data.update({\",\n                    \"                'stacktrace': {\",\n                    \"                    'frames': get_stack_info(frames,\",\n                    \"                        list_max_length=self.list_max_length,\",\n                ],\n                \"vars\": {\n                    \"culprit\": \"raven.scripts.runner\",\n                    \"date\": \"datetime.datetime(2013, 2, 14, 20, 6, 33, 479471)\",\n                    \"event_id\": \"598fb19363e745ec8be665e6ba88b1b2\",\n                    \"event_type\": \"raven.events.Message\",\n                    \"frames\": \"<generator object iter_stack_frames at 0x103fef050>\",\n                    \"handler\": \"<raven.events.Message object at 0x103feb710>\",\n                    \"k\": \"logentry\",\n                    \"public_key\": None,\n                    \"result\": {\n                        \"logentry\": \"{'message': 'This is a test message generated using ``raven test``', 'params': []}\"\n                    },\n                    \"self\": \"<raven.base.Client object at 0x104397f10>\",\n                    \"stack\": True,\n                    \"tags\": None,\n                    \"time_spent\": None,\n                },\n            },\n            {\n                \"abs_path\": \"/Users/dcramer/.virtualenvs/sentry/lib/python2.7/site-packages/raven/base.py\",\n                \"context_line\": \"                        string_max_length=self.string_max_length)\",\n                \"filename\": \"raven/base.py\",\n                \"function\": \"build_msg\",\n                \"in_app\": False,\n                \"lineno\": 290,\n                \"module\": \"raven.base\",\n                \"post_context\": [\n                    \"                },\",\n                    \"            })\",\n                    \"\",\n                    \"        if 'stacktrace' in data:\",\n                    \"            if self.include_paths:\",\n                ],\n                \"pre_context\": [\n                    \"\",\n                    \"            data.update({\",\n                    \"                'stacktrace': {\",\n                    \"                    'frames': get_stack_info(frames,\",\n                    \"                        list_max_length=self.list_max_length,\",\n                ],\n                \"vars\": {\n                    \"culprit\": \"raven.scripts.runner\",\n                    \"date\": \"datetime.datetime(2013, 2, 14, 20, 6, 33, 479471)\",\n                    \"event_id\": \"598fb19363e745ec8be665e6ba88b1b2\",\n                    \"event_type\": \"raven.events.Message\",\n                    \"frames\": \"<generator object iter_stack_frames at 0x103fef050>\",\n                    \"handler\": \"<raven.events.Message object at 0x103feb710>\",\n                    \"k\": \"logentry\",\n                    \"public_key\": None,\n                    \"result\": {\n                        \"logentry\": \"{'message': 'This is a test message generated using ``raven test``', 'params': []}\"\n                    },\n                    \"self\": \"<raven.base.Client object at 0x104397f10>\",\n                    \"stack\": True,\n                    \"tags\": None,\n                    \"time_spent\": None,\n                },\n            },\n        ]\n    },\n    \"tags\": [],\n    \"platform\": \"python\",\n}\n\ndefault_detector_config_data = {\n    MetricIssue.slug: {\"threshold_period\": 1, \"detection_type\": \"static\"}\n}\n\n\ndef _patch_artifact_manifest(path, org=None, release=None, project=None, extra_files=None):\n    with open(path, \"rb\") as fp:\n        manifest = orjson.loads(fp.read())\n    if org:\n        manifest[\"org\"] = org\n    if release:\n        manifest[\"release\"] = release\n    if project:\n        manifest[\"project\"] = project\n    for path in extra_files or {}:\n        manifest[\"files\"][path] = {\"url\": path}\n    return orjson.dumps(manifest).decode()\n\n\ndef _set_sample_rate_from_error_sampling(normalized_data: MutableMapping[str, Any]) -> None:\n    \"\"\"Set 'sample_rate' on normalized_data if contexts.error_sampling.client_sample_rate is present and valid.\"\"\"\n    client_sample_rate = None\n    try:\n        client_sample_rate = (\n            normalized_data.get(\"contexts\", {}).get(\"error_sampling\", {}).get(\"client_sample_rate\")\n        )\n    except Exception:\n        pass\n    if client_sample_rate:\n        try:\n            normalized_data[\"sample_rate\"] = float(client_sample_rate)\n        except Exception:\n            pass\n\n\n# TODO(dcramer): consider moving to something more scalable like factoryboy\nclass Factories:\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_organization(name=None, owner=None, region: Region | str | None = None, **kwargs):\n        if not name:\n            name = petname.generate(2, \" \", letters=10).title()\n\n        with contextlib.ExitStack() as ctx:\n            if region is None or SiloMode.get_current_mode() == SiloMode.MONOLITH:\n                region_name = get_local_region().name\n            else:\n                if isinstance(region, Region):\n                    region_name = region.name\n                else:\n                    region_obj = get_region_by_name(region)  # Verify it exists\n                    region_name = region_obj.name\n\n                ctx.enter_context(\n                    override_settings(SILO_MODE=SiloMode.REGION, SENTRY_REGION=region_name)\n                )\n\n            with outbox_context(flush=False):\n                org = Organization.objects.create(name=name, **kwargs)\n\n            with assume_test_silo_mode(SiloMode.CONTROL):\n                # Organization mapping creation relies on having a matching org slug reservation\n                OrganizationSlugReservation(\n                    organization_id=org.id,\n                    region_name=region_name,\n                    user_id=owner.id if owner else -1,\n                    slug=org.slug,\n                ).save(unsafe_write=True)\n\n            # Manually replicate org data after adding an org slug reservation\n            org.handle_async_replication(org.id)\n\n            # Flush remaining organization update outboxes accumulated by org create\n            RegionOutbox(\n                shard_identifier=org.id,\n                shard_scope=OutboxScope.ORGANIZATION_SCOPE,\n                category=OutboxCategory.ORGANIZATION_UPDATE,\n            ).drain_shard()\n\n        if owner:\n            Factories.create_member(organization=org, user_id=owner.id, role=\"owner\")\n        return org\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_org_mapping(org=None, **kwds):\n        if org:\n            kwds.setdefault(\"organization_id\", org.id)\n            kwds.setdefault(\"slug\", org.slug)\n            kwds.setdefault(\"name\", org.name)\n            kwds.setdefault(\"idempotency_key\", uuid4().hex)\n            kwds.setdefault(\"region_name\", \"na\")\n        return OrganizationMapping.objects.create(**kwds)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_member(teams=None, team_roles=None, **kwargs):\n        kwargs.setdefault(\"role\", \"member\")\n        teamRole = kwargs.pop(\"teamRole\", None)\n\n        # user_id will have precedence over user\n        user = kwargs.pop(\"user\", None)\n        user_id = kwargs.pop(\"user_id\", None)\n        if not user_id and user:\n            user_id = user.id\n        kwargs[\"user_id\"] = user_id\n\n        # inviter_id will have precedence over inviter\n        inviter = kwargs.pop(\"inviter\", None)\n        inviter_id = kwargs.pop(\"inviter_id\", None)\n        if not inviter_id and inviter:\n            inviter_id = inviter.id\n        kwargs[\"inviter_id\"] = inviter_id\n\n        om = OrganizationMember.objects.create(**kwargs)\n\n        if team_roles:\n            for team, role in team_roles:\n                Factories.create_team_membership(team=team, member=om, role=role)\n        elif teams:\n            for team in teams:\n                Factories.create_team_membership(team=team, member=om, role=teamRole)\n        return om\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_member_invite(\n        organization: Organization | None = None,\n        email: str | None = None,\n        **kwargs,\n    ) -> OrganizationMemberInvite:\n        if organization is None:\n            organization = Factories.create_organization()\n        if email is None:\n            email = f\"{petname.generate().title()}@email.com\"\n        om = OrganizationMember.objects.create(organization=organization)\n        return OrganizationMemberInvite.objects.create(\n            organization=organization, organization_member_id=om.id, email=email, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_team_membership(team, member=None, user=None, role=None):\n        if member is None:\n            member, created = OrganizationMember.objects.get_or_create(\n                user_id=user.id if user else None,\n                organization=team.organization,\n                defaults={\"role\": \"member\"},\n            )\n\n        return OrganizationMemberTeam.objects.create(\n            team=team, organizationmember=member, is_active=True, role=role\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_api_key(organization, **kwargs) -> ApiKey:\n        return ApiKey.objects.create(organization_id=organization.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_auth_provider(**kwargs):\n        return AuthProvider.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_auth_identity(**kwargs):\n        return AuthIdentity.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_auth_token(user, scope_list: list[str] | None = None, **kwargs) -> ApiToken:\n        if scope_list is None:\n            scope_list = []\n        return ApiToken.objects.create(\n            user=user,\n            scope_list=scope_list,\n            token_type=AuthTokenType.USER,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_org_auth_token(*args, **kwargs) -> OrgAuthToken:\n        return OrgAuthToken.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_team(organization, **kwargs):\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n        if not kwargs.get(\"slug\"):\n            kwargs[\"slug\"] = slugify(str(kwargs[\"name\"]))\n        members = kwargs.pop(\"members\", None)\n\n        team = Team.objects.create(organization=organization, **kwargs)\n        if members:\n            for user in members:\n                Factories.create_team_membership(team=team, user=user)\n        return team\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_environment(project, **kwargs):\n        name = kwargs.get(\"name\", petname.generate(3, \" \", letters=10)[:64])\n\n        organization = kwargs.get(\"organization\")\n        organization_id = organization.id if organization else project.organization_id\n\n        env = Environment.objects.create(organization_id=organization_id, name=name)\n        env.add_project(project, is_hidden=kwargs.get(\"is_hidden\"))\n        return env\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project(\n        organization=None, teams=None, fire_project_created=False, **kwargs\n    ) -> Project:\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n        if not kwargs.get(\"slug\"):\n            kwargs[\"slug\"] = slugify(str(kwargs[\"name\"]))\n        if not organization and teams:\n            organization = teams[0].organization\n\n        with transaction.atomic(router.db_for_write(Project)):\n            project = Project.objects.create(organization=organization, **kwargs)\n            if teams:\n                for team in teams:\n                    project.add_team(team)\n            if fire_project_created:\n                project_created.send(\n                    project=project, user=AnonymousUser(), default_rules=True, sender=Factories\n                )\n        return project\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_template(project=None, organization=None, **kwargs) -> ProjectTemplate:\n        if not kwargs.get(\"name\"):\n            kwargs[\"name\"] = petname.generate(2, \" \", letters=10).title()\n\n        with transaction.atomic(router.db_for_write(Project)):\n            project_template = ProjectTemplate.objects.create(organization=organization, **kwargs)\n\n        return project_template\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_bookmark(project, user):\n        return ProjectBookmark.objects.create(project_id=project.id, user_id=user.id)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_rule(\n        project,\n        action_data=None,\n        allow_no_action_data=False,\n        condition_data=None,\n        name=\"Test Alert\",\n        action_match=\"all\",\n        filter_match=\"all\",\n        frequency=30,\n        **kwargs,\n    ):\n        actions = None\n        if not allow_no_action_data:\n            action_data = action_data or [\n                {\n                    \"id\": \"sentry.rules.actions.notify_event.NotifyEventAction\",\n                    \"name\": \"Send a notification (for all legacy integrations)\",\n                },\n                {\n                    \"id\": \"sentry.rules.actions.notify_event_service.NotifyEventServiceAction\",\n                    \"service\": \"mail\",\n                    \"name\": \"Send a notification via mail\",\n                },\n            ]\n            actions = action_data\n        condition_data = condition_data or [\n            {\n                \"id\": \"sentry.rules.conditions.first_seen_event.FirstSeenEventCondition\",\n                \"name\": \"A new issue is created\",\n            },\n            {\n                \"id\": \"sentry.rules.conditions.every_event.EveryEventCondition\",\n                \"name\": \"The event occurs\",\n            },\n        ]\n        data = {\n            \"conditions\": condition_data,\n            \"action_match\": action_match,\n            \"filter_match\": filter_match,\n            \"frequency\": frequency,\n        }\n        if actions:\n            data[\"actions\"] = actions\n\n        return Rule.objects.create(\n            label=name,\n            project=project,\n            data=data,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_slack_project_rule(project, integration_id, channel_id=None, channel_name=None):\n        action_data = [\n            {\n                \"id\": \"sentry.rules.actions.notify_event.SlackNotifyServiceAction\",\n                \"name\": \"Send a Slack notification\",\n                \"workspace\": integration_id,\n                \"channel_id\": channel_id or \"123453\",\n                \"channel\": channel_name or \"#general\",\n            }\n        ]\n        return Factories.create_project_rule(project, action_data)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_project_key(project):\n        return project.key_set.get_or_create()[0]\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_tempest_credentials(\n        project: Project,\n        created_by: User | None = None,\n        client_id: str | None = None,\n        client_secret: str | None = None,\n        message: str = \"\",\n        message_type: str | None = None,\n        latest_fetched_item_id: str | None = None,\n    ):\n        if client_id is None:\n            client_id = str(uuid4())\n        if client_secret is None:\n            client_secret = str(uuid4())\n        if message_type is None:\n            message_type = TempestMessageType.ERROR\n\n        return TempestCredentials.objects.create(\n            project=project,\n            created_by_id=created_by.id if created_by else None,\n            client_id=client_id,\n            client_secret=client_secret,\n            message=message,\n            message_type=message_type,\n            latest_fetched_item_id=latest_fetched_item_id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release(\n        project: Project,\n        user: User | None = None,\n        version: str | None = None,\n        date_added: datetime | None = None,\n        additional_projects: Sequence[Project] | None = None,\n        environments: Sequence[Environment] | None = None,\n        date_released: datetime | None = None,\n        adopted: datetime | None = None,\n        unadopted: datetime | None = None,\n        status: int | None = ReleaseStatus.OPEN,\n    ):\n        if version is None:\n            version = hexlify(os.urandom(20)).decode()\n\n        if date_added is None:\n            date_added = timezone.now()\n\n        if additional_projects is None:\n            additional_projects = []\n\n        release = Release.objects.create(\n            version=version,\n            organization_id=project.organization_id,\n            date_added=date_added,\n            date_released=date_released,\n            status=status,\n        )\n\n        release.add_project(project)\n        for additional_project in additional_projects:\n            release.add_project(additional_project)\n\n        for environment in environments or []:\n            ReleaseEnvironment.objects.create(\n                organization=project.organization, release=release, environment=environment\n            )\n            for project in [project, *additional_projects]:\n                ReleaseProjectEnvironment.objects.create(\n                    project=project,\n                    release=release,\n                    environment=environment,\n                    adopted=adopted,\n                    unadopted=unadopted,\n                )\n\n        Activity.objects.create(\n            type=ActivityType.RELEASE.value,\n            project=project,\n            ident=Activity.get_version_ident(version),\n            user_id=user.id if user else None,\n            data={\"version\": version},\n        )\n\n        # add commits\n        if user:\n            author = Factories.create_commit_author(project=project, user=user)\n            repo = Factories.create_repo(project, name=f\"organization-{project.slug}\")\n            commit = Factories.create_commit(\n                project=project,\n                repo=repo,\n                author=author,\n                release=release,\n                key=\"deadbeef\",\n                message=\"placeholder commit message\",\n            )\n\n            release.update(authors=[str(author.id)], commit_count=1, last_commit_id=commit.id)\n\n        return release\n\n    @staticmethod\n    def create_group_release(project: Project, group: Group, release: Release) -> GroupRelease:\n        return GroupRelease.objects.create(\n            project_id=project.id,\n            group_id=group.id,\n            release_id=release.id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release_file(release_id, file=None, name=None, dist_id=None):\n        if file is None:\n            file = Factories.create_file(\n                name=\"log.txt\",\n                size=32,\n                headers={\"Content-Type\": \"text/plain\"},\n                checksum=\"dc1e3f3e411979d336c3057cce64294f3420f93a\",\n            )\n\n        if name is None:\n            name = file.name\n\n        organization_id = Release.objects.get(pk=release_id).organization.id\n\n        return ReleaseFile.objects.create(\n            organization_id=organization_id,\n            release_id=release_id,\n            name=name,\n            file=file,\n            dist_id=dist_id,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_artifact_bundle_zip(\n        org=None, release=None, project=None, extra_files=None, fixture_path=\"artifact_bundle\"\n    ):\n        bundle = io.BytesIO()\n        bundle_dir = get_fixture_path(fixture_path)\n        with zipfile.ZipFile(bundle, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n            for path, content in (extra_files or {}).items():\n                zipf.writestr(path, content)\n            for path, _, files in os.walk(bundle_dir):\n                for filename in files:\n                    fullpath = os.path.join(path, filename)\n                    relpath = os.path.relpath(fullpath, bundle_dir)\n                    if filename == \"manifest.json\":\n                        manifest = _patch_artifact_manifest(\n                            fullpath, org, release, project, extra_files\n                        )\n                        zipf.writestr(relpath, manifest)\n                    else:\n                        zipf.write(fullpath, relpath)\n\n        return bundle.getvalue()\n\n    @classmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_release_archive(cls, org, release: str, project=None, dist=None):\n        bundle = cls.create_artifact_bundle_zip(org, release, project)\n        file = File.objects.create(name=\"release-artifacts.zip\")\n        file.putfile(ContentFile(bundle))\n        release_obj = Release.objects.get(organization__slug=org, version=release)\n        return update_artifact_index(release_obj, dist, file)\n\n    @classmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_artifact_bundle(\n        cls,\n        org,\n        bundle_id=None,\n        artifact_count=0,\n        fixture_path=\"artifact_bundle_debug_ids\",\n        date_uploaded=None,\n        date_last_modified=None,\n    ):\n        if date_uploaded is None:\n            date_uploaded = timezone.now()\n\n        bundle = cls.create_artifact_bundle_zip(org.slug, fixture_path=fixture_path)\n        file_ = File.objects.create(name=\"artifact-bundle.zip\")\n        file_.putfile(ContentFile(bundle))\n        # The 'artifact_count' should correspond to the 'bundle' contents but for the purpose of tests we can also\n        # mock it with an arbitrary value.\n        artifact_bundle = ArtifactBundle.objects.create(\n            organization_id=org.id,\n            bundle_id=bundle_id or uuid4(),\n            file=file_,\n            artifact_count=artifact_count,\n            date_uploaded=date_uploaded,\n            date_last_modified=date_last_modified,\n        )\n        return artifact_bundle\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_code_mapping(project, repo=None, organization_integration=None, **kwargs):\n        kwargs.setdefault(\"stack_root\", \"\")\n        kwargs.setdefault(\"source_root\", \"\")\n        kwargs.setdefault(\"default_branch\", \"master\")\n\n        if not repo:\n            repo = Factories.create_repo(project=project)\n        return RepositoryProjectPathConfig.objects.create(\n            project=project,\n            repository=repo,\n            organization_integration_id=organization_integration.id,\n            integration_id=organization_integration.integration_id,\n            organization_id=organization_integration.organization_id,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_repo(\n        project, name=None, provider=None, integration_id=None, url=None, external_id=None\n    ):\n        repo, _ = Repository.objects.get_or_create(\n            organization_id=project.organization_id,\n            name=name\n            or \"{}-{}\".format(petname.generate(2, \"\", letters=10), random.randint(1000, 9999)),\n            provider=provider,\n            integration_id=integration_id,\n            url=url,\n            external_id=external_id,\n        )\n        return repo\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit(\n        repo, project=None, author=None, release=None, message=None, key=None, date_added=None\n    ):\n        commit = Commit.objects.get_or_create(\n            organization_id=repo.organization_id,\n            repository_id=repo.id,\n            key=key or sha1(uuid4().hex.encode(\"utf-8\")).hexdigest(),\n            defaults={\n                \"message\": message or make_sentence(),\n                \"author\": author\n                or Factories.create_commit_author(organization_id=repo.organization_id),\n                \"date_added\": date_added or timezone.now(),\n            },\n        )[0]\n\n        if release:\n            assert project\n            ReleaseCommit.objects.create(\n                organization_id=repo.organization_id,\n                project_id=project.id,\n                release=release,\n                commit=commit,\n                order=1,\n            )\n\n        Factories.create_commit_file_change(commit=commit, filename=\"/models/foo.py\")\n        Factories.create_commit_file_change(commit=commit, filename=\"/worsematch/foo.py\")\n        Factories.create_commit_file_change(commit=commit, filename=\"/models/other.py\")\n\n        return commit\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit_author(organization_id=None, project=None, user=None, email=None):\n        if email:\n            user_email = email\n        else:\n            user_email = user.email if user else f\"{make_word()}@example.com\"\n        return CommitAuthor.objects.get_or_create(\n            organization_id=organization_id or project.organization_id,\n            email=user_email,\n            defaults={\"name\": user.name if user else make_word()},\n        )[0]\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_commit_file_change(commit, filename):\n        return CommitFileChange.objects.get_or_create(\n            organization_id=commit.organization_id, commit=commit, filename=filename, type=\"M\"\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user(\n        email=None, is_superuser=False, is_staff=False, is_active=True, **kwargs\n    ) -> User:\n        if email is None:\n            email = uuid4().hex + \"@example.com\"\n\n        kwargs.setdefault(\"username\", email)\n\n        user = User(\n            email=email, is_superuser=is_superuser, is_staff=is_staff, is_active=is_active, **kwargs\n        )\n        if kwargs.get(\"password\") is None:\n            user.set_password(\"admin\")\n        user.save()\n\n        # UserEmail is created by a signal\n        assert UserEmail.objects.filter(user=user, email=email).update(is_verified=True)\n\n        return user\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_useremail(user, email=None, **kwargs):\n        if not email:\n            email = uuid4().hex + \"@example.com\"\n\n        kwargs.setdefault(\"is_verified\", True)\n\n        useremail = UserEmail(user=user, email=email, **kwargs)\n        useremail.save()\n\n        return useremail\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_avatar(*args, **kwargs):\n        return UserAvatar.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_role(*args, **kwargs):\n        return UserRole.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_usersocialauth(\n        user: User,\n        provider: str | None = None,\n        uid: str | None = None,\n        extra_data: dict[str, Any] | None = None,\n    ):\n        if not provider:\n            provider = \"asana\"\n        if not uid:\n            uid = \"abc-123\"\n        usa = UserSocialAuth(user=user, provider=provider, uid=uid, extra_data=extra_data)\n        usa.save()\n        return usa\n\n    @staticmethod\n    def inject_performance_problems(jobs, _):\n        for job in jobs:\n            job[\"performance_problems\"] = []\n            for f in job[\"data\"][\"fingerprint\"]:\n                f_data = f.split(\"-\", 1)\n                if len(f_data) < 2:\n                    raise ValueError(\n                        \"Invalid performance fingerprint data. Format must be 'group_type-fingerprint'.\"\n                    )\n                group_type = get_group_type_by_type_id(int(f_data[0]))\n                perf_fingerprint = f_data[1]\n\n                job[\"performance_problems\"].append(\n                    PerformanceProblem(\n                        fingerprint=perf_fingerprint,\n                        op=\"db\",\n                        desc=\"\",\n                        type=group_type,\n                        parent_span_ids=None,\n                        cause_span_ids=None,\n                        offender_span_ids=[],\n                        evidence_data={},\n                        evidence_display=[],\n                    )\n                )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def store_event(\n        data,\n        project_id: int,\n        assert_no_errors: bool = True,\n        default_event_type: EventType | None = None,\n        sent_at: datetime | None = None,\n    ) -> Event:\n        \"\"\"\n        Like `create_event`, but closer to how events are actually\n        ingested. Prefer to use this method over `create_event`\n        \"\"\"\n\n        # this creates a basic message event\n        if default_event_type == EventType.DEFAULT:\n            data.update({\"stacktrace\": copy.deepcopy(DEFAULT_EVENT_DATA[\"stacktrace\"])})\n\n        # this creates an error event\n        elif default_event_type == EventType.ERROR:\n            data.update({\"exception\": [{\"value\": \"BadError\"}]})\n\n        manager = EventManager(data, sent_at=sent_at)\n        manager.normalize()\n        if assert_no_errors:\n            errors = manager.get_data().get(\"errors\")\n            assert not errors, errors\n\n        normalized_data = manager.get_data()\n\n        _set_sample_rate_from_error_sampling(normalized_data)\n\n        event = None\n\n        # When fingerprint is present on transaction, inject performance problems\n        if (\n            normalized_data.get(\"type\") == \"transaction\"\n            and normalized_data.get(\"fingerprint\") is not None\n        ):\n            with mock.patch(\n                \"sentry.event_manager._detect_performance_problems\",\n                Factories.inject_performance_problems,\n            ):\n                event = manager.save(project_id)\n\n        else:\n            event = manager.save(project_id)\n\n        if event.groups:\n            for group in event.groups:\n                group.save()\n\n        if event.group:\n            event.group.save()\n\n        return event\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_group(project, create_open_period=True, **kwargs):\n        from sentry.models.group import GroupStatus\n        from sentry.models.groupopenperiod import GroupOpenPeriod\n        from sentry.testutils.helpers.datetime import before_now\n        from sentry.types.group import GroupSubStatus\n\n        kwargs.setdefault(\"message\", \"Hello world\")\n        kwargs.setdefault(\"data\", {})\n        if \"type\" not in kwargs[\"data\"]:\n            kwargs[\"data\"].update({\"type\": \"default\", \"metadata\": {\"title\": kwargs[\"message\"]}})\n        if \"short_id\" not in kwargs:\n            kwargs[\"short_id\"] = project.next_short_id()\n        if \"metadata\" in kwargs:\n            metadata = kwargs.pop(\"metadata\")\n            kwargs[\"data\"].setdefault(\"metadata\", {}).update(metadata)\n        if \"status\" not in kwargs:\n            kwargs[\"status\"] = GroupStatus.UNRESOLVED\n            kwargs[\"substatus\"] = GroupSubStatus.NEW\n\n        group = Group.objects.create(project=project, **kwargs)\n        if create_open_period:\n            open_period = GroupOpenPeriod.objects.create(\n                group=group,\n                project=project,\n                date_started=group.first_seen or before_now(minutes=5),\n            )\n            if group.status == GroupStatus.RESOLVED:\n                open_period.update(\n                    date_ended=group.resolved_at if group.resolved_at else timezone.now()\n                )\n\n        return group\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_file(**kwargs):\n        return File.objects.create(**kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_file_from_path(path, name=None, **kwargs):\n        if name is None:\n            name = os.path.basename(path)\n\n        file = Factories.create_file(name=name, **kwargs)\n        with open(path) as f:\n            file.putfile(f)\n        return file\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dif_file(\n        project,\n        debug_id=None,\n        object_name=None,\n        features=None,\n        data=None,\n        file=None,\n        cpu_name=None,\n        code_id=None,\n        **kwargs,\n    ):\n        if debug_id is None:\n            debug_id = str(uuid4())\n\n        if object_name is None:\n            object_name = \"%s.dSYM\" % debug_id\n\n        if features is not None:\n            if data is None:\n                data = {}\n            data[\"features\"] = features\n\n        if file is None:\n            file = Factories.create_file(\n                name=object_name,\n                size=42,\n                headers={\"Content-Type\": \"application/x-mach-binary\"},\n                checksum=\"dc1e3f3e411979d336c3057cce64294f3420f93a\",\n            )\n\n        return ProjectDebugFile.objects.create(\n            debug_id=debug_id,\n            code_id=code_id,\n            project_id=project.id,\n            object_name=object_name,\n            cpu_name=cpu_name or \"x86_64\",\n            file=file,\n            checksum=file.checksum,\n            data=data,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dif_from_path(path, object_name=None, **kwargs):\n        if object_name is None:\n            object_name = os.path.basename(path)\n\n        headers = {\"Content-Type\": \"application/x-mach-binary\"}\n        file = Factories.create_file_from_path(path, name=object_name, headers=headers)\n        return Factories.create_dif_file(file=file, object_name=object_name, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def add_user_permission(user, permission):\n        UserPermission.objects.create(user=user, permission=permission)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app(**kwargs):\n        published = kwargs.pop(\"published\", False)\n        args = Factories._sentry_app_kwargs(**kwargs)\n        user = args.pop(\"user\", None)\n        app = SentryAppCreator(is_internal=False, **args).run(user=user, request=None)\n\n        if published:\n            app.update(status=SentryAppStatus.PUBLISHED)\n\n        return app\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_avatar(*args, **kwargs):\n        return SentryAppAvatar.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_internal_integration(**kwargs) -> SentryApp:\n        args = Factories._sentry_app_kwargs(**kwargs)\n        args[\"verify_install\"] = False\n        user = args.pop(\"user\", None)\n        app = SentryAppCreator(is_internal=True, **args).run(\n            user=user, request=None, skip_default_auth_token=True\n        )\n        return app\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_internal_integration_token(\n        user,\n        internal_integration: SentryApp | None = None,\n        install: SentryAppInstallation | None = None,\n        request=None,\n    ) -> ApiToken:\n        if internal_integration and install:\n            raise ValueError(\"Only one of internal_integration or install arg can be provided\")\n        elif internal_integration is None and install is None:\n            raise ValueError(\"Must pass in either internal_integration or install arg\")\n\n        if internal_integration is not None and install is None:\n            # Fetch install from provided or created internal integration\n            with assume_test_silo_mode(SiloMode.CONTROL):\n                install = SentryAppInstallation.objects.get(\n                    sentry_app=internal_integration.id,\n                    organization_id=internal_integration.owner_id,\n                )\n        elif install is None:\n            raise AssertionError(\"unreachable\")\n\n        return SentryAppInstallationTokenCreator(sentry_app_installation=install).run(\n            user=user, request=request\n        )\n\n    @staticmethod\n    def _sentry_app_kwargs(**kwargs):\n        _kwargs = {\n            \"user\": kwargs.get(\"user\", Factories.create_user()),\n            \"name\": kwargs.get(\"name\", petname.generate(2, \" \", letters=10).title()),\n            \"organization_id\": kwargs.get(\n                \"organization_id\", kwargs.pop(\"organization\", Factories.create_organization()).id\n            ),\n            \"author\": kwargs.get(\"author\", \"A Company\"),\n            \"scopes\": kwargs.get(\"scopes\", ()),\n            \"verify_install\": kwargs.get(\"verify_install\", True),\n            \"webhook_url\": kwargs.get(\"webhook_url\", \"https://example.com/webhook\"),\n            \"events\": [],\n            \"schema\": {},\n        }\n\n        _kwargs.update(**kwargs)\n        return _kwargs\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_sentry_app_installation(\n        organization=None,\n        slug=None,\n        user=None,\n        status=None,\n        prevent_token_exchange=False,\n    ):\n        if not organization:\n            organization = Factories.create_organization()\n\n        Factories.create_project(organization=organization)\n\n        with assume_test_silo_mode(SiloMode.CONTROL):\n            install = SentryAppInstallationCreator(\n                slug=(slug or Factories.create_sentry_app(organization=organization).slug),\n                organization_id=organization.id,\n            ).run(\n                user=(user or Factories.create_user()),\n                request=None,\n            )\n\n            install.status = SentryAppInstallationStatus.INSTALLED if status is None else status\n            install.save()\n            if not prevent_token_exchange and (\n                install.sentry_app.status != SentryAppStatus.INTERNAL\n            ):\n                assert install.api_grant is not None\n                assert install.sentry_app.application is not None\n                assert install.sentry_app.proxy_user is not None\n                GrantExchanger(\n                    install=install,\n                    code=install.api_grant.code,\n                    client_id=install.sentry_app.application.client_id,\n                    user=install.sentry_app.proxy_user,\n                ).run()\n                install = SentryAppInstallation.objects.get(id=install.id)\n        return install\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_installation_for_provider(\n        sentry_app_id: int,\n        organization_id: int,\n        provider: str,\n    ) -> SentryAppInstallationForProvider:\n        installation = SentryAppInstallation.objects.get(\n            sentry_app_id=sentry_app_id, organization_id=organization_id\n        )\n        return SentryAppInstallationForProvider.objects.create(\n            organization_id=organization_id,\n            provider=provider,\n            sentry_app_installation=installation,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_stacktrace_link_schema():\n        return {\"type\": \"stacktrace-link\", \"uri\": \"/redirect/\"}\n\n    @staticmethod\n    def create_issue_link_schema():\n        return {\n            \"type\": \"issue-link\",\n            \"link\": {\n                \"uri\": \"/sentry/issues/link\",\n                \"required_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    }\n                ],\n            },\n            \"create\": {\n                \"uri\": \"/sentry/issues/create\",\n                \"required_fields\": [\n                    {\"type\": \"text\", \"name\": \"title\", \"label\": \"Title\"},\n                    {\"type\": \"text\", \"name\": \"summary\", \"label\": \"Summary\"},\n                ],\n                \"optional_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"points\",\n                        \"label\": \"Points\",\n                        \"options\": [[\"1\", \"1\"], [\"2\", \"2\"], [\"3\", \"3\"], [\"5\", \"5\"], [\"8\", \"8\"]],\n                    },\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    },\n                ],\n            },\n        }\n\n    @staticmethod\n    def create_alert_rule_action_schema():\n        return {\n            \"type\": \"alert-rule-action\",\n            \"title\": \"Create Task with App\",\n            \"settings\": {\n                \"type\": \"alert-rule-settings\",\n                \"uri\": \"/sentry/alert-rule\",\n                \"required_fields\": [\n                    {\"type\": \"text\", \"name\": \"title\", \"label\": \"Title\"},\n                    {\"type\": \"text\", \"name\": \"summary\", \"label\": \"Summary\"},\n                ],\n                \"optional_fields\": [\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"points\",\n                        \"label\": \"Points\",\n                        \"options\": [[\"1\", \"1\"], [\"2\", \"2\"], [\"3\", \"3\"], [\"5\", \"5\"], [\"8\", \"8\"]],\n                    },\n                    {\n                        \"type\": \"select\",\n                        \"name\": \"assignee\",\n                        \"label\": \"Assignee\",\n                        \"uri\": \"/sentry/members\",\n                    },\n                ],\n            },\n        }\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_service_hook(\n        actor=None, org=None, project=None, events=None, url=None, project_ids=None, **kwargs\n    ):\n        if project:\n            if project_ids is not None:\n                raise ValueError(\"Cannot provide both project and project_ids\")\n            project_ids = [project.id]\n        if not actor:\n            actor = Factories.create_user()\n        if not org:\n            if project:\n                org = project.organization\n            else:\n                org = Factories.create_organization(owner=actor)\n        if project_ids is None:  # empty list for project_ids is valid and means no project filter\n            project_ids = [Factories.create_project(organization=org).id]\n        if events is None:\n            events = [\"event.created\"]\n        if not url:\n            url = \"https://example.com/sentry/webhook\"\n\n        app_id = kwargs.pop(\"application_id\", None)\n        if app_id is None and \"application\" in kwargs:\n            app_id = kwargs[\"application\"].id\n        installation_id = kwargs.pop(\"installation_id\", None)\n        if installation_id is None and \"installation\" in kwargs:\n            installation_id = kwargs[\"installation\"].id\n        hook_id = hook_service.create_service_hook(\n            application_id=app_id,\n            actor_id=actor.id,\n            installation_id=installation_id,\n            organization_id=org.id,\n            project_ids=project_ids,\n            events=events,\n            url=url,\n        ).id\n        return ServiceHook.objects.get(id=hook_id)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_sentry_app_feature(feature=None, sentry_app=None, description=None):\n        if not sentry_app:\n            sentry_app = Factories.create_sentry_app()\n\n        integration_feature = IntegrationFeature.objects.create(\n            target_id=sentry_app.id,\n            target_type=IntegrationTypes.SENTRY_APP.value,\n            feature=feature or Feature.API,\n        )\n\n        if description:\n            integration_feature.update(user_description=description)\n\n        return integration_feature\n\n    @staticmethod\n    def _doc_integration_kwargs(**kwargs):\n        _kwargs = {\n            \"name\": kwargs.get(\"name\", petname.generate(2, \" \", letters=10).title()),\n            \"author\": kwargs.get(\"author\", \"me\"),\n            \"description\": kwargs.get(\"description\", \"hi im a description\"),\n            \"url\": kwargs.get(\"url\", \"https://sentry.io\"),\n            \"popularity\": kwargs.get(\"popularity\", 1),\n            \"is_draft\": kwargs.get(\"is_draft\", True),\n            \"metadata\": kwargs.get(\"metadata\", {}),\n        }\n        _kwargs[\"slug\"] = slugify(_kwargs[\"name\"])\n        _kwargs.update(**kwargs)\n        return _kwargs\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration(features=None, has_avatar: bool = False, **kwargs) -> DocIntegration:\n        doc = DocIntegration.objects.create(**Factories._doc_integration_kwargs(**kwargs))\n        if features:\n            Factories.create_doc_integration_features(features=features, doc_integration=doc)\n        if has_avatar:\n            Factories.create_doc_integration_avatar(doc_integration=doc)\n        return doc\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration_features(\n        features=None, doc_integration=None\n    ) -> list[IntegrationFeature]:\n        if not features:\n            features = [Feature.API]\n        if not doc_integration:\n            doc_integration = Factories.create_doc_integration()\n        return IntegrationFeature.objects.bulk_create(\n            [\n                IntegrationFeature(\n                    target_id=doc_integration.id,\n                    target_type=IntegrationTypes.DOC_INTEGRATION.value,\n                    feature=feature,\n                )\n                for feature in features\n            ]\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_doc_integration_avatar(doc_integration=None, **kwargs) -> DocIntegrationAvatar:\n        if not doc_integration:\n            doc_integration = Factories.create_doc_integration()\n        photo = ControlFile.objects.create(name=\"test.png\", type=\"avatar.file\")\n        photo.putfile(io.BytesIO(b\"imaginethiswasphotobytes\"))\n\n        return DocIntegrationAvatar.objects.create(\n            doc_integration=doc_integration, avatar_type=0, control_file_id=photo.id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_userreport(\n        project: Project, event_id: str | None = None, **kwargs: Any\n    ) -> UserReport:\n        event = Factories.store_event(\n            data={\n                \"timestamp\": datetime.now(UTC).isoformat(),\n                \"event_id\": event_id or \"a\" * 32,\n                \"message\": \"testing\",\n            },\n            project_id=project.id,\n        )\n        assert event.group is not None\n\n        return UserReport.objects.create(\n            group_id=event.group.id,\n            event_id=event.event_id,\n            project_id=project.id,\n            name=\"Jane Bloggs\",\n            email=\"jane@example.com\",\n            comments=\"the application crashed\",\n            **kwargs,\n        )\n\n    @staticmethod\n    def create_session():\n        engine = import_module(settings.SESSION_ENGINE)\n\n        session = engine.SessionStore()\n        session.save()\n        return session\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_platform_external_issue(\n        group=None, service_type=None, display_name=None, web_url=None\n    ):\n        return PlatformExternalIssue.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            service_type=service_type,\n            display_name=display_name,\n            web_url=web_url,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_integration_external_issue(group=None, integration=None, key=None, **kwargs):\n        external_issue = ExternalIssue.objects.create(\n            organization_id=group.organization.id, integration_id=integration.id, key=key, **kwargs\n        )\n\n        GroupLink.objects.create(\n            group_id=group.id,\n            project_id=group.project_id,\n            linked_type=GroupLink.LinkedType.issue,\n            linked_id=external_issue.id,\n            relationship=GroupLink.Relationship.references,\n        )\n\n        return external_issue\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_integration_external_project(\n        organization_id: int, integration_id: int, *args: Any, **kwargs: Any\n    ) -> IntegrationExternalProject:\n        oi = OrganizationIntegration.objects.get(\n            organization_id=organization_id, integration_id=integration_id\n        )\n        return IntegrationExternalProject.objects.create(\n            organization_integration_id=oi.id, *args, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident(\n        organization,\n        projects,\n        detection_uuid=None,\n        status=1,\n        title=None,\n        query=\"test query\",\n        date_started=None,\n        date_detected=None,\n        date_closed=None,\n        alert_rule=None,\n        subscription=None,\n    ):\n        if not title:\n            title = petname.generate(2, \" \", letters=10).title()\n        if alert_rule is None:\n            alert_rule = Factories.create_alert_rule(\n                organization, projects, query=query, time_window=1\n            )\n\n        incident = Incident.objects.create(\n            organization=organization,\n            detection_uuid=detection_uuid,\n            status=status,\n            title=title,\n            alert_rule=alert_rule,\n            date_started=date_started or timezone.now(),\n            date_detected=date_detected or timezone.now(),\n            date_closed=timezone.now() if date_closed is not None else date_closed,\n            type=IncidentType.ALERT_TRIGGERED.value,\n            subscription=subscription,\n        )\n        for project in projects:\n            IncidentProject.objects.create(incident=incident, project=project)\n\n        return incident\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_activity(incident, type, comment=None, user_id=None):\n        return IncidentActivity.objects.create(\n            incident=incident, type=type, comment=comment, user_id=user_id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule(\n        organization,\n        projects,\n        name=None,\n        owner=None,\n        query=\"level:error\",\n        aggregate=\"count()\",\n        time_window=10,\n        threshold_period=1,\n        environment=None,\n        date_added=None,\n        query_type=None,\n        dataset=Dataset.Events,\n        threshold_type=AlertRuleThresholdType.ABOVE,\n        resolve_threshold=None,\n        user=None,\n        event_types=None,\n        comparison_delta=None,\n        description=None,\n        sensitivity=None,\n        seasonality=None,\n        detection_type=AlertRuleDetectionType.STATIC,\n    ):\n        if not name:\n            name = petname.generate(2, \" \", letters=10).title()\n\n        if query_type is None:\n            query_type = query_datasets_to_type[dataset]\n\n        alert_rule = create_alert_rule(\n            organization,\n            projects,\n            name,\n            query,\n            aggregate,\n            time_window,\n            threshold_type,\n            threshold_period,\n            owner=owner,\n            resolve_threshold=resolve_threshold,\n            query_type=query_type,\n            dataset=dataset,\n            environment=environment,\n            user=user,\n            event_types=event_types,\n            comparison_delta=comparison_delta,\n            description=description,\n            sensitivity=sensitivity,\n            seasonality=seasonality,\n            detection_type=detection_type,\n        )\n\n        if date_added is not None:\n            alert_rule.update(date_added=date_added)\n\n        return alert_rule\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_trigger(alert_rule, label=None, alert_threshold=100):\n        if not label:\n            label = petname.generate(2, \" \", letters=10).title()\n\n        return create_alert_rule_trigger(alert_rule, label, alert_threshold)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_trigger(incident, alert_rule_trigger, status=None):\n        if status is None:\n            status = TriggerStatus.ACTIVE.value\n\n        return IncidentTrigger.objects.create(\n            alert_rule_trigger=alert_rule_trigger, incident=incident, status=status\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_trigger_action(\n        trigger,\n        type=AlertRuleTriggerAction.Type.EMAIL,\n        target_type=AlertRuleTriggerAction.TargetType.USER,\n        target_identifier=None,\n        integration=None,\n        sentry_app=None,\n        sentry_app_config=None,\n    ):\n        return create_alert_rule_trigger_action(\n            trigger,\n            type,\n            target_type,\n            target_identifier,\n            integration.id if integration else None,\n            sentry_app.id if sentry_app else None,\n            sentry_app_config=sentry_app_config,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_external_user(user: User, **kwargs: Any) -> ExternalActor:\n        kwargs.setdefault(\"provider\", ExternalProviders.GITHUB.value)\n        kwargs.setdefault(\"external_name\", \"\")\n\n        return ExternalActor.objects.create(user_id=user.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_external_team(team: Team, **kwargs: Any) -> ExternalActor:\n        kwargs.setdefault(\"provider\", ExternalProviders.GITHUB.value)\n        kwargs.setdefault(\"external_name\", \"@getsentry/ecosystem\")\n\n        return ExternalActor.objects.create(team_id=team.id, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_codeowners(project, code_mapping, **kwargs):\n        kwargs.setdefault(\"raw\", \"\")\n\n        return ProjectCodeOwners.objects.create(\n            project=project, repository_project_path_config=code_mapping, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_slack_integration(\n        organization: Organization, external_id: str, **kwargs: Any\n    ) -> Integration:\n        integration = Integration.objects.create(\n            provider=\"slack\",\n            name=\"Team A\",\n            external_id=external_id,\n            metadata={\n                \"access_token\": \"xoxp-xxxxxxxxx-xxxxxxxxxx-xxxxxxxxxxxx\",\n                \"installation_type\": \"born_as_bot\",\n            },\n        )\n        integration.add_organization(organization)\n        return integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_integration(\n        organization: Organization,\n        external_id: str,\n        oi_params: Mapping[str, Any] | None = None,\n        **integration_params: Any,\n    ) -> Integration:\n        integration = Integration.objects.create(external_id=external_id, **integration_params)\n        with outbox_runner():\n            organization_integration = integration.add_organization(organization)\n            assert organization_integration is not None\n        organization_integration.update(**(oi_params or {}))\n\n        return integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_provider_integration(**integration_params: Any) -> Integration:\n        return Integration.objects.create(**integration_params)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_provider_integration_for(\n        organization: Organization | RpcOrganization,\n        user: User | RpcUser | None,\n        **integration_params: Any,\n    ) -> tuple[Integration, OrganizationIntegration]:\n        integration = Integration.objects.create(**integration_params)\n        org_integration = integration.add_organization(organization, user)\n        assert org_integration is not None\n        return integration, org_integration\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity_integration(\n        user: User | RpcUser,\n        organization: Organization | RpcOrganization,\n        integration_params: Mapping[Any, Any],\n        identity_params: Mapping[Any, Any],\n    ) -> tuple[Integration, OrganizationIntegration, Identity, IdentityProvider]:\n        # Avoid common pitfalls in tests\n        assert \"provider\" in integration_params\n        assert \"external_id\" in integration_params\n        assert \"external_id\" in identity_params\n\n        integration = Factories.create_provider_integration(**integration_params)\n        identity_provider = Factories.create_identity_provider(integration=integration)\n        identity = Factories.create_identity(\n            user=user, identity_provider=identity_provider, **identity_params\n        )\n        organization_integration = integration.add_organization(\n            organization_id=organization.id, user=user, default_auth_id=identity.id\n        )\n        assert organization_integration is not None\n        return integration, organization_integration, identity, identity_provider\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_organization_integration(**integration_params: Any) -> OrganizationIntegration:\n        return OrganizationIntegration.objects.create(**integration_params)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity_provider(\n        integration: Integration | None = None,\n        config: dict[str, Any] | None = None,\n        **kwargs: Any,\n    ) -> IdentityProvider:\n        if integration is not None:\n            integration_values = dict(\n                type=integration.provider,\n                external_id=integration.external_id,\n            )\n            if any((key in kwargs) for key in integration_values):\n                raise ValueError(\n                    \"Values from integration should not be in kwargs: \"\n                    + repr(list(integration_values.keys()))\n                )\n            kwargs.update(integration_values)\n\n        return IdentityProvider.objects.create(config=config or {}, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_identity(\n        user: User | RpcUser, identity_provider: IdentityProvider, external_id: str, **kwargs: Any\n    ) -> Identity:\n        return Identity.objects.create(\n            external_id=external_id,\n            idp=identity_provider,\n            user_id=user.id,\n            status=IdentityStatus.VALID,\n            scopes=[],\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_group_history(\n        group: Group,\n        status: int,\n        release: Release | None = None,\n        user_id: int | None = None,\n        team_id: int | None = None,\n        prev_history: GroupHistory | None = None,\n        date_added: datetime | None = None,\n    ) -> GroupHistory:\n        prev_history_date = None\n        if prev_history:\n            prev_history_date = prev_history.date_added\n\n        kwargs = {}\n        if date_added:\n            kwargs[\"date_added\"] = date_added\n        return GroupHistory.objects.create(\n            organization=group.organization,\n            group=group,\n            project=group.project,\n            release=release,\n            user_id=user_id,\n            team_id=team_id,\n            status=status,\n            prev_history=prev_history,\n            prev_history_date=prev_history_date,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_comment(issue, project, user, text=\"hello world\"):\n        data = {\"text\": text}\n        return Activity.objects.create(\n            project=project,\n            group=issue,\n            type=ActivityType.NOTE.value,\n            user_id=user.id,\n            data=data,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_saved_search(name: str, **kwargs):\n        if \"owner\" in kwargs:\n            owner = kwargs.pop(\"owner\")\n            kwargs[\"owner_id\"] = owner.id if not isinstance(owner, int) else owner\n        return SavedSearch.objects.create(name=name, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_notification_action(\n        organization: Organization | None = None,\n        projects: list[Project] | None = None,\n        **kwargs,\n    ):\n        if not organization:\n            organization = Factories.create_organization()\n\n        if not projects:\n            projects = []\n\n        action_kwargs = {\n            \"organization\": organization,\n            \"type\": ActionService.SENTRY_NOTIFICATION,\n            \"target_type\": ActionTarget.USER,\n            \"target_identifier\": \"1\",\n            \"target_display\": \"Sentry User\",\n            \"trigger_type\": ActionTrigger.AUDIT_LOG,\n            **kwargs,\n        }\n\n        action = NotificationAction.objects.create(**action_kwargs)\n        action.projects.add(*projects)\n        action.save()\n\n        return action\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_notification_settings_provider(*args, **kwargs) -> NotificationSettingProvider:\n        return NotificationSettingProvider.objects.create(*args, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_user_option(*args, **kwargs) -> UserOption:\n        return UserOption.objects.create(*args, **kwargs)\n\n    @staticmethod\n    def create_basic_auth_header(username: str, password: str = \"\") -> bytes:\n        return b\"Basic \" + b64encode(f\"{username}:{password}\".encode())\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def snooze_rule(**kwargs):\n        return RuleSnooze.objects.create(**kwargs)\n\n    @staticmethod\n    def create_request_access(\n        sso_state: RpcMemberSsoState | None = None,\n        permissions: list | None = None,\n        org_context: RpcUserOrganizationContext | None = None,\n        scopes_upper_bound: frozenset | None = frozenset(),\n    ) -> RpcBackedAccess:\n        if not sso_state:\n            sso_state = RpcMemberSsoState()\n        if not permissions:\n            permissions = []\n        if not org_context:\n            org_context = RpcUserOrganizationContext()\n\n        auth_state = RpcAuthState(sso_state=sso_state, permissions=permissions)\n        return RpcBackedAccess(\n            rpc_user_organization_context=org_context,\n            auth_state=auth_state,\n            scopes_upper_bound=scopes_upper_bound,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.CONTROL)\n    def create_webhook_payload(mailbox_name: str, region_name: str, **kwargs) -> WebhookPayload:\n        payload_kwargs = {\n            \"request_method\": \"POST\",\n            \"request_path\": \"/extensions/github/webhook/\",\n            \"request_headers\": '{\"Content-Type\": \"application/json\"}',\n            \"request_body\": \"{}\",\n            **kwargs,\n        }\n        return WebhookPayload.objects.create(\n            mailbox_name=mailbox_name, region_name=region_name, **payload_kwargs\n        )\n\n    @staticmethod\n    def create_uptime_subscription(\n        type: str,\n        subscription_id: str | None,\n        status: UptimeSubscription.Status,\n        url: str | None,\n        url_domain: str,\n        url_domain_suffix: str,\n        host_provider_id: str,\n        host_provider_name: str,\n        interval_seconds: IntervalSecondsLiteral,\n        timeout_ms: int,\n        method,\n        headers,\n        body,\n        date_updated: datetime,\n        uptime_status: UptimeStatus,\n        uptime_status_update_date: datetime,\n        trace_sampling: bool = False,\n    ):\n        if url is None:\n            url = petname.generate().title()\n            url = f\"http://{url}.com\"\n\n        return UptimeSubscription.objects.create(\n            type=type,\n            subscription_id=subscription_id,\n            status=status.value,\n            url=url,\n            url_domain=url_domain,\n            url_domain_suffix=url_domain_suffix,\n            host_provider_id=host_provider_id,\n            host_provider_name=host_provider_name,\n            interval_seconds=interval_seconds,\n            timeout_ms=timeout_ms,\n            date_updated=date_updated,\n            method=method,\n            headers=headers,\n            body=body,\n            trace_sampling=trace_sampling,\n            uptime_status=uptime_status,\n            uptime_status_update_date=uptime_status_update_date,\n        )\n\n    @staticmethod\n    def create_project_uptime_subscription(\n        project: Project,\n        env: Environment | None,\n        uptime_subscription: UptimeSubscription,\n        status: int,\n        mode: UptimeMonitorMode,\n        name: str | None,\n        owner: Actor | None,\n        id: int | None,\n    ):\n        if name is None:\n            name = petname.generate().title()\n        owner_team_id = None\n        owner_user_id = None\n        if owner:\n            if owner.is_team:\n                owner_team_id = owner.id\n            elif owner.is_user:\n                owner_user_id = owner.id\n\n        return ProjectUptimeSubscription.objects.create(\n            uptime_subscription=uptime_subscription,\n            project=project,\n            environment=env,\n            status=status,\n            mode=mode,\n            name=name,\n            owner_team_id=owner_team_id,\n            owner_user_id=owner_user_id,\n            pk=id,\n        )\n\n    @staticmethod\n    def create_uptime_subscription_region(\n        subscription: UptimeSubscription,\n        region_slug: str,\n        mode: UptimeSubscriptionRegion.RegionMode,\n    ) -> UptimeSubscriptionRegion:\n        return UptimeSubscriptionRegion.objects.create(\n            uptime_subscription=subscription,\n            region_slug=region_slug,\n            mode=mode,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard(\n        organization: Organization | None = None,\n        title: str | None = None,\n        created_by: User | None = None,\n        **kwargs,\n    ):\n        if organization is None:\n            organization = Factories.create_organization()\n        if created_by is None:\n            created_by = Factories.create_user()\n            Factories.create_member(organization=organization, user=created_by, role=\"owner\")\n        if title is None:\n            title = petname.generate(2, \" \", letters=10).title()\n        return Dashboard.objects.create(\n            organization=organization, title=title, created_by_id=created_by.id, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard_widget(\n        order: int,\n        dashboard: Dashboard | None = None,\n        title: str | None = None,\n        display_type: int | None = None,\n        **kwargs,\n    ):\n        if dashboard is None:\n            dashboard = Factories.create_dashboard()\n        if display_type is None:\n            display_type = DashboardWidgetDisplayTypes.AREA_CHART\n        if title is None:\n            title = petname.generate(2, \" \", letters=10).title()\n\n        return DashboardWidget.objects.create(\n            dashboard=dashboard, title=title, display_type=display_type, order=order, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_dashboard_widget_query(\n        order: int,\n        widget: DashboardWidget | None = None,\n        name: str | None = None,\n        **kwargs,\n    ):\n        if widget is None:\n            widget = Factories.create_dashboard_widget(order=order)\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        return DashboardWidgetQuery.objects.create(widget=widget, name=name, order=order, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_workflow(\n        name: str | None = None,\n        organization: Organization | None = None,\n        config: dict[str, Any] | None = None,\n        **kwargs,\n    ) -> Workflow:\n        if organization is None:\n            organization = Factories.create_organization()\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        if config is None:\n            config = {}\n        return Workflow.objects.create(\n            organization=organization, name=name, config=config, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition_group(\n        organization: Organization | None = None,\n        **kwargs,\n    ) -> DataConditionGroup:\n        if organization is None:\n            organization = Factories.create_organization()\n        return DataConditionGroup.objects.create(organization=organization, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_workflow_data_condition_group(\n        workflow: Workflow | None = None,\n        condition_group: DataConditionGroup | None = None,\n        **kwargs,\n    ) -> WorkflowDataConditionGroup:\n        if workflow is None:\n            workflow = Factories.create_workflow()\n\n        if not condition_group:\n            condition_group = Factories.create_data_condition_group()\n\n        return WorkflowDataConditionGroup.objects.create(\n            workflow=workflow, condition_group=condition_group\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition(\n        condition_group: DataConditionGroup | None = None, **kwargs\n    ) -> DataCondition:\n        if condition_group is None:\n            condition_group = Factories.create_data_condition_group()\n        return DataCondition.objects.create(condition_group=condition_group, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_source(\n        organization: Organization | None = None,\n        source_id: str | None = None,\n        type: str | None = None,\n        **kwargs,\n    ) -> DataSource:\n        if organization is None:\n            organization = Factories.create_organization()\n        if source_id is None:\n            source_id = str(random.randint(1, 10000))\n        if type is None:\n            type = data_source_type_registry.get_key(QuerySubscriptionDataSourceHandler)\n        return DataSource.objects.create(organization=organization, source_id=source_id, type=type)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector(\n        name: str | None = None,\n        config: dict | None = None,\n        **kwargs,\n    ) -> Detector:\n        if name is None:\n            name = petname.generate(2, \" \", letters=10).title()\n        if config is None:\n            config = default_detector_config_data.get(kwargs[\"type\"], {})\n\n        return Detector.objects.create(\n            name=name,\n            config=config,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector_state(\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> DetectorState:\n        if detector is None:\n            detector = Factories.create_detector()\n\n        return DetectorState.objects.create(detector=detector, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_source_detector(\n        data_source: DataSource | None = None,\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> DataSourceDetector:\n        if data_source is None:\n            data_source = Factories.create_data_source()\n        if detector is None:\n            detector = Factories.create_detector()\n        return DataSourceDetector.objects.create(data_source=data_source, detector=detector)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_action(\n        config: dict[str, Any] | None = None,\n        type: Action.Type | None = None,\n        data: dict[str, Any] | None = None,\n        **kwargs,\n    ) -> Action:\n        if config is None and type is None and data is None:\n            # Default to a slack action with nice defaults so someone can just do\n            # self.create_action() and have a sane default\n            config = {\n                \"target_identifier\": \"1\",\n                \"target_display\": \"Sentry User\",\n                \"target_type\": ActionTarget.SPECIFIC,\n            }\n\n            data = {\"notes\": \"bufos are great\", \"tags\": \"bufo-bot\"}\n\n        if config is None:\n            config = {}\n\n        if data is None:\n            data = {}\n\n        if type is None:\n            type = Action.Type.SLACK\n\n        return Action.objects.create(type=type, config=config, data=data, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_detector_workflow(\n        detector: Detector | None = None,\n        workflow: Workflow | None = None,\n        **kwargs,\n    ) -> DetectorWorkflow:\n        if detector is None:\n            detector = Factories.create_detector()\n        if workflow is None:\n            workflow = Factories.create_workflow()\n        return DetectorWorkflow.objects.create(detector=detector, workflow=workflow, **kwargs)\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_workflow(\n        alert_rule_id: int | None = None,\n        rule_id: int | None = None,\n        workflow: Workflow | None = None,\n        **kwargs,\n    ) -> AlertRuleWorkflow:\n        if rule_id is None and alert_rule_id is None:\n            raise ValueError(\"Either rule_id or alert_rule_id must be provided\")\n\n        if rule_id is not None and alert_rule_id is not None:\n            raise ValueError(\"Only one of rule_id or alert_rule_id can be provided\")\n\n        if workflow is None:\n            workflow = Factories.create_workflow()\n\n        return AlertRuleWorkflow.objects.create(\n            alert_rule_id=alert_rule_id, rule_id=rule_id, workflow=workflow, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_incident_group_open_period(\n        incident: Incident,\n        group_open_period: GroupOpenPeriod,\n        **kwargs,\n    ) -> IncidentGroupOpenPeriod:\n        return IncidentGroupOpenPeriod.objects.create(\n            incident_id=incident.id,\n            incident_identifier=incident.identifier,\n            group_open_period=group_open_period,\n            **kwargs,\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_alert_rule_detector(\n        alert_rule_id: int | None = None,\n        rule_id: int | None = None,\n        detector: Detector | None = None,\n        **kwargs,\n    ) -> AlertRuleDetector:\n        if rule_id is None and alert_rule_id is None:\n            raise ValueError(\"Either rule_id or alert_rule_id must be provided\")\n\n        if rule_id is not None and alert_rule_id is not None:\n            raise ValueError(\"Only one of rule_id or alert_rule_id can be provided\")\n\n        if detector is None:\n            detector = Factories.create_detector()\n\n        return AlertRuleDetector.objects.create(\n            alert_rule_id=alert_rule_id, rule_id=rule_id, detector=detector, **kwargs\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_action_alert_rule_trigger_action(\n        alert_rule_trigger_action_id: int,\n        action: Action | None = None,\n        **kwargs,\n    ) -> ActionAlertRuleTriggerAction:\n        if action is None:\n            action = Factories.create_action()\n\n        return ActionAlertRuleTriggerAction.objects.create(\n            action=action, alert_rule_trigger_action_id=alert_rule_trigger_action_id\n        )\n\n    @staticmethod\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_data_condition_group_action(\n        action: Action | None = None,\n        condition_group: DataConditionGroup | None = None,\n        **kwargs,\n    ) -> DataConditionGroupAction:\n        if action is None:\n            action = Factories.create_action()\n        if condition_group is None:\n            condition_group = Factories.create_data_condition_group()\n        return DataConditionGroupAction.objects.create(\n            action=action, condition_group=condition_group, **kwargs\n        )\n",
    "file_path": "/Users/wangyue/Code/CodeReviewData/ReviewDataset/sentry-greptile/src/sentry/testutils/factories.py",
    "line_count": 2375,
    "encoding": "utf-8",
    "error": null
}

Message 7 [Assistant]:
--------------------------------------------------------------------------------
Content:
让我搜索 `_set_sample_rate_from_error_sampling` 函数的定义：
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "def _set_sample_rate_from_error_sampling",
      "context_lines": 20
    },
    "id": "call_00_SDCciViFIU67Uwt4sOd7m0zb",
    "type": "tool_call"
  }
]

Message 8 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_SDCciViFIU67Uwt4sOd7m0zb
Result:
{
    "matches": [
        {
            "file": "src/sentry/testutils/factories.py",
            "line_number": 344,
            "matched_line": "def _set_sample_rate_from_error_sampling(normalized_data: MutableMapping[str, Any]) -> None:",
            "context_start_line": 324,
            "context_end_line": 364,
            "context": [
                "324: ",
                "325: default_detector_config_data = {",
                "326:     MetricIssue.slug: {\"threshold_period\": 1, \"detection_type\": \"static\"}",
                "327: }",
                "328: ",
                "329: ",
                "330: def _patch_artifact_manifest(path, org=None, release=None, project=None, extra_files=None):",
                "331:     with open(path, \"rb\") as fp:",
                "332:         manifest = orjson.loads(fp.read())",
                "333:     if org:",
                "334:         manifest[\"org\"] = org",
                "335:     if release:",
                "336:         manifest[\"release\"] = release",
                "337:     if project:",
                "338:         manifest[\"project\"] = project",
                "339:     for path in extra_files or {}:",
                "340:         manifest[\"files\"][path] = {\"url\": path}",
                "341:     return orjson.dumps(manifest).decode()",
                "342: ",
                "343: ",
                "344: def _set_sample_rate_from_error_sampling(normalized_data: MutableMapping[str, Any]) -> None:",
                "345:     \"\"\"Set 'sample_rate' on normalized_data if contexts.error_sampling.client_sample_rate is present and valid.\"\"\"",
                "346:     client_sample_rate = None",
                "347:     try:",
                "348:         client_sample_rate = (",
                "349:             normalized_data.get(\"contexts\", {}).get(\"error_sampling\", {}).get(\"client_sample_rate\")",
                "350:         )",
                "351:     except Exception:",
                "352:         pass",
                "353:     if client_sample_rate:",
                "354:         try:",
                "355:             normalized_data[\"sample_rate\"] = float(client_sample_rate)",
                "356:         except Exception:",
                "357:             pass",
                "358: ",
                "359: ",
                "360: # TODO(dcramer): consider moving to something more scalable like factoryboy",
                "361: class Factories:",
                "362:     @staticmethod",
                "363:     @assume_test_silo_mode(SiloMode.REGION)",
                "364:     def create_organization(name=None, owner=None, region: Region | str | None = None, **kwargs):"
            ]
        }
    ],
    "total": 1
}

Message 9 [Assistant]:
--------------------------------------------------------------------------------
Content:
现在我需要查看 `EventManager` 的 `get_data()` 方法实现，看看它是否返回内部数据的引用。让我搜索 `EventManager` 类：
Tool Calls:
[
  {
    "name": "cpg_symbol_search",
    "args": {
      "query": "EventManager",
      "include_callsites": true,
      "limit": 10
    },
    "id": "call_00_yP2GgkQIREIR14KBoICm1YEF",
    "type": "tool_call"
  }
]

Message 10 [Tool]:
--------------------------------------------------------------------------------
Tool: cpg_symbol_search
Tool Call ID: call_00_yP2GgkQIREIR14KBoICm1YEF
Result:
{
    "rev": "head",
    "query": "EventManager",
    "symbols": [],
    "callsites": [],
    "_budget": {
        "attrs_dropped": true,
        "symbols_total": 0,
        "symbols_returned": 0,
        "symbols_truncated": false,
        "callsites_total": 0,
        "callsites_returned": 0,
        "callsites_truncated": false
    },
    "error": null,
    "_payload": {
        "approx_chars": 247,
        "approx_bytes_utf8": 247,
        "keys": [
            "_budget",
            "callsites",
            "error",
            "query",
            "rev",
            "symbols"
        ]
    }
}

Message 11 [Assistant]:
--------------------------------------------------------------------------------
Content:
让我搜索 `EventManager` 的定义：
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "class EventManager",
      "context_lines": 10
    },
    "id": "call_00_jty7QbP2pUMpIQsGYqT3OYYm",
    "type": "tool_call"
  }
]

Message 12 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_jty7QbP2pUMpIQsGYqT3OYYm
Result:
{
    "matches": [
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 113,
            "matched_line": "class EventManagerTestMixin:",
            "context_start_line": 103,
            "context_end_line": 123,
            "context": [
                "103:     result = {",
                "104:         \"event_id\": uuid.uuid1().hex,",
                "105:         \"level\": logging.ERROR,",
                "106:         \"logger\": \"default\",",
                "107:         \"tags\": [],",
                "108:     }",
                "109:     result.update(kwargs)",
                "110:     return result",
                "111: ",
                "112: ",
                "113: class EventManagerTestMixin:",
                "114:     def make_release_event(self, release_name: str, project_id: int) -> Event:",
                "115:         manager = EventManager(make_event(release=release_name))",
                "116:         manager.normalize()",
                "117:         event = manager.save(project_id)",
                "118:         return event",
                "119: ",
                "120: ",
                "121: class EventManagerTest(TestCase, SnubaTestCase, EventManagerTestMixin, PerformanceIssueTestCase):",
                "122:     def test_ephemeral_interfaces_removed_on_save(self) -> None:",
                "123:         manager = EventManager(make_event(platform=\"python\"))"
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager.py",
            "line_number": 121,
            "matched_line": "class EventManagerTest(TestCase, SnubaTestCase, EventManagerTestMixin, PerformanceIssueTestCase):",
            "context_start_line": 111,
            "context_end_line": 131,
            "context": [
                "111: ",
                "112: ",
                "113: class EventManagerTestMixin:",
                "114:     def make_release_event(self, release_name: str, project_id: int) -> Event:",
                "115:         manager = EventManager(make_event(release=release_name))",
                "116:         manager.normalize()",
                "117:         event = manager.save(project_id)",
                "118:         return event",
                "119: ",
                "120: ",
                "121: class EventManagerTest(TestCase, SnubaTestCase, EventManagerTestMixin, PerformanceIssueTestCase):",
                "122:     def test_ephemeral_interfaces_removed_on_save(self) -> None:",
                "123:         manager = EventManager(make_event(platform=\"python\"))",
                "124:         manager.normalize()",
                "125:         event = manager.save(self.project.id)",
                "126: ",
                "127:         group = event.group",
                "128:         assert group is not None",
                "129:         assert group.platform == \"python\"",
                "130:         assert event.platform == \"python\"",
                "131: "
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager_grouping.py",
            "line_number": 37,
            "matched_line": "class EventManagerGroupingTest(TestCase):",
            "context_start_line": 27,
            "context_end_line": 47,
            "context": [
                "27: pytestmark = [requires_snuba]",
                "28: ",
                "29: ",
                "30: def get_relevant_metrics_calls(mock_fn: MagicMock, key: str) -> list[mock._Call]:",
                "31:     \"\"\"",
                "32:     Given a mock metrics function, grab only the calls which record the metric with the given key.",
                "33:     \"\"\"",
                "34:     return [call for call in mock_fn.call_args_list if call.args[0] == key]",
                "35: ",
                "36: ",
                "37: class EventManagerGroupingTest(TestCase):",
                "38:     def test_puts_events_with_matching_fingerprints_in_same_group(self):",
                "39:         event = save_new_event(",
                "40:             {\"message\": \"Dogs are great!\", \"fingerprint\": [\"maisey\"]}, self.project",
                "41:         )",
                "42:         # Normally this should go into a different group, since the messages don't match, but the",
                "43:         # fingerprint takes precedence.",
                "44:         event2 = save_new_event(",
                "45:             {\"message\": \"Adopt don't shop\", \"fingerprint\": [\"maisey\"]}, self.project",
                "46:         )",
                "47: "
            ]
        },
        {
            "file": "tests/sentry/event_manager/test_event_manager_grouping.py",
            "line_number": 434,
            "matched_line": "class EventManagerGroupingMetricsTest(TestCase):",
            "context_start_line": 424,
            "context_end_line": 444,
            "context": [
                "424:         (None, None, None),",
                "425:     ],",
                "426: )",
                "427: def test_get_updated_group_title(existing_title, incoming_title, expected_title):",
                "428:     existing_data = {\"title\": existing_title} if existing_title is not None else {}",
                "429:     incoming_data = {\"title\": incoming_title} if incoming_title is not None else {}",
                "430: ",
                "431:     assert _get_updated_group_title(existing_data, incoming_data) == expected_title",
                "432: ",
                "433: ",
                "434: class EventManagerGroupingMetricsTest(TestCase):",
                "435:     @mock.patch(\"sentry.event_manager.metrics.incr\")",
                "436:     def test_records_avg_calculations_per_event_metrics(self, mock_metrics_incr: MagicMock):",
                "437:         project = self.project",
                "438: ",
                "439:         cases: list[Any] = [",
                "440:             [\"Dogs are great!\", LEGACY_GROUPING_CONFIG, None, None, 1],",
                "441:             [\"Adopt don't shop\", DEFAULT_GROUPING_CONFIG, LEGACY_GROUPING_CONFIG, time() + 3600, 2],",
                "442:         ]",
                "443: ",
                "444:         for ("
            ]
        },
        {
            "file": "src/sentry/event_manager.py",
            "line_number": 328,
            "matched_line": "class EventManager:",
            "context_start_line": 318,
            "context_end_line": 338,
            "context": [
                "318:         pass",
                "319:     except Exception:",
                "320:         logger.exception(\"Failed to update GroupTombstone count for id: %s\", tombstone_id)",
                "321: ",
                "322: ",
                "323: ProjectsMapping = Mapping[int, Project]",
                "324: ",
                "325: Job = MutableMapping[str, Any]",
                "326: ",
                "327: ",
                "328: class EventManager:",
                "329:     \"\"\"",
                "330:     Handles normalization in both the store endpoint and the save task. The",
                "331:     intention is to swap this class out with a reimplementation in Rust.",
                "332:     \"\"\"",
                "333: ",
                "334:     def __init__(",
                "335:         self,",
                "336:         data: MutableMapping[str, Any],",
                "337:         version: str = \"5\",",
                "338:         project: Project | None = None,"
            ]
        },
        {
            "file": "src/sentry/analytics/event_manager.py",
            "line_number": 9,
            "matched_line": "class EventManager:",
            "context_start_line": 1,
            "context_end_line": 19,
            "context": [
                "1: __all__ = (\"default_manager\", \"EventManager\")",
                "2: ",
                "3: from collections.abc import MutableMapping",
                "4: from typing import Any",
                "5: ",
                "6: from sentry.analytics.event import Event",
                "7: ",
                "8: ",
                "9: class EventManager:",
                "10:     def __init__(self) -> None:",
                "11:         self._event_types: MutableMapping[Any, type[Event]] = {}",
                "12: ",
                "13:     def register(self, event_cls: type[Event]) -> None:",
                "14:         event_type = event_cls.type",
                "15:         if event_type in self._event_types:",
                "16:             assert self._event_types[event_type] == event_cls",
                "17:         else:",
                "18:             self._event_types[event_type] = event_cls",
                "19: "
            ]
        }
    ],
    "total": 6
}

Message 13 [Assistant]:
--------------------------------------------------------------------------------
Content:
现在让我查看 `EventManager` 类的定义，特别是 `get_data()` 方法：
Tool Calls:
[
  {
    "name": "read_file",
    "args": {
      "file_path": "src/sentry/event_manager.py",
      "encoding": "utf-8"
    },
    "id": "call_00_4lBusGe2MkGtFFMRL6DgPVoj",
    "type": "tool_call"
  }
]

Message 14 [Tool]:
--------------------------------------------------------------------------------
Tool: read_file
Tool Call ID: call_00_4lBusGe2MkGtFFMRL6DgPVoj
Result:
{
    "content": "from __future__ import annotations\n\nimport ipaddress\nimport logging\nimport uuid\nfrom collections.abc import Callable, Mapping, MutableMapping, Sequence\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta, timezone\nfrom typing import TYPE_CHECKING, Any, Literal, TypedDict, overload\n\nimport orjson\nimport sentry_sdk\nfrom django.conf import settings\nfrom django.core.cache import cache\nfrom django.core.exceptions import ValidationError\nfrom django.db import IntegrityError, OperationalError, connection, router, transaction\nfrom django.db.models import Max\nfrom django.db.models.signals import post_save\nfrom django.utils.encoding import force_str\nfrom urllib3.exceptions import MaxRetryError, TimeoutError\nfrom usageaccountant import UsageUnit\n\nfrom sentry import (\n    eventstore,\n    eventstream,\n    eventtypes,\n    features,\n    options,\n    quotas,\n    reprocessing2,\n    tsdb,\n)\nfrom sentry.attachments import CachedAttachment, MissingAttachmentChunks, attachment_cache\nfrom sentry.constants import (\n    DEFAULT_STORE_NORMALIZER_ARGS,\n    INSIGHT_MODULE_FILTERS,\n    LOG_LEVELS_MAP,\n    MAX_TAG_VALUE_LENGTH,\n    PLACEHOLDER_EVENT_TITLES,\n    DataCategory,\n    InsightModules,\n)\nfrom sentry.culprit import generate_culprit\nfrom sentry.dynamic_sampling import record_latest_release\nfrom sentry.eventstore.processing import event_processing_store\nfrom sentry.eventstream.base import GroupState\nfrom sentry.eventtypes import EventType\nfrom sentry.eventtypes.transaction import TransactionEvent\nfrom sentry.exceptions import HashDiscarded\nfrom sentry.grouping.api import (\n    NULL_GROUPHASH_INFO,\n    GroupHashInfo,\n    GroupingConfig,\n    get_grouping_config_dict_for_project,\n)\nfrom sentry.grouping.enhancer import get_enhancements_version\nfrom sentry.grouping.grouptype import ErrorGroupType\nfrom sentry.grouping.ingest.config import is_in_transition, update_or_set_grouping_config_if_needed\nfrom sentry.grouping.ingest.hashing import (\n    find_grouphash_with_group,\n    get_or_create_grouphashes,\n    maybe_run_background_grouping,\n    maybe_run_secondary_grouping,\n    run_primary_grouping,\n)\nfrom sentry.grouping.ingest.metrics import record_hash_calculation_metrics, record_new_group_metrics\nfrom sentry.grouping.ingest.seer import maybe_check_seer_for_matching_grouphash\nfrom sentry.grouping.ingest.utils import (\n    add_group_id_to_grouphashes,\n    check_for_group_creation_load_shed,\n    is_non_error_type_group,\n)\nfrom sentry.grouping.variants import BaseVariant\nfrom sentry.ingest.inbound_filters import FilterStatKeys\nfrom sentry.ingest.transaction_clusterer.datasource.redis import (\n    record_transaction_name as record_transaction_name_for_clustering,\n)\nfrom sentry.integrations.tasks.kick_off_status_syncs import kick_off_status_syncs\nfrom sentry.issues.issue_occurrence import IssueOccurrence\nfrom sentry.issues.producer import PayloadType, produce_occurrence_to_kafka\nfrom sentry.killswitches import killswitch_matches_context\nfrom sentry.lang.native.utils import STORE_CRASH_REPORTS_ALL, convert_crashreport_count\nfrom sentry.models.activity import Activity\nfrom sentry.models.environment import Environment\nfrom sentry.models.event import EventDict\nfrom sentry.models.eventattachment import CRASH_REPORT_TYPES, EventAttachment, get_crashreport_key\nfrom sentry.models.group import Group, GroupStatus\nfrom sentry.models.groupenvironment import GroupEnvironment\nfrom sentry.models.grouphash import GroupHash\nfrom sentry.models.grouphistory import GroupHistoryStatus, record_group_history\nfrom sentry.models.grouplink import GroupLink\nfrom sentry.models.groupopenperiod import (\n    GroupOpenPeriod,\n    create_open_period,\n    has_initial_open_period,\n)\nfrom sentry.models.grouprelease import GroupRelease\nfrom sentry.models.groupresolution import GroupResolution\nfrom sentry.models.organization import Organization\nfrom sentry.models.project import Project\nfrom sentry.models.projectkey import ProjectKey\nfrom sentry.models.pullrequest import PullRequest\nfrom sentry.models.release import Release, follows_semver_versioning_scheme\nfrom sentry.models.releasecommit import ReleaseCommit\nfrom sentry.models.releaseenvironment import ReleaseEnvironment\nfrom sentry.models.releaseprojectenvironment import ReleaseProjectEnvironment\nfrom sentry.models.releases.release_project import ReleaseProject\nfrom sentry.net.http import connection_from_url\nfrom sentry.performance_issues.performance_detection import detect_performance_problems\nfrom sentry.performance_issues.performance_problem import PerformanceProblem\nfrom sentry.plugins.base import plugins\nfrom sentry.quotas.base import index_data_category\nfrom sentry.receivers.features import record_event_processed\nfrom sentry.receivers.onboarding import record_release_received\nfrom sentry.reprocessing2 import is_reprocessed_event\nfrom sentry.seer.signed_seer_api import make_signed_seer_api_request\nfrom sentry.signals import (\n    first_event_received,\n    first_event_with_minified_stack_trace_received,\n    first_insight_span_received,\n    first_transaction_received,\n    issue_unresolved,\n)\nfrom sentry.tasks.process_buffer import buffer_incr\nfrom sentry.tsdb.base import TSDBModel\nfrom sentry.types.activity import ActivityType\nfrom sentry.types.group import GroupSubStatus, PriorityLevel\nfrom sentry.usage_accountant import record\nfrom sentry.utils import metrics\nfrom sentry.utils.cache import cache_key_for_event\nfrom sentry.utils.circuit_breaker import (\n    ERROR_COUNT_CACHE_KEY,\n    CircuitBreakerPassthrough,\n    circuit_breaker_activated,\n)\nfrom sentry.utils.dates import to_datetime\nfrom sentry.utils.event import has_event_minified_stack_trace, has_stacktrace, is_handled\nfrom sentry.utils.eventuser import EventUser\nfrom sentry.utils.metrics import MutableTags\nfrom sentry.utils.outcomes import Outcome, track_outcome\nfrom sentry.utils.projectflags import set_project_flag_and_signal\nfrom sentry.utils.safe import get_path, safe_execute, setdefault_path, trim\nfrom sentry.utils.sdk import set_span_attribute\nfrom sentry.utils.tag_normalization import normalized_sdk_tag_from_event\n\nfrom .utils.event_tracker import TransactionStageStatus, track_sampled_event\n\nif TYPE_CHECKING:\n    from sentry.eventstore.models import BaseEvent, Event\n\nlogger = logging.getLogger(\"sentry.events\")\n\nSECURITY_REPORT_INTERFACES = (\"csp\", \"hpkp\", \"expectct\", \"expectstaple\", \"nel\")\n\n# Timeout for cached group crash report counts\nCRASH_REPORT_TIMEOUT = 24 * 3600  # one day\n\n\nHIGH_SEVERITY_THRESHOLD = 0.1\n\nSEER_ERROR_COUNT_KEY = ERROR_COUNT_CACHE_KEY(\"sentry.seer.severity-failures\")\n\n\n@dataclass\nclass GroupInfo:\n    group: Group\n    is_new: bool\n    is_regression: bool\n    group_release: GroupRelease | None = None\n    is_new_group_environment: bool = False\n\n\ndef pop_tag(data: dict[str, Any], key: str) -> None:\n    if \"tags\" not in data:\n        return\n\n    data[\"tags\"] = [kv for kv in data[\"tags\"] if kv is None or kv[0] != key]\n\n\ndef set_tag(data: dict[str, Any], key: str, value: Any) -> None:\n    pop_tag(data, key)\n    if value is not None:\n        data.setdefault(\"tags\", []).append((key, trim(value, MAX_TAG_VALUE_LENGTH)))\n\n\ndef get_tag(data: dict[str, Any], key: str) -> Any | None:\n    for k, v in get_path(data, \"tags\", filter=True) or ():\n        if k == key:\n            return v\n    return None\n\n\ndef sdk_metadata_from_event(event: Event) -> Mapping[str, Any]:\n    \"\"\"\n    Returns a metadata dictionary with \"sdk\" field populated, including a normalized name\n    Returns {} when event type of event is known to not be SDK generated.\n    \"\"\"\n\n    if event.get_event_type() in SECURITY_REPORT_INTERFACES:\n        return {}\n\n    if not (sdk_metadata := event.data.get(\"sdk\")):\n        return {}\n\n    try:\n        return {\n            \"sdk\": {\n                \"name\": sdk_metadata.get(\"name\") or \"unknown\",\n                \"name_normalized\": normalized_sdk_tag_from_event(event.data),\n            }\n        }\n    except Exception:\n        logger.warning(\"failed to set normalized SDK name\", exc_info=True)\n        return {}\n\n\ndef plugin_is_regression(group: Group, event: BaseEvent) -> bool:\n    project = event.project\n    for plugin in plugins.for_project(project):\n        result = safe_execute(plugin.is_regression, group, event, version=1)\n        if result is not None:\n            return bool(result)\n    return True\n\n\ndef has_pending_commit_resolution(group: Group) -> bool:\n    \"\"\"\n    Checks that the most recent commit that fixes a group has had a chance to release\n    \"\"\"\n    latest_issue_commit_resolution = (\n        GroupLink.objects.filter(\n            group_id=group.id,\n            linked_type=GroupLink.LinkedType.commit,\n            relationship=GroupLink.Relationship.resolves,\n        )\n        .order_by(\"-datetime\")\n        .first()\n    )\n    if latest_issue_commit_resolution is None:\n        return False\n\n    # commit has been released and is not in pending commit state\n    if ReleaseCommit.objects.filter(commit__id=latest_issue_commit_resolution.linked_id).exists():\n        return False\n    else:\n        # check if this commit is a part of a PR\n        pr_ids = PullRequest.objects.filter(\n            pullrequestcommit__commit=latest_issue_commit_resolution.linked_id\n        ).values_list(\"id\", flat=True)\n        # assume that this commit has been released if any commits in this PR have been released\n        if ReleaseCommit.objects.filter(\n            commit__pullrequestcommit__pull_request__in=pr_ids\n        ).exists():\n            return False\n        return True\n\n\n@overload\ndef get_max_crashreports(model: Project | Organization) -> int: ...\n\n\n@overload\ndef get_max_crashreports(\n    model: Project | Organization, *, allow_none: Literal[True]\n) -> int | None: ...\n\n\ndef get_max_crashreports(model: Project | Organization, *, allow_none: bool = False) -> int | None:\n    value = model.get_option(\"sentry:store_crash_reports\")\n    return convert_crashreport_count(value, allow_none=allow_none)\n\n\ndef crashreports_exceeded(current_count: int, max_count: int) -> bool:\n    if max_count == STORE_CRASH_REPORTS_ALL:\n        return False\n    return current_count >= max_count\n\n\ndef get_stored_crashreports(cache_key: str | None, event: Event, max_crashreports: int) -> int:\n    # There are two common cases: Storing crash reports is disabled, or is\n    # unbounded. In both cases, there is no need in caching values or querying\n    # the database.\n    if max_crashreports in (0, STORE_CRASH_REPORTS_ALL):\n        return max_crashreports\n\n    cached_reports = cache.get(cache_key, None)\n    if cached_reports is not None and cached_reports >= max_crashreports:\n        return cached_reports\n\n    # Fall-through if max_crashreports was bumped to get a more accurate number.\n    # We don't need the actual number, but just whether it's more or equal to\n    # the currently allowed maximum.\n    query = EventAttachment.objects.filter(group_id=event.group_id, type__in=CRASH_REPORT_TYPES)\n    return query[:max_crashreports].count()\n\n\ndef increment_group_tombstone_hit_counter(tombstone_id: int | None, event: Event) -> None:\n    if tombstone_id is None:\n        return\n    try:\n        from sentry.models.grouptombstone import GroupTombstone\n\n        group_tombstone = GroupTombstone.objects.get(id=tombstone_id)\n        buffer_incr(\n            GroupTombstone,\n            {\"times_seen\": 1},\n            {\"id\": tombstone_id},\n            {\n                \"last_seen\": (\n                    max(event.datetime, group_tombstone.last_seen)\n                    if group_tombstone.last_seen\n                    else event.datetime\n                )\n            },\n        )\n    except GroupTombstone.DoesNotExist:\n        # This can happen due to a race condition with deletion.\n        pass\n    except Exception:\n        logger.exception(\"Failed to update GroupTombstone count for id: %s\", tombstone_id)\n\n\nProjectsMapping = Mapping[int, Project]\n\nJob = MutableMapping[str, Any]\n\n\nclass EventManager:\n    \"\"\"\n    Handles normalization in both the store endpoint and the save task. The\n    intention is to swap this class out with a reimplementation in Rust.\n    \"\"\"\n\n    def __init__(\n        self,\n        data: MutableMapping[str, Any],\n        version: str = \"5\",\n        project: Project | None = None,\n        grouping_config: GroupingConfig | None = None,\n        client_ip: str | None = None,\n        user_agent: str | None = None,\n        auth: Any | None = None,\n        key: Any | None = None,\n        content_encoding: str | None = None,\n        is_renormalize: bool = False,\n        remove_other: bool | None = None,\n        project_config: Any | None = None,\n        sent_at: datetime | None = None,\n    ):\n        self._data: MutableMapping[str, Any] = data\n        self.version = version\n        self._project = project\n        # if not explicitly specified try to get the grouping from project_config\n        if grouping_config is None and project_config is not None:\n            config = project_config.config\n            grouping_config = config.get(\"grouping_config\")\n        # if we still don't have a grouping also try the project\n        if grouping_config is None and project is not None:\n            grouping_config = get_grouping_config_dict_for_project(project)\n        self._grouping_config = grouping_config\n        self._client_ip = client_ip\n        self._user_agent = user_agent\n        self._auth = auth\n        self._key = key\n        self._is_renormalize = is_renormalize\n        self._remove_other = remove_other\n        self._normalized = False\n        self.project_config = project_config\n        self.sent_at = sent_at\n\n    def normalize(self, project_id: int | None = None) -> None:\n        with metrics.timer(\"events.store.normalize.duration\"):\n            self._normalize_impl(project_id=project_id)\n\n    def _normalize_impl(self, project_id: int | None = None) -> None:\n        if self._project and project_id and project_id != self._project.id:\n            raise RuntimeError(\n                \"Initialized EventManager with one project ID and called save() with another one\"\n            )\n\n        if self._normalized:\n            raise RuntimeError(\"Already normalized\")\n\n        self._normalized = True\n\n        from sentry_relay.processing import StoreNormalizer\n\n        rust_normalizer = StoreNormalizer(\n            project_id=self._project.id if self._project else project_id,\n            client_ip=self._client_ip,\n            client=self._auth.client if self._auth else None,\n            key_id=str(self._key.id) if self._key else None,\n            grouping_config=self._grouping_config,\n            protocol_version=str(self.version) if self.version is not None else None,\n            is_renormalize=self._is_renormalize,\n            remove_other=self._remove_other,\n            normalize_user_agent=True,\n            sent_at=self.sent_at.isoformat() if self.sent_at is not None else None,\n            json_dumps=orjson.dumps,\n            **DEFAULT_STORE_NORMALIZER_ARGS,\n        )\n\n        pre_normalize_type = self._data.get(\"type\")\n        self._data = rust_normalizer.normalize_event(dict(self._data), json_loads=orjson.loads)\n\n        # XXX: This is a hack to make generic events work (for now?). I'm not sure whether we should\n        # include this in the rust normalizer, since we don't want people sending us these via the\n        # sdk.\n        if pre_normalize_type in (\"generic\", \"feedback\"):\n            self._data[\"type\"] = pre_normalize_type\n\n    def get_data(self) -> MutableMapping[str, Any]:\n        return self._data\n\n    @sentry_sdk.tracing.trace\n    def save(\n        self,\n        project_id: int | None,\n        raw: bool = False,\n        assume_normalized: bool = False,\n        start_time: float | None = None,\n        cache_key: str | None = None,\n        skip_send_first_transaction: bool = False,\n        has_attachments: bool = False,\n    ) -> Event:\n        \"\"\"\n        After normalizing and processing an event, save adjacent models such as\n        releases and environments to postgres and write the event into\n        eventstream. From there it will be picked up by Snuba and\n        post-processing.\n\n        We re-insert events with duplicate IDs into Snuba, which is responsible\n        for deduplicating events. Since deduplication in Snuba is on the primary\n        key (based on event ID, project ID and day), events with same IDs are only\n        deduplicated if their timestamps fall on the same day. The latest event\n        always wins and overwrites the value of events received earlier in that day.\n\n        Since we increment counters and frequencies here before events get inserted\n        to eventstream these numbers may be larger than the total number of\n        events if we receive duplicate event IDs that fall on the same day\n        (that do not hit cache first).\n        \"\"\"\n\n        # Normalize if needed\n        if not self._normalized:\n            if not assume_normalized:\n                self.normalize(project_id=project_id)\n            self._normalized = True\n\n        project = Project.objects.get_from_cache(id=project_id)\n        project.set_cached_field_value(\n            \"organization\", Organization.objects.get_from_cache(id=project.organization_id)\n        )\n\n        projects = {project.id: project}\n\n        job: dict[str, Any] = {\n            \"data\": self._data,\n            \"project_id\": project.id,\n            \"raw\": raw,\n            \"start_time\": start_time,\n        }\n\n        # After calling _pull_out_data we get some keys in the job like the platform\n        _pull_out_data([job], projects)\n\n        event_type = self._data.get(\"type\")\n        if event_type == \"transaction\":\n            job[\"data\"][\"project\"] = project.id\n            jobs = save_transaction_events([job], projects, skip_send_first_transaction)\n            return jobs[0][\"event\"]\n        elif event_type == \"generic\":\n            job[\"data\"][\"project\"] = project.id\n            jobs = save_generic_events([job], projects)\n            return jobs[0][\"event\"]\n        else:\n            project = job[\"event\"].project\n            job[\"in_grouping_transition\"] = is_in_transition(project)\n            metric_tags = {\n                \"platform\": job[\"event\"].platform or \"unknown\",\n                \"sdk\": normalized_sdk_tag_from_event(job[\"event\"].data),\n                \"in_transition\": job[\"in_grouping_transition\"],\n                \"split_enhancements\": get_enhancements_version(project) == 3,\n            }\n            # This metric allows differentiating from all calls to the `event_manager.save` metric\n            # and adds support for differentiating based on platforms\n            with metrics.timer(\"event_manager.save_error_events\", tags=metric_tags):\n                return self.save_error_events(\n                    project,\n                    job,\n                    projects,\n                    metric_tags,\n                    raw,\n                    cache_key,\n                    has_attachments=has_attachments,\n                )\n\n    @sentry_sdk.tracing.trace\n    def save_error_events(\n        self,\n        project: Project,\n        job: Job,\n        projects: ProjectsMapping,\n        metric_tags: MutableTags,\n        raw: bool = False,\n        cache_key: str | None = None,\n        has_attachments: bool = False,\n    ) -> Event:\n        jobs = [job]\n\n        is_reprocessed = is_reprocessed_event(job[\"data\"])\n\n        _get_or_create_release_many(jobs, projects)\n        _get_event_user_many(jobs, projects)\n\n        job[\"project_key\"] = None\n        if job[\"key_id\"] is not None:\n            try:\n                job[\"project_key\"] = ProjectKey.objects.get_from_cache(id=job[\"key_id\"])\n            except ProjectKey.DoesNotExist:\n                pass\n\n        _derive_plugin_tags_many(jobs, projects)\n        _derive_interface_tags_many(jobs)\n        _derive_client_error_sampling_rate(jobs, projects)\n\n        # Load attachments first, but persist them at the very last after\n        # posting to eventstream to make sure all counters and eventstream are\n        # incremented for sure. Also wait for grouping to remove attachments\n        # based on the group counter.\n        if has_attachments:\n            attachments = get_attachments(cache_key, job)\n        else:\n            attachments = []\n\n        try:\n            group_info = assign_event_to_group(event=job[\"event\"], job=job, metric_tags=metric_tags)\n\n        except HashDiscarded as e:\n            if features.has(\"organizations:grouptombstones-hit-counter\", project.organization):\n                increment_group_tombstone_hit_counter(\n                    getattr(e, \"tombstone_id\", None), job[\"event\"]\n                )\n            discard_event(job, attachments)\n            raise\n\n        if not group_info:\n            return job[\"event\"]\n\n        # store a reference to the group id to guarantee validation of isolation\n        # XXX(markus): No clue what this does\n        job[\"event\"].data.bind_ref(job[\"event\"])\n\n        _get_or_create_environment_many(jobs, projects)\n        _get_or_create_group_environment_many(jobs)\n        _get_or_create_release_associated_models(jobs, projects)\n        _increment_release_associated_counts_many(jobs, projects)\n        _get_or_create_group_release_many(jobs)\n        _tsdb_record_all_metrics(jobs)\n\n        if attachments:\n            attachments = filter_attachments_for_group(attachments, job)\n\n        # XXX: DO NOT MUTATE THE EVENT PAYLOAD AFTER THIS POINT\n        _materialize_event_metrics(jobs)\n\n        for attachment in attachments:\n            key = f\"bytes.stored.{attachment.type}\"\n            old_bytes = job[\"event_metrics\"].get(key) or 0\n            job[\"event_metrics\"][key] = old_bytes + attachment.size\n\n        _nodestore_save_many(jobs=jobs, app_feature=\"errors\")\n\n        if not raw:\n            if not project.first_event:\n                project.update(first_event=job[\"event\"].datetime)\n                first_event_received.send_robust(\n                    project=project, event=job[\"event\"], sender=Project\n                )\n\n            if has_event_minified_stack_trace(job[\"event\"]):\n                set_project_flag_and_signal(\n                    project,\n                    \"has_minified_stack_trace\",\n                    first_event_with_minified_stack_trace_received,\n                    event=job[\"event\"],\n                )\n\n        if is_reprocessed:\n            safe_execute(\n                reprocessing2.buffered_delete_old_primary_hash,\n                project_id=job[\"event\"].project_id,\n                group_id=reprocessing2.get_original_group_id(job[\"event\"]),\n                event_id=job[\"event\"].event_id,\n                datetime=job[\"event\"].datetime,\n                old_primary_hash=reprocessing2.get_original_primary_hash(job[\"event\"]),\n                current_primary_hash=job[\"event\"].get_primary_hash(),\n            )\n\n        _eventstream_insert_many(jobs)\n\n        # Do this last to ensure signals get emitted even if connection to the\n        # file store breaks temporarily.\n        #\n        # We do not need this for reprocessed events as for those we update the\n        # group_id on existing models in post_process_group, which already does\n        # this because of indiv. attachments.\n        if not is_reprocessed and attachments:\n            save_attachments(cache_key, attachments, job)\n\n        metric_tags = {\"from_relay\": str(\"_relay_processed\" in job[\"data\"])}\n\n        metrics.timing(\n            \"events.latency\",\n            job[\"received_timestamp\"] - job[\"recorded_timestamp\"],\n            tags=metric_tags,\n        )\n        metrics.distribution(\n            \"events.size.data.post_save\", job[\"event\"].size, tags=metric_tags, unit=\"byte\"\n        )\n        metrics.incr(\n            \"events.post_save.normalize.errors\",\n            amount=len(job[\"data\"].get(\"errors\") or ()),\n            tags=metric_tags,\n        )\n\n        _track_outcome_accepted_many(jobs)\n\n        self._data = job[\"event\"].data.data\n\n        return job[\"event\"]\n\n\n@sentry_sdk.tracing.trace\ndef _pull_out_data(jobs: Sequence[Job], projects: ProjectsMapping) -> None:\n    \"\"\"\n    Update every job in the list with required information and store it in the nodestore.\n\n    A bunch of (probably) CPU bound stuff.\n    \"\"\"\n\n    for job in jobs:\n        job[\"project_id\"] = int(job[\"project_id\"])\n\n        data = job[\"data\"]\n\n        # Pull the toplevel data we're interested in\n\n        transaction_name = data.get(\"transaction\")\n        if transaction_name:\n            transaction_name = force_str(transaction_name)\n        job[\"transaction\"] = transaction_name\n\n        key_id = None if data is None else data.get(\"key_id\")\n        if key_id is not None:\n            key_id = int(key_id)\n        job[\"key_id\"] = key_id\n\n        job[\"logger_name\"] = logger_name = data.get(\"logger\")\n        job[\"level\"] = level = data.get(\"level\")\n        job[\"release\"] = data.get(\"release\")\n        job[\"dist\"] = data.get(\"dist\")\n        job[\"environment\"] = environment = data.get(\"environment\")\n        job[\"recorded_timestamp\"] = data.get(\"timestamp\")\n        # Stores the event in the nodestore\n        job[\"event\"] = event = _get_event_instance(job[\"data\"], project_id=job[\"project_id\"])\n        # Overwrite the data key with the event's updated data\n        job[\"data\"] = data = event.data.data\n\n        event._project_cache = project = projects[job[\"project_id\"]]\n        job[\"category\"] = index_data_category(data.get(\"type\"), project.organization)\n        job[\"platform\"] = event.platform\n\n        # Some of the data that are toplevel attributes are duplicated\n        # into tags (logger, level, environment, transaction).  These are\n        # different from legacy attributes which are normalized into tags\n        # ahead of time (site, server_name).\n        setdefault_path(data, \"tags\", value=[])\n        set_tag(data, \"level\", level)\n        if logger_name:\n            set_tag(data, \"logger\", logger_name)\n        if environment:\n            set_tag(data, \"environment\", environment)\n        if transaction_name:\n            set_tag(data, \"transaction\", transaction_name)\n\n        job[\"received_timestamp\"] = job[\"event\"].data.get(\"received\") or float(\n            job[\"event\"].datetime.strftime(\"%s\")\n        )\n        job[\"groups\"] = []\n\n\n@sentry_sdk.tracing.trace\ndef _get_or_create_release_many(jobs: Sequence[Job], projects: ProjectsMapping) -> None:\n    for job in jobs:\n        data = job[\"data\"]\n        if not data.get(\"release\"):\n            return\n\n        project = projects[job[\"project_id\"]]\n        date = job[\"event\"].datetime\n\n        try:\n            release = Release.get_or_create(\n                project=project,\n                version=data[\"release\"],\n                date_added=date,\n            )\n        except ValidationError:\n            logger.exception(\n                \"Failed creating Release due to ValidationError\",\n                extra={\"project\": project, \"version\": data[\"release\"]},\n            )\n            release = None\n\n        job[\"release\"] = release\n        if not release:\n            return\n\n        # Don't allow a conflicting 'release' tag\n        pop_tag(data, \"release\")\n        set_tag(data, \"sentry:release\", release.version)\n\n        if data.get(\"dist\"):\n            job[\"dist\"] = release.add_dist(data[\"dist\"], date)\n\n            # don't allow a conflicting 'dist' tag\n            pop_tag(job[\"data\"], \"dist\")\n            set_tag(job[\"data\"], \"sentry:dist\", job[\"dist\"].name)\n\n\n@sentry_sdk.tracing.trace\ndef _get_event_user_many(jobs: Sequence[Job], projects: ProjectsMapping) -> None:\n    for job in jobs:\n        data = job[\"data\"]\n        user = _get_event_user(projects[job[\"project_id\"]], data)\n\n        if user:\n            pop_tag(data, \"user\")\n            set_tag(data, \"sentry:user\", user.tag_value)\n\n        job[\"user\"] = user\n\n\n@sentry_sdk.tracing.trace\ndef _derive_plugin_tags_many(jobs: Sequence[Job], projects: ProjectsMapping) -> None:\n    # XXX: We ought to inline or remove this one for sure\n    plugins_for_projects = {p.id: plugins.for_project(p, version=None) for p in projects.values()}\n\n    for job in jobs:\n        for plugin in plugins_for_projects[job[\"project_id\"]]:\n            added_tags = safe_execute(plugin.get_tags, job[\"event\"])\n            if added_tags:\n                data = job[\"data\"]\n                # plugins should not override user provided tags\n                for key, value in added_tags:\n                    if get_tag(data, key) is None:\n                        set_tag(data, key, value)\n\n\ndef _derive_interface_tags_many(jobs: Sequence[Job]) -> None:\n    # XXX: We ought to inline or remove this one for sure\n    for job in jobs:\n        data = job[\"data\"]\n        for path, iface in job[\"event\"].interfaces.items():\n            for k, v in iface.iter_tags():\n                set_tag(data, k, v)\n\n            # Get rid of ephemeral interface data\n            if iface.ephemeral:\n                data.pop(iface.path, None)\n\n\ndef _derive_client_error_sampling_rate(jobs: Sequence[Job], projects: ProjectsMapping) -> None:\n    for job in jobs:\n        if job[\"project_id\"] in options.get(\"issues.client_error_sampling.project_allowlist\"):\n            try:\n                client_sample_rate = (\n                    job[\"data\"]\n                    .get(\"contexts\", {})\n                    .get(\"error_sampling\", {})\n                    .get(\"client_sample_rate\")\n                )\n\n                if client_sample_rate is not None and isinstance(client_sample_rate, (int, float)):\n                    if 0 < client_sample_rate <= 1:\n                        job[\"data\"][\"sample_rate\"] = client_sample_rate\n                    else:\n                        logger.warning(\n                            \"Client sent invalid error sample_rate outside valid range (0-1)\",\n                            extra={\n                                \"project_id\": job[\"project_id\"],\n                                \"client_sample_rate\": client_sample_rate,\n                            },\n                        )\n                        metrics.incr(\"issues.client_error_sampling.invalid_range\")\n            except (KeyError, TypeError, AttributeError):\n                pass\n\n\ndef _materialize_metadata_many(jobs: Sequence[Job]) -> None:\n    for job in jobs:\n        # we want to freeze not just the metadata and type in but also the\n        # derived attributes.  The reason for this is that we push this\n        # data into kafka for snuba processing and our postprocessing\n        # picks up the data right from the snuba topic.  For most usage\n        # however the data is dynamically overridden by Event.title and\n        # Event.location (See Event.as_dict)\n        #\n        # We also need to ensure the culprit is accurately reflected at\n        # the point of metadata materialization as we need to ensure that\n        # processing happens before.\n        data = job[\"data\"]\n        event_type = get_event_type(data)\n        event_metadata = event_type.get_metadata(data)\n        job[\"event_metadata\"] = dict(event_metadata)\n\n        data.update(materialize_metadata(data, event_type, event_metadata))\n        job[\"culprit\"] = data[\"culprit\"]\n\n\ndef _get_group_processing_kwargs(job: Job) -> dict[str, Any]:\n    \"\"\"\n    Pull together all the metadata used when creating a group or updating a group's metadata based\n    on a new event.\n\n    Note: Must be called *after* grouping has run, because the grouping process can affect the title\n    (by setting `main_exception_id` or by setting the title directly using a custom fingerprint\n    rule).\n    \"\"\"\n    _materialize_metadata_many([job])\n\n    event_data = job[\"event\"].data\n    event_metadata = job[\"event_metadata\"]\n\n    group_metadata = materialize_metadata(\n        event_data,\n        # In principle the group gets the same metadata as the event, so common\n        # attributes can be defined in eventtypes.\n        get_event_type(event_data),\n        event_metadata,\n    )\n    group_metadata[\"last_received\"] = job[\"received_timestamp\"]\n\n    kwargs = {\n        \"data\": group_metadata,\n        \"platform\": job[\"platform\"],\n        \"message\": job[\"event\"].search_message,\n        \"logger\": job[\"logger_name\"],\n        \"level\": LOG_LEVELS_MAP.get(job[\"level\"]),\n        \"last_seen\": job[\"event\"].datetime,\n        \"first_seen\": job[\"event\"].datetime,\n        \"active_at\": job[\"event\"].datetime,\n        \"culprit\": job[\"culprit\"],\n    }\n\n    if job[\"release\"]:\n        kwargs[\"first_release\"] = job[\"release\"]\n\n    return kwargs\n\n\n@sentry_sdk.tracing.trace\ndef _get_or_create_environment_many(jobs: Sequence[Job], projects: ProjectsMapping) -> None:\n    for job in jobs:\n        job[\"environment\"] = Environment.get_or_create(\n            project=projects[job[\"project_id\"]], name=job[\"environment\"]\n        )\n\n\n@sentry_sdk.tracing.trace\ndef _get_or_create_group_environment_many(jobs: Sequence[Job]) -> None:\n    for job in jobs:\n        _get_or_create_group_environment(job[\"environment\"], job[\"release\"], job[\"groups\"])\n\n\ndef _get_or_create_group_environment(\n    environment: Environment, release: Release | None, groups: Sequence[GroupInfo]\n) -> None:\n    for group_info in groups:\n        group_info.is_new_group_environment = GroupEnvironment.get_or_create(\n            group_id=group_info.group.id,\n            environment_id=environment.id,\n            defaults={\"first_release\": release or None},\n        )[1]\n\n\ndef _get_or_create_release_associated_models(\n    jobs: Sequence[Job], projects: ProjectsMapping\n) -> None:\n    # XXX: This is possibly unnecessarily detached from\n    # _get_or_create_release_many, but we do not want to destroy order of\n    # execution right now\n    for job in jobs:\n        release = job[\"release\"]\n        if not release:\n            continue\n\n        project = projects[job[\"project_id\"]]\n        environment = job[\"environment\"]\n        date = job[\"event\"].datetime\n\n        ReleaseEnvironment.get_or_create(\n            project=project, release=release, environment=environment, datetime=date\n        )\n\n        ReleaseProjectEnvironment.get_or_create(\n            project=project, release=release, environment=environment, datetime=date\n        )\n\n\ndef _increment_release_associated_counts_many(\n    jobs: Sequence[Job], projects: ProjectsMapping\n) -> None:\n    for job in jobs:\n        _increment_release_associated_counts(\n            projects[job[\"project_id\"]], job[\"environment\"], job[\"release\"], job[\"groups\"]\n        )\n\n\ndef _increment_release_associated_counts(\n    project: Project,\n    environment: Environment,\n    release: Release | None,\n    groups: Sequence[GroupInfo],\n) -> None:\n    if not release:\n        return\n\n    rp_new_groups = 0\n    rpe_new_groups = 0\n    for group_info in groups:\n        if group_info.is_new:\n            rp_new_groups += 1\n        if group_info.is_new_group_environment:\n            rpe_new_groups += 1\n    if rp_new_groups:\n        buffer_incr(\n            ReleaseProject,\n            {\"new_groups\": rp_new_groups},\n            {\"release_id\": release.id, \"project_id\": project.id},\n        )\n    if rpe_new_groups:\n        buffer_incr(\n            ReleaseProjectEnvironment,\n            {\"new_issues_count\": rpe_new_groups},\n            {\n                \"project_id\": project.id,\n                \"release_id\": release.id,\n                \"environment_id\": environment.id,\n            },\n        )\n\n\ndef _get_or_create_group_release_many(jobs: Sequence[Job]) -> None:\n    for job in jobs:\n        _get_or_create_group_release(\n            job[\"environment\"], job[\"release\"], job[\"event\"], job[\"groups\"]\n        )\n\n\ndef _get_or_create_group_release(\n    environment: Environment,\n    release: Release | None,\n    event: BaseEvent,\n    groups: Sequence[GroupInfo],\n) -> None:\n    if release:\n        for group_info in groups:\n            group_info.group_release = GroupRelease.get_or_create(\n                group=group_info.group,\n                release=release,\n                environment=environment,\n                datetime=event.datetime,\n            )\n\n\ndef _tsdb_record_all_metrics(jobs: Sequence[Job]) -> None:\n    \"\"\"\n    Do all tsdb-related things for save_event in here s.t. we can potentially\n    put everything in a single redis pipeline someday.\n    \"\"\"\n\n    # XXX: validate whether anybody actually uses those metrics\n\n    for job in jobs:\n        incrs = []\n        frequencies = []\n        records = []\n        incrs.append((TSDBModel.project, job[\"project_id\"]))\n        event = job[\"event\"]\n        release = job[\"release\"]\n        environment = job[\"environment\"]\n        user = job[\"user\"]\n\n        for group_info in job[\"groups\"]:\n            incrs.append((TSDBModel.group, group_info.group.id))\n            frequencies.append(\n                (\n                    TSDBModel.frequent_environments_by_group,\n                    {group_info.group.id: {environment.id: 1}},\n                )\n            )\n\n            if group_info.group_release:\n                frequencies.append(\n                    (\n                        TSDBModel.frequent_releases_by_group,\n                        {group_info.group.id: {group_info.group_release.id: 1}},\n                    )\n                )\n            if user:\n                records.append(\n                    (TSDBModel.users_affected_by_group, group_info.group.id, (user.tag_value,))\n                )\n\n        if release:\n            incrs.append((TSDBModel.release, release.id))\n\n        if user:\n            project_id = job[\"project_id\"]\n            records.append((TSDBModel.users_affected_by_project, project_id, (user.tag_value,)))\n\n        if incrs:\n            tsdb.backend.incr_multi(incrs, timestamp=event.datetime, environment_id=environment.id)\n\n        if records:\n            tsdb.backend.record_multi(\n                records, timestamp=event.datetime, environment_id=environment.id\n            )\n\n        if frequencies:\n            tsdb.backend.record_frequency_multi(frequencies, timestamp=event.datetime)\n\n\ndef _nodestore_save_many(jobs: Sequence[Job], app_feature: str) -> None:\n    inserted_time = datetime.now(timezone.utc).timestamp()\n    for job in jobs:\n        # Write the event to Nodestore\n        subkeys = {}\n\n        event = job[\"event\"]\n        # We only care about `unprocessed` for error events\n        if event.get_event_type() not in (\"transaction\", \"generic\") and job[\"groups\"]:\n            unprocessed = event_processing_store.get(\n                cache_key_for_event({\"project\": event.project_id, \"event_id\": event.event_id}),\n                unprocessed=True,\n            )\n            if unprocessed is not None:\n                subkeys[\"unprocessed\"] = unprocessed\n\n        if app_feature:\n            event_size = 0\n            event_metrics = job.get(\"event_metrics\")\n            if event_metrics:\n                event_size = event_metrics.get(\"bytes.stored.event\", 0)\n            record(\n                resource_id=settings.COGS_EVENT_STORE_LABEL,\n                app_feature=app_feature,\n                amount=event_size,\n                usage_type=UsageUnit.BYTES,\n            )\n        job[\"event\"].data[\"nodestore_insert\"] = inserted_time\n        job[\"event\"].data.save(subkeys=subkeys)\n\n\ndef _eventstream_insert_many(jobs: Sequence[Job]) -> None:\n    for job in jobs:\n\n        if job[\"event\"].project_id == settings.SENTRY_PROJECT:\n            metrics.incr(\n                \"internal.captured.eventstream_insert\",\n                tags={\"event_type\": job[\"event\"].data.get(\"type\") or \"null\"},\n            )\n\n        # XXX: Temporary hack so that we keep this group info working for error issues. We'll need\n        # to change the format of eventstream to be able to handle data for multiple groups\n        if not job[\"groups\"]:\n            group_states: list[GroupState] | None = None\n            is_new = False\n            is_regression = False\n            is_new_group_environment = False\n        else:\n            # error issues\n            group_info = job[\"groups\"][0]\n            is_new = group_info.is_new\n            is_regression = group_info.is_regression\n            is_new_group_environment = group_info.is_new_group_environment\n\n            # performance issues with potentially multiple groups to a transaction\n            group_states = [\n                {\n                    \"id\": gi.group.id,\n                    \"is_new\": gi.is_new,\n                    \"is_regression\": gi.is_regression,\n                    \"is_new_group_environment\": gi.is_new_group_environment,\n                }\n                for gi in job[\"groups\"]\n                if gi is not None\n            ]\n\n        # Skip running grouping for \"transaction\" events:\n        primary_hash = (\n            None if job[\"data\"].get(\"type\") == \"transaction\" else job[\"event\"].get_primary_hash()\n        )\n\n        eventstream.backend.insert(\n            event=job[\"event\"],\n            is_new=is_new,\n            is_regression=is_regression,\n            is_new_group_environment=is_new_group_environment,\n            primary_hash=primary_hash,\n            received_timestamp=job[\"received_timestamp\"],\n            # We are choosing to skip consuming the event back\n            # in the eventstream if it's flagged as raw.\n            # This means that we want to publish the event\n            # through the event stream, but we don't care\n            # about post processing and handling the commit.\n            skip_consume=job.get(\"raw\", False),\n            group_states=group_states,\n        )\n\n\ndef _track_outcome_accepted_many(jobs: Sequence[Job]) -> None:\n    for job in jobs:\n        event = job[\"event\"]\n\n        track_outcome(\n            org_id=event.project.organization_id,\n            project_id=job[\"project_id\"],\n            key_id=job[\"key_id\"],\n            outcome=Outcome.ACCEPTED,\n            reason=None,\n            timestamp=to_datetime(job[\"start_time\"]),\n            event_id=event.event_id,\n            category=job[\"category\"],\n        )\n\n\ndef _get_event_instance(data: MutableMapping[str, Any], project_id: int) -> Event:\n    return eventstore.backend.create_event(\n        project_id=project_id,\n        event_id=data.get(\"event_id\"),\n        group_id=None,\n        data=EventDict(data, skip_renormalization=True),\n    )\n\n\ndef _get_event_user(project: Project, data: Mapping[str, Any]) -> EventUser | None:\n    with metrics.timer(\"event_manager.get_event_user\") as metrics_tags:\n        return _get_event_user_impl(project, data, metrics_tags)\n\n\ndef _get_event_user_impl(\n    project: Project, data: Mapping[str, Any], metrics_tags: MutableTags\n) -> EventUser | None:\n    user_data = data.get(\"user\")\n    if not user_data:\n        metrics_tags[\"event_has_user\"] = \"false\"\n        return None\n\n    metrics_tags[\"event_has_user\"] = \"true\"\n\n    ip_address = user_data.get(\"ip_address\")\n\n    if ip_address:\n        try:\n            ipaddress.ip_address(str(ip_address))\n        except ValueError:\n            ip_address = None\n\n    euser = EventUser(\n        project_id=project.id,\n        user_ident=user_data.get(\"id\"),\n        email=user_data.get(\"email\"),\n        username=user_data.get(\"username\"),\n        ip_address=ip_address,\n        name=user_data.get(\"name\"),\n    )\n\n    return euser\n\n\ndef get_event_type(data: Mapping[str, Any]) -> EventType:\n    return eventtypes.get(data.get(\"type\", \"default\"))()\n\n\nEventMetadata = dict[str, Any]\n\n\ndef materialize_metadata(\n    data: Mapping[str, Any], event_type: EventType, event_metadata: dict[str, Any]\n) -> EventMetadata:\n    \"\"\"Returns the materialized metadata to be merged with group or\n    event data.  This currently produces the keys `type`, `culprit`,\n    `metadata`, `title` and `location`.\n    \"\"\"\n\n    # XXX(markus): Ideally this wouldn't take data or event_type, and instead\n    # calculate culprit + type from event_metadata\n\n    # Don't clobber existing metadata\n    try:\n        event_metadata.update(data.get(\"metadata\", {}))\n    except TypeError:\n        # On a small handful of occasions, the line above has errored with `TypeError: 'NoneType'\n        # object is not iterable`, even though it's clear from looking at the local variable values\n        # in the event in Sentry that this shouldn't be possible.\n        logger.exception(\n            \"Non-None being read as None\",\n            extra={\n                \"data is None\": data is None,\n                \"event_metadata is None\": event_metadata is None,\n                \"data.get\": data.get,\n                \"event_metadata.update\": event_metadata.update,\n                \"data.get('metadata', {})\": data.get(\"metadata\", {}),\n            },\n        )\n\n    return {\n        \"type\": event_type.key,\n        \"culprit\": get_culprit(data),\n        \"metadata\": event_metadata,\n        \"title\": event_type.get_title(event_metadata),\n        \"location\": event_type.get_location(event_metadata),\n    }\n\n\ndef get_culprit(data: Mapping[str, Any]) -> str:\n    \"\"\"Helper to calculate the default culprit\"\"\"\n    return str(\n        force_str(data.get(\"culprit\") or data.get(\"transaction\") or generate_culprit(data) or \"\")\n    )\n\n\n@sentry_sdk.tracing.trace\ndef assign_event_to_group(\n    event: Event,\n    job: Job,\n    metric_tags: MutableTags,\n) -> GroupInfo | None:\n    project = event.project\n    secondary = NULL_GROUPHASH_INFO\n\n    # Try looking for an existing group using the current grouping config\n    primary = get_hashes_and_grouphashes(job, run_primary_grouping, metric_tags)\n\n    # If we've found one, great. No need to do any more calculations\n    if primary.existing_grouphash:\n        group_info = handle_existing_grouphash(job, primary.existing_grouphash, primary.grouphashes)\n        result = \"found_primary\"\n    # If we haven't, try again using the secondary config. (If there is no secondary config, or\n    # we're out of the transition period, we'll get back the empty `NULL_GROUPHASH_INFO`.)\n    else:\n        secondary = get_hashes_and_grouphashes(job, maybe_run_secondary_grouping, metric_tags)\n        all_grouphashes = primary.grouphashes + secondary.grouphashes\n\n        if secondary.existing_grouphash:\n            group_info = handle_existing_grouphash(\n                job, secondary.existing_grouphash, all_grouphashes\n            )\n            result = \"found_secondary\"\n        # If we still haven't found a group, ask Seer for a match (if enabled for the project)\n        else:\n            seer_matched_grouphash = maybe_check_seer_for_matching_grouphash(\n                event, primary.grouphashes[0], primary.variants, all_grouphashes\n            )\n\n            if seer_matched_grouphash:\n                group_info = handle_existing_grouphash(job, seer_matched_grouphash, all_grouphashes)\n            # If we *still* haven't found a group into which to put the event, create a new group\n            else:\n                group_info = create_group_with_grouphashes(job, all_grouphashes)\n            result = \"no_match\"\n\n    # From here on out, we're just doing housekeeping\n\n    # Background grouping is a way for us to get performance metrics for a new\n    # config without having it actually affect on how events are grouped. It runs\n    # either before or after the main grouping logic, depending on the option value.\n    maybe_run_background_grouping(project, job)\n\n    record_hash_calculation_metrics(\n        project, primary.config, primary.hashes, secondary.config, secondary.hashes, result\n    )\n\n    # Now that we've used the current and possibly secondary grouping config(s) to calculate the\n    # hashes, we're free to perform a config update if needed. Future events will use the new\n    # config, but will also be grandfathered into the current config for a week, so as not to\n    # erroneously create new groups.\n    update_or_set_grouping_config_if_needed(project, \"ingest\")\n\n    # The only way there won't be group info is we matched to a performance, cron, replay, or\n    # other-non-error-type group because of a hash collision - exceedingly unlikely, and not\n    # something we've ever observed, but theoretically possible.\n    if group_info:\n        event.group = group_info.group\n    job[\"groups\"] = [group_info]\n\n    return group_info\n\n\n@sentry_sdk.tracing.trace\ndef get_hashes_and_grouphashes(\n    job: Job,\n    hash_calculation_function: Callable[\n        [Project, Job, MutableTags],\n        tuple[GroupingConfig, list[str], dict[str, BaseVariant]],\n    ],\n    metric_tags: MutableTags,\n) -> GroupHashInfo:\n    \"\"\"\n    Calculate hashes for the job's event, create corresponding `GroupHash` entries if they don't yet\n    exist, and determine if there's an existing group associated with any of the hashes.\n\n    If the callback determines that it doesn't need to run its calculations (as may be the case with\n    secondary grouping), this will return an empty list of grouphashes (so iteration won't break)\n    and Nones for everything else.\n    \"\"\"\n    event = job[\"event\"]\n    project = event.project\n\n    # These will come back as Nones if the calculation decides it doesn't need to run\n    grouping_config, hashes, variants = hash_calculation_function(project, job, metric_tags)\n\n    if hashes:\n        grouphashes = get_or_create_grouphashes(\n            event, project, variants, hashes, grouping_config[\"id\"]\n        )\n\n        existing_grouphash = find_grouphash_with_group(grouphashes)\n\n        return GroupHashInfo(grouping_config, variants, hashes, grouphashes, existing_grouphash)\n    else:\n        return NULL_GROUPHASH_INFO\n\n\n@sentry_sdk.tracing.trace\ndef handle_existing_grouphash(\n    job: Job,\n    existing_grouphash: GroupHash,\n    all_grouphashes: list[GroupHash],\n) -> GroupInfo | None:\n    \"\"\"\n    Handle the case where an incoming event matches an existing group, by assigning the event to the\n    group, updating the group metadata with data from the event, and linking any newly-calculated\n    grouphashes to the group.\n    \"\"\"\n\n    # There is a race condition here where two processes could \"steal\"\n    # hashes from each other. In practice this should not be user-visible\n    # as group creation is synchronized, meaning the only way hashes could\n    # jump between groups is if there were two processes that:\n    #\n    # 1) have BOTH found an existing group\n    #    (otherwise at least one of them would be in the group creation\n    #    codepath which has transaction isolation/acquires row locks)\n    # 2) AND are looking at the same set, or an overlapping set of hashes\n    #    (otherwise they would not operate on the same rows)\n    # 3) yet somehow also retrieve different groups here\n    #    (otherwise the update would not change anything)\n    #\n    # We think this is a very unlikely situation. A previous version of\n    # this function had races around group creation which made this race\n    # more user visible. For more context, see 84c6f75a and d0e22787, as\n    # well as GH-5085.\n    group = Group.objects.get(id=existing_grouphash.group_id)\n\n    # As far as we know this has never happened, but in theory at least, the error event hashing\n    # algorithm and other event hashing algorithms could come up with the same hash value in the\n    # same project and our hash could have matched to a non-error group. Just to be safe, we make\n    # sure that's not the case before proceeding.\n    if is_non_error_type_group(group):\n        return None\n\n    # There may still be hashes that we did not use to find an existing\n    # group. A classic example is when grouping makes changes to the\n    # app-hash (changes to in_app logic), but the system hash stays\n    # stable and is used to find an existing group. Associate any new\n    # hashes with the group such that event saving continues to be\n    # resilient against grouping algorithm changes.\n    add_group_id_to_grouphashes(group, all_grouphashes)\n\n    is_regression = _process_existing_aggregate(\n        group=group,\n        event=job[\"event\"],\n        incoming_group_values=_get_group_processing_kwargs(job),\n        release=job[\"release\"],\n    )\n\n    return GroupInfo(group=group, is_new=False, is_regression=is_regression)\n\n\ndef create_group_with_grouphashes(job: Job, grouphashes: list[GroupHash]) -> GroupInfo | None:\n    \"\"\"\n    Create a group from the data in `job` and link it to the given grouphashes.\n\n    In very rare circumstances, we can end up in a race condition with another process trying to\n    create the same group. If the current process loses the race, this function will update the\n    group the other process just created, rather than creating a group itself.\n    \"\"\"\n    event = job[\"event\"]\n    project = event.project\n\n    # If the load-shed killswitch is enabled, this will raise a `HashDiscarded` error to pop us out\n    # of this function all the way back to `save_error_events`, preventing group creation\n    check_for_group_creation_load_shed(project, event)\n\n    with (\n        sentry_sdk.start_span(op=\"event_manager.create_group_transaction\") as span,\n        metrics.timer(\"event_manager.create_group_transaction\") as metrics_timer_tags,\n        transaction.atomic(router.db_for_write(GroupHash)),\n    ):\n        # These values will get overridden with whatever happens inside the lock if we do manage to\n        # acquire it, so it should only end up with `wait-for-lock` if we don't\n        span.set_tag(\"outcome\", \"wait_for_lock\")\n        metrics_timer_tags[\"outcome\"] = \"wait_for_lock\"\n\n        # If we're in this branch, we checked our grouphashes and didn't find one with a group\n        # attached. We thus want to create a new group, but we need to guard against another\n        # event with the same hash coming in before we're done here and also thinking it needs\n        # to create a new group. To prevent this, we're using double-checked locking\n        # (https://en.wikipedia.org/wiki/Double-checked_locking).\n\n        # First, try to lock the relevant rows in the `GroupHash` table. If another (identically\n        # hashed) event is also in the process of creating a group and has grabbed the lock\n        # before us, we'll block here until it's done. If not, we've now got the lock and other\n        # identically-hashed events will have to wait for us.\n        grouphashes = list(\n            GroupHash.objects.filter(\n                id__in=[h.id for h in grouphashes],\n            ).select_for_update()\n        )\n\n        # Now check again to see if any of our grouphashes have a group. In the first race\n        # condition scenario above, we'll have been blocked long enough for the other event to\n        # have created the group and updated our grouphashes with a group id, which means this\n        # time, we'll find something.\n        existing_grouphash = find_grouphash_with_group(grouphashes)\n\n        # If we still haven't found a matching grouphash, we're now safe to go ahead and create\n        # the group.\n        if existing_grouphash is None:\n            span.set_tag(\"outcome\", \"new_group\")\n            metrics_timer_tags[\"outcome\"] = \"new_group\"\n            record_new_group_metrics(event)\n\n            group = _create_group(project, event, **_get_group_processing_kwargs(job))\n            add_group_id_to_grouphashes(group, grouphashes)\n\n            return GroupInfo(group=group, is_new=True, is_regression=False)\n\n        # On the other hand, if we did in fact end up on the losing end of a race condition, treat\n        # this the same way we would if we'd found a grouphash to begin with (and never landed in\n        # this function at all)\n        else:\n            # TODO: should we be setting tags here, too?\n            return handle_existing_grouphash(job, existing_grouphash, grouphashes)\n\n\ndef _create_group(\n    project: Project,\n    event: Event,\n    *,\n    first_release: Release | None = None,\n    **group_creation_kwargs: Any,\n) -> Group:\n\n    short_id = _get_next_short_id(project)\n\n    # it's possible the release was deleted between\n    # when we queried for the release and now, so\n    # make sure it still exists\n    group_creation_kwargs[\"first_release_id\"] = (\n        Release.objects.filter(id=first_release.id).values_list(\"id\", flat=True).first()\n        if first_release\n        else None\n    )\n    group_creation_kwargs[\"substatus\"] = GroupSubStatus.NEW\n\n    group_data = group_creation_kwargs.pop(\"data\", {})\n\n    # add sdk tag to metadata\n    group_data.setdefault(\"metadata\", {}).update(sdk_metadata_from_event(event))\n\n    # add severity to metadata for alert filtering\n    severity: Mapping[str, Any] = {}\n    try:\n        group_type = group_creation_kwargs.get(\"type\", None)\n        severity = _get_severity_metadata_for_group(event, project.id, group_type)\n        group_data[\"metadata\"].update(severity)\n    except Exception as e:\n        logger.exception(\n            \"Failed to get severity metadata for group\",\n            repr(e),\n            extra={\"event_id\": event.event_id},\n        )\n\n    # the kwargs only include priority for non-error issue platform events, which takes precedence.\n    priority = group_creation_kwargs.get(\"priority\", None)\n    if priority is None:\n        priority = _get_priority_for_group(severity, group_creation_kwargs)\n\n    group_creation_kwargs[\"priority\"] = priority\n    group_data[\"metadata\"][\"initial_priority\"] = priority\n    group_creation_kwargs[\"data\"] = group_data\n\n    # Set initial times_seen\n    group_creation_kwargs[\"times_seen\"] = 1\n\n    # If the project is in the allowlist, use the client sample rate to weight the times_seen\n    if project.id in options.get(\"issues.client_error_sampling.project_allowlist\"):\n        group_creation_kwargs[\"times_seen\"] = _get_error_weighted_times_seen(event)\n\n    try:\n        with transaction.atomic(router.db_for_write(Group)):\n            # This is the 99.999% path. The rest of the function is all to handle a very rare and\n            # very confounding bug which keeps projects from creating new groups.\n            group = Group.objects.create(\n                project=project,\n                short_id=short_id,\n                **group_creation_kwargs,\n            )\n\n    # Attempt to handle The Mysterious Case of the Stuck Project Counter\n    except IntegrityError as err:\n        if not _is_stuck_counter_error(err, project, short_id):\n            raise\n\n        # Note: There is a potential race condition here, if two events simultaneously try to fix\n        # the counter. Our hunch is that the only effect of that would be to over-increment, which\n        # shouldn't cause any problems. Nonetheless, if we run into trouble with this workaround,\n        # that's one thing to further investigate.\n        new_short_id = _handle_stuck_project_counter(project, short_id)\n\n        # Now that we've theoretically unstuck the counter, try again to create the group\n        try:\n            with transaction.atomic(router.db_for_write(Group)):\n                group = Group.objects.create(\n                    project=project,\n                    short_id=new_short_id,\n                    **group_creation_kwargs,\n                )\n\n        except Exception:\n            # Maybe the stuck counter was hiding some other error\n            logger.exception(\"Error after unsticking project counter\")\n            raise\n\n    if features.has(\"organizations:issue-open-periods\", project.organization):\n        GroupOpenPeriod.objects.create(\n            group=group,\n            project_id=project.id,\n            date_started=group.first_seen,\n            date_ended=None,\n        )\n    return group\n\n\ndef _get_error_weighted_times_seen(event: BaseEvent) -> int:\n    if event.get_event_type() in (\"error\", \"default\"):\n        error_sample_rate = event.data.get(\"sample_rate\")\n        if error_sample_rate is not None and error_sample_rate > 0:\n            return int(1 / error_sample_rate)\n    return 1\n\n\ndef _is_stuck_counter_error(err: Exception, project: Project, short_id: int) -> bool:\n    \"\"\"Decide if this is `UniqueViolation` error on the `Group` table's project and short id values.\"\"\"\n\n    error_message = err.args[0]\n\n    if not error_message.startswith(\"UniqueViolation\"):\n        return False\n\n    for substring in [\n        f\"Key (project_id, short_id)=({project.id}, {short_id}) already exists.\",\n        'duplicate key value violates unique constraint \"sentry_groupedmessage_project_id_short_id',\n    ]:\n        if substring in error_message:\n            return True\n\n    return False\n\n\ndef _handle_stuck_project_counter(project: Project, current_short_id: int) -> int:\n    \"\"\"\n    Sometimes, for reasons unknown, a project's `Counter` value falls behind its latest group `short_id` value.\n    When that happens, that incorrect counter value leads us to try to create groups with `short_id`s which\n    are already taken.\n\n    This handles that case by updating the counter's value to the latest group `short_id`, and then returns\n    the new value.\n    \"\"\"\n    new_short_id = current_short_id\n\n    # Ordinarily running max on this many rows would be prohibitively expensive, but a) this is\n    # a very rare case (< 20 ever that we know of), and b) project and short id are indexed\n    # together in order to enforce the unique constraint which got us here in the first place,\n    # so it's faster than it otherwise might be. We can time it just in case, though.\n    with metrics.timer(\"stuck_project.max_short_id_query\"):\n        max_short_id_for_project = Group.objects.filter(project_id=project.id).aggregate(\n            Max(\"short_id\")\n        )[\"short_id__max\"]\n\n    # Add 1 because we're trying to mimic a value which would already have been incremented\n    correct_value = max_short_id_for_project + 1\n\n    if current_short_id < correct_value:\n        difference = correct_value - current_short_id\n        # `_get_next_short_id` corrects the `Counter` value before it returns the new short_id\n        new_short_id = _get_next_short_id(project, delta=difference)\n\n        logger.info(\n            \"Fixed stuck counter value.\", extra={\"project\": project.id, \"difference\": difference}\n        )\n        metrics.incr(\n            \"stuck_project.fixed_counter\", tags={\"difference\": difference}, sample_rate=1.0\n        )\n\n    return new_short_id\n\n\ndef _get_next_short_id(project: Project, delta: int = 1) -> int:\n    try:\n        short_id = project.next_short_id(delta=delta)\n    except OperationalError:\n        metrics.incr(\"next_short_id.timeout\")\n        sentry_sdk.capture_message(\"short_id.timeout\")\n        raise HashDiscarded(\"Timeout when getting next_short_id\", reason=\"timeout\")\n\n    return short_id\n\n\ndef _handle_regression(group: Group, event: BaseEvent, release: Release | None) -> bool | None:\n    if not group.is_resolved():\n        return None\n\n    # we only mark it as a regression if the event's release is newer than\n    # the release which we originally marked this as resolved\n    elif GroupResolution.has_resolution(group, release):\n        return None\n\n    elif has_pending_commit_resolution(group):\n        return None\n\n    if not plugin_is_regression(group, event):\n        return None\n\n    # we now think its a regression, rely on the database to validate that\n    # no one beat us to this\n    date = max(event.datetime, group.last_seen)\n    is_regression = bool(\n        Group.objects.filter(\n            id=group.id,\n            # ensure we can't update things if the status has been set to\n            # ignored\n            status__in=[GroupStatus.RESOLVED, GroupStatus.UNRESOLVED],\n        )\n        .exclude(\n            # add to the regression window to account for races here\n            active_at__gte=date\n            - timedelta(seconds=5)\n        )\n        .update(\n            active_at=date,\n            # explicitly set last_seen here as ``is_resolved()`` looks\n            # at the value\n            last_seen=date,\n            status=GroupStatus.UNRESOLVED,\n            substatus=GroupSubStatus.REGRESSED,\n        )\n    )\n\n    group.active_at = date\n    group.status = GroupStatus.UNRESOLVED\n    group.substatus = GroupSubStatus.REGRESSED\n    # groups may have been updated already from a separate event that groups to the same group\n    # only fire these signals the first time the row was actually updated\n    if is_regression:\n        issue_unresolved.send_robust(\n            project=group.project,\n            user=None,\n            group=group,\n            transition_type=\"automatic\",\n            sender=\"handle_regression\",\n        )\n        if not options.get(\"groups.enable-post-update-signal\"):\n            post_save.send_robust(\n                sender=Group,\n                instance=group,\n                created=False,\n                update_fields=[\"last_seen\", \"active_at\", \"status\", \"substatus\"],\n            )\n\n    follows_semver = False\n    resolved_in_activity = None\n    if is_regression and release:\n        resolution = None\n\n        # resolutions are only valid if the state of the group is still\n        # resolved -- if it were to change the resolution should get removed\n        try:\n            resolution = GroupResolution.objects.get(group=group)\n        except GroupResolution.DoesNotExist:\n            affected = False\n        else:\n            cursor = connection.cursor()\n            # delete() API does not return affected rows\n            cursor.execute(\"DELETE FROM sentry_groupresolution WHERE id = %s\", [resolution.id])\n            affected = cursor.rowcount > 0\n\n        if affected and resolution:\n            # if we had to remove the GroupResolution (i.e. we beat the\n            # the queue to handling this) then we need to also record\n            # the corresponding event\n            try:\n                resolved_in_activity = Activity.objects.filter(\n                    group=group,\n                    type=ActivityType.SET_RESOLVED_IN_RELEASE.value,\n                    ident=resolution.id,\n                ).order_by(\"-datetime\")[0]\n            except IndexError:\n                # XXX: handle missing data, as its not overly important\n                pass\n            else:\n                try:\n                    # We should only update last activity version prior to the regression in the\n                    # case where we have \"Resolved in upcoming release\" i.e. version == \"\"\n                    # We also should not override the `data` attribute here because it might have\n                    # a `current_release_version` for semver releases and we wouldn't want to\n                    # lose that\n                    if resolved_in_activity.data[\"version\"] == \"\":\n                        resolved_in_activity.update(\n                            data={**resolved_in_activity.data, \"version\": release.version}\n                        )\n                except KeyError:\n                    # Safeguard in case there is no \"version\" key. However, should not happen\n                    resolved_in_activity.update(data={\"version\": release.version})\n\n            # Record how we compared the two releases\n            follows_semver = follows_semver_versioning_scheme(\n                project_id=group.project.id,\n                org_id=group.organization.id,\n                release_version=release.version,\n            )\n\n    if is_regression:\n        activity_data: dict[str, str | bool] = {\n            \"event_id\": event.event_id,\n            \"version\": release.version if release else \"\",\n        }\n        if resolved_in_activity and release:\n            activity_data.update(\n                {\n                    \"follows_semver\": follows_semver,\n                    \"resolved_in_version\": resolved_in_activity.data.get(\n                        \"version\", release.version\n                    ),\n                }\n            )\n\n        Activity.objects.create_group_activity(\n            group,\n            ActivityType.SET_REGRESSION,\n            data=activity_data,\n        )\n        record_group_history(group, GroupHistoryStatus.REGRESSED, actor=None, release=release)\n\n        kick_off_status_syncs.apply_async(\n            kwargs={\"project_id\": group.project_id, \"group_id\": group.id}\n        )\n        if has_initial_open_period(group):\n            create_open_period(group, date)\n\n    return is_regression\n\n\ndef _is_placeholder_title(title):\n    return title in PLACEHOLDER_EVENT_TITLES\n\n\ndef _is_real_title(title):\n    return bool(title) and title not in PLACEHOLDER_EVENT_TITLES\n\n\ndef _get_updated_group_title(existing_container, incoming_container):\n    \"\"\"\n    Given either `group.data` or `group.data[\"metadata\"]`, in both existing and incoming forms, pick\n    the correct title to use when updating the group. Uses the incoming title (or `None` if there\n    isn't one) except in  the case where a placeholder title (`<unlabeled event>`, `<untitled>`,\n    etc) would be replacing a non-placeholder title (either `None` or a real title).\n\n    This stems from an incident during which we were interpreting error events as default-type\n    events and thereby overwriting good titles with placeholder ones and inserting placeholder\n    titles where there shouldn't have been a title at all. (The second case matters because\n    default-type and error-type events differ in where they include a `title` attribute, and we\n    count on the lack of a `title` attribute in certain cases as well as the presence of one.) This\n    prevents that from happening in the future and will delete errant placeholder titles by\n    overwriting them with `None`.\n    \"\"\"\n\n    existing_title = existing_container.get(\"title\")\n    incoming_title = incoming_container.get(\"title\")\n\n    return (\n        incoming_title\n        if (\n            # Real titles beat both placeholder and non-existent titles\n            _is_real_title(incoming_title)\n            or\n            # Conversely, placeholder titles lose to both real titles and lack of a title (the\n            # latter in order to fix the regression caused by error events being interpreted as\n            # default-type events)\n            _is_placeholder_title(existing_title)\n        )\n        else existing_title\n    )\n\n\ndef _process_existing_aggregate(\n    group: Group,\n    event: BaseEvent,\n    incoming_group_values: Mapping[str, Any],\n    release: Release | None,\n) -> bool:\n    last_seen = max(event.datetime, group.last_seen)\n    updated_group_values: dict[str, Any] = {\"last_seen\": last_seen}\n    # Unclear why this is necessary, given that it's also in `updated_group_values`, but removing\n    # it causes unrelated tests to fail. Hard to say if that's the tests or the removal, though.\n    group.last_seen = updated_group_values[\"last_seen\"]\n\n    if (\n        event.search_message\n        and event.search_message != group.message\n        and not _is_placeholder_title(event.search_message)\n        and event.get_event_type() != TransactionEvent.key\n    ):\n        updated_group_values[\"message\"] = event.search_message\n    if group.level != incoming_group_values[\"level\"]:\n        updated_group_values[\"level\"] = incoming_group_values[\"level\"]\n    if group.culprit != incoming_group_values[\"culprit\"]:\n        updated_group_values[\"culprit\"] = incoming_group_values[\"culprit\"]\n\n    # If the new event has a timestamp earlier than our current `fist_seen` value (which can happen,\n    # for example because of misaligned internal clocks on two different host machines or because of\n    # race conditions) then we want to use the current event's time\n    if group.first_seen > event.datetime:\n        updated_group_values[\"first_seen\"] = event.datetime\n\n    is_regression = _handle_regression(group, event, release)\n\n    existing_data = group.data\n    existing_metadata = group.data.get(\"metadata\", {})\n\n    incoming_data = incoming_group_values[\"data\"]\n    incoming_metadata = incoming_group_values[\"data\"].get(\"metadata\", {})\n\n    # Merge old and new data/metadata, keeping the existing title if the incoming title is a\n    # placeholder (`<unlabeled event`, `<untitled>`, etc.) and the existing one isn't. See\n    # `_get_updated_group_title` docstring.\n    updated_group_values[\"data\"] = {\n        **existing_data,\n        **incoming_data,\n        \"title\": _get_updated_group_title(existing_data, incoming_data),\n    }\n    updated_group_values[\"data\"][\"metadata\"] = {\n        **existing_metadata,\n        **incoming_metadata,\n        \"title\": _get_updated_group_title(existing_metadata, incoming_metadata),\n    }\n    initial_priority = updated_group_values[\"data\"][\"metadata\"].get(\"initial_priority\")\n    if initial_priority is not None:\n        # cast to an int, as we don't want to pickle enums into task args.\n        updated_group_values[\"data\"][\"metadata\"][\"initial_priority\"] = int(initial_priority)\n\n    # We pass `times_seen` separately from all of the other columns so that `buffer_inr` knows to\n    # increment rather than overwrite the existing value\n    times_seen = 1\n    if group.project.id in options.get(\"issues.client_error_sampling.project_allowlist\"):\n        times_seen = _get_error_weighted_times_seen(event)\n\n    buffer_incr(Group, {\"times_seen\": times_seen}, {\"id\": group.id}, updated_group_values)\n\n    return bool(is_regression)\n\n\nseverity_connection_pool = connection_from_url(\n    settings.SEER_SEVERITY_URL,\n    retries=settings.SEER_SEVERITY_RETRIES,\n    timeout=settings.SEER_SEVERITY_TIMEOUT,  # Defaults to 300 milliseconds\n)\n\n\ndef _get_severity_metadata_for_group(\n    event: Event, project_id: int, group_type: int | None\n) -> Mapping[str, Any]:\n    \"\"\"\n    Returns severity metadata for an event if all of the following are true\n    - the feature flag is enabled\n    - the event platform supports severity\n    - the event group type is an error\n\n    Returns {} if conditions aren't met or on exception.\n    \"\"\"\n    from sentry.receivers.rules import PLATFORMS_WITH_PRIORITY_ALERTS\n\n    if killswitch_matches_context(\n        \"issues.severity.skip-seer-requests\", {\"project_id\": event.project_id}\n    ):\n        logger.warning(\n            \"get_severity_metadata_for_group.seer_killswitch_enabled\",\n            extra={\"event_id\": event.event_id, \"project_id\": project_id},\n        )\n        metrics.incr(\"issues.severity.seer_killswitch_enabled\")\n        return {}\n\n    seer_based_priority_enabled = features.has(\n        \"organizations:seer-based-priority\", event.project.organization, actor=None\n    )\n    if not seer_based_priority_enabled:\n        return {}\n\n    feature_enabled = features.has(\"projects:first-event-severity-calculation\", event.project)\n    if not feature_enabled:\n        return {}\n\n    is_supported_platform = (\n        any(event.platform.startswith(platform) for platform in PLATFORMS_WITH_PRIORITY_ALERTS)\n        if event.platform\n        else False\n    )\n    if not is_supported_platform:\n        return {}\n\n    is_error_group = group_type == ErrorGroupType.type_id if group_type else True\n    if not is_error_group:\n        return {}\n\n    passthrough_data = options.get(\n        \"issues.severity.seer-circuit-breaker-passthrough-limit\",\n        CircuitBreakerPassthrough(limit=1, window=10),\n    )\n    if circuit_breaker_activated(\"sentry.seer.severity\", passthrough_data=passthrough_data):\n        logger.warning(\n            \"get_severity_metadata_for_group.circuit_breaker_activated\",\n            extra={\"event_id\": event.event_id, \"project_id\": project_id},\n        )\n        return {}\n\n    from sentry import ratelimits as ratelimiter\n\n    ratelimit = options.get(\"issues.severity.seer-global-rate-limit\")\n    # This is temporary until we update the option values to be a dict\n    if \"limit\" not in ratelimit or \"window\" not in ratelimit:\n        return {}\n\n    if ratelimiter.backend.is_limited(\n        \"seer:severity-calculation:global-limit\",\n        limit=ratelimit[\"limit\"],\n        window=ratelimit[\"window\"],\n    ):\n        logger.warning(\n            \"get_severity_metadata_for_group.rate_limited_globally\",\n            extra={\"event_id\": event.event_id, \"project_id\": project_id},\n        )\n        metrics.incr(\"issues.severity.rate_limited_globally\")\n        return {}\n\n    ratelimit = options.get(\"issues.severity.seer-project-rate-limit\")\n    # This is temporary until we update the option values to be a dict\n    if \"limit\" not in ratelimit or \"window\" not in ratelimit:\n        return {}\n\n    if ratelimiter.backend.is_limited(\n        f\"seer:severity-calculation:{project_id}\",\n        limit=ratelimit[\"limit\"],\n        window=ratelimit[\"window\"],\n    ):\n        logger.warning(\n            \"get_severity_metadata_for_group.rate_limited_for_project\",\n            extra={\"event_id\": event.event_id, \"project_id\": project_id},\n        )\n        metrics.incr(\"issues.severity.rate_limited_for_project\", tags={\"project_id\": project_id})\n        return {}\n\n    try:\n        severity, reason = _get_severity_score(event)\n\n        return {\n            \"severity\": severity,\n            \"severity_reason\": reason,\n        }\n    except Exception as e:\n        logger.warning(\"Failed to calculate severity score for group\", repr(e))\n        update_severity_error_count()\n        metrics.incr(\"issues.severity.error\")\n        return {}\n\n\ndef _get_priority_for_group(severity: Mapping[str, Any], kwargs: Mapping[str, Any]) -> int:\n    \"\"\"\n    Returns priority for an event based on severity score and log level.\n    \"\"\"\n    try:\n        level = kwargs.get(\"level\", None)\n        severity_score = severity.get(\"severity\", None)\n\n        if level in [logging.INFO, logging.DEBUG]:\n            return PriorityLevel.LOW\n\n        elif level == logging.FATAL:\n            return PriorityLevel.HIGH\n\n        elif level == logging.WARNING:\n            if severity_score is None or severity_score < HIGH_SEVERITY_THRESHOLD:\n                return PriorityLevel.MEDIUM\n\n            return PriorityLevel.HIGH  # severity_score >= HIGH_SEVERITY_THRESHOLD\n        elif level == logging.ERROR:\n            if severity_score is None or severity_score >= HIGH_SEVERITY_THRESHOLD:\n                return PriorityLevel.HIGH\n\n            return PriorityLevel.MEDIUM  # severity_score < HIGH_SEVERITY_THRESHOLD\n\n        logger.warning(\"Unknown log level %s or severity score %s\", level, severity_score)\n        return PriorityLevel.MEDIUM\n    except Exception as e:\n        logger.exception(\n            \"Failed to calculate priority for group\",\n            repr(e),\n            extra={\n                \"severity\": severity,\n                \"kwargs\": kwargs,\n            },\n        )\n\n        return PriorityLevel.MEDIUM\n\n\ndef update_severity_error_count(reset=False) -> None:\n    timeout = 60 * 60  # 1 hour\n    if reset:\n        cache.set(SEER_ERROR_COUNT_KEY, 0, timeout=timeout)\n        return\n\n    try:\n        cache.incr(SEER_ERROR_COUNT_KEY)\n        cache.touch(SEER_ERROR_COUNT_KEY, timeout=timeout)\n    except ValueError:\n        cache.set(SEER_ERROR_COUNT_KEY, 1, timeout=timeout)\n\n\ndef _get_severity_score(event: Event) -> tuple[float, str]:\n    # Short circuit the severity value if we know the event is fatal or info/debug\n    level = str(event.data.get(\"level\", \"error\"))\n    if LOG_LEVELS_MAP[level] == logging.FATAL:\n        return 1.0, \"log_level_fatal\"\n    if LOG_LEVELS_MAP[level] <= logging.INFO:\n        return 0.0, \"log_level_info\"\n\n    op = \"event_manager._get_severity_score\"\n    logger_data = {\"event_id\": event.data[\"event_id\"], \"op\": op}\n    severity = 1.0\n    reason = None\n\n    event_type = get_event_type(event.data)\n    metadata = event_type.get_metadata(event.data)\n\n    exception_type = metadata.get(\"type\")\n    exception_value = metadata.get(\"value\")\n\n    if exception_type:\n        title = exception_type\n        if exception_value:\n            title += f\": {exception_value}\"\n\n        # We truncate the title to 128 characters as any more than that is unlikely to be helpful\n        # and would slow down the model.\n        title = trim(title, 128)\n    else:\n        # Fall back to using just the title for events without an exception.\n        title = event.title\n\n    # If all we have is `<unlabeled event>` (or one of its equally unhelpful friends), bail\n    if title in PLACEHOLDER_EVENT_TITLES:\n        logger_data.update({\"event_type\": event_type.key, \"title\": title})\n        logger.warning(\n            \"Unable to get severity score because of unusable `message` value '%s'\",\n            title,\n            extra=logger_data,\n        )\n        return 0.0, \"bad_title\"\n\n    payload = {\n        \"message\": title,\n        \"has_stacktrace\": int(has_stacktrace(event.data)),\n        \"handled\": is_handled(event.data),\n    }\n\n    if options.get(\"processing.severity-backlog-test.timeout\"):\n        payload[\"trigger_timeout\"] = True\n    if options.get(\"processing.severity-backlog-test.error\"):\n        payload[\"trigger_error\"] = True\n\n    logger_data[\"payload\"] = payload\n\n    with sentry_sdk.start_span(op=op):\n        try:\n            with metrics.timer(op):\n                timeout = options.get(\n                    \"issues.severity.seer-timout\",\n                    settings.SEER_SEVERITY_TIMEOUT / 1000,\n                )\n                response = make_signed_seer_api_request(\n                    severity_connection_pool,\n                    \"/v0/issues/severity-score\",\n                    body=orjson.dumps(payload),\n                    timeout=timeout,\n                )\n                severity = orjson.loads(response.data).get(\"severity\")\n                reason = \"ml\"\n        except MaxRetryError:\n            reason = \"microservice_max_retry\"\n            update_severity_error_count()\n            metrics.incr(\"issues.severity.error\", tags={\"reason\": \"max_retries\"})\n            logger.exception(\"Seer severity microservice max retries exceeded\")\n        except TimeoutError:\n            reason = \"microservice_timeout\"\n            update_severity_error_count()\n            metrics.incr(\"issues.severity.error\", tags={\"reason\": \"timeout\"})\n            logger.exception(\"Seer severity microservice timeout\")\n        except Exception:\n            reason = \"microservice_error\"\n            update_severity_error_count()\n            metrics.incr(\"issues.severity.error\", tags={\"reason\": \"unknown\"})\n            logger.exception(\"Seer severity microservice error\")\n            sentry_sdk.capture_exception()\n        else:\n            update_severity_error_count(reset=True)\n\n    return severity, reason\n\n\nAttachment = CachedAttachment\n\n\n@sentry_sdk.tracing.trace\ndef discard_event(job: Job, attachments: Sequence[Attachment]) -> None:\n    \"\"\"\n    Refunds consumed quotas for an event and its attachments.\n\n    For the event and each dropped attachment, an outcome\n    FILTERED(discarded-hash) is emitted.\n\n    :param job:         The job context container.\n    :param attachments: The full list of attachments to filter.\n    \"\"\"\n\n    project = job[\"event\"].project\n\n    quotas.backend.refund(\n        project,\n        key=job[\"project_key\"],\n        timestamp=job[\"start_time\"],\n        category=job[\"category\"],\n        quantity=1,\n    )\n\n    track_outcome(\n        org_id=project.organization_id,\n        project_id=job[\"project_id\"],\n        key_id=job[\"key_id\"],\n        outcome=Outcome.FILTERED,\n        reason=FilterStatKeys.DISCARDED_HASH,\n        timestamp=to_datetime(job[\"start_time\"]),\n        event_id=job[\"event\"].event_id,\n        category=job[\"category\"],\n    )\n\n    attachment_quantity = 0\n    for attachment in attachments:\n        # Quotas are counted with at least ``1`` for attachments.\n        attachment_quantity += attachment.size or 1\n\n        track_outcome(\n            org_id=project.organization_id,\n            project_id=job[\"project_id\"],\n            key_id=job[\"key_id\"],\n            outcome=Outcome.FILTERED,\n            reason=FilterStatKeys.DISCARDED_HASH,\n            timestamp=to_datetime(job[\"start_time\"]),\n            event_id=job[\"event\"].event_id,\n            category=DataCategory.ATTACHMENT,\n            quantity=attachment.size,\n        )\n\n    if attachment_quantity:\n        quotas.backend.refund(\n            project,\n            key=job[\"project_key\"],\n            timestamp=job[\"start_time\"],\n            category=DataCategory.ATTACHMENT,\n            quantity=attachment_quantity,\n        )\n\n    metrics.incr(\n        \"events.discarded\",\n        skip_internal=True,\n        tags={\n            \"platform\": job[\"platform\"],\n            \"sdk\": normalized_sdk_tag_from_event(job[\"event\"].data),\n        },\n    )\n\n\n@sentry_sdk.tracing.trace\ndef get_attachments(cache_key: str | None, job: Job) -> list[Attachment]:\n    \"\"\"\n    Retrieves the list of attachments for this event.\n\n    This method skips attachments that have been marked for rate limiting by\n    earlier ingestion pipeline.\n\n    :param cache_key: The cache key at which the event payload is stored in the\n                      cache. This is used to retrieve attachments.\n    :param job:       The job context container.\n    \"\"\"\n    if cache_key is None:\n        return []\n\n    project = job[\"event\"].project\n    if not features.has(\"organizations:event-attachments\", project.organization, actor=None):\n        return []\n\n    attachments = list(attachment_cache.get(cache_key))\n    if not attachments:\n        return []\n\n    return [attachment for attachment in attachments if not attachment.rate_limited]\n\n\n@sentry_sdk.tracing.trace\ndef filter_attachments_for_group(attachments: list[Attachment], job: Job) -> list[Attachment]:\n    \"\"\"\n    Removes crash reports exceeding the group-limit.\n\n    If the project or organization is configured to limit the amount of crash\n    reports per group, the number of stored crashes is limited. This requires\n    `event.group` to be set.\n\n    Emits one outcome per removed attachment.\n\n    :param attachments: The full list of attachments to filter.\n    :param job:         The job context container.\n    \"\"\"\n    if not attachments:\n        return attachments\n\n    event = job[\"event\"]\n    project = event.project\n\n    # The setting is both an organization and project setting. The project\n    # setting strictly overrides the organization setting, unless set to the\n    # default.\n    max_crashreports = get_max_crashreports(project, allow_none=True)\n    if max_crashreports is None:\n        max_crashreports = get_max_crashreports(project.organization)\n\n    # The number of crash reports is cached per group\n    crashreports_key = get_crashreport_key(event.group_id)\n\n    # Only fetch the number of stored crash reports if there is a crash report\n    # in the list of attachments. Otherwise, we won't require this number.\n    if any(attachment.type in CRASH_REPORT_TYPES for attachment in attachments):\n        cached_reports = get_stored_crashreports(crashreports_key, event, max_crashreports)\n    else:\n        cached_reports = 0\n    stored_reports = cached_reports\n\n    filtered = []\n    refund_quantity = 0\n    for attachment in attachments:\n        # If the attachment is a crash report (e.g. minidump), we need to honor\n        # the store_crash_reports setting. Otherwise, we assume that the client\n        # has already verified PII and just store the attachment.\n        if attachment.type in CRASH_REPORT_TYPES:\n            if crashreports_exceeded(stored_reports, max_crashreports):\n                # Indicate that the crash report has been removed due to a limit\n                # on the maximum number of crash reports. If this flag is True,\n                # it indicates that there are *other* events in the same group\n                # that store a crash report. This flag will therefore *not* be\n                # set if storage of crash reports is completely disabled.\n                if max_crashreports > 0:\n                    job[\"data\"][\"metadata\"][\"stripped_crash\"] = True\n\n                track_outcome(\n                    org_id=event.project.organization_id,\n                    project_id=job[\"project_id\"],\n                    key_id=job[\"key_id\"],\n                    outcome=Outcome.FILTERED,\n                    reason=FilterStatKeys.CRASH_REPORT_LIMIT,\n                    timestamp=to_datetime(job[\"start_time\"]),\n                    event_id=event.event_id,\n                    category=DataCategory.ATTACHMENT,\n                    quantity=attachment.size,\n                )\n\n                # Quotas are counted with at least ``1`` for attachments.\n                refund_quantity += attachment.size or 1\n                continue\n            stored_reports += 1\n\n        filtered.append(attachment)\n\n    # Check if we have exceeded the stored crash reports count. If so, we\n    # persist the current maximum (not the actual number!) into the cache. Next\n    # time when loading from the cache, we will validate that this number has\n    # not changed, or otherwise re-fetch from the database.\n    if crashreports_exceeded(stored_reports, max_crashreports) and stored_reports > cached_reports:\n        cache.set(crashreports_key, max_crashreports, CRASH_REPORT_TIMEOUT)\n\n    if refund_quantity:\n        quotas.backend.refund(\n            project,\n            key=job[\"project_key\"],\n            timestamp=job[\"start_time\"],\n            category=DataCategory.ATTACHMENT,\n            quantity=refund_quantity,\n        )\n\n    return filtered\n\n\n@sentry_sdk.tracing.trace\ndef save_attachment(\n    cache_key: str | None,\n    attachment: Attachment,\n    project: Project,\n    event_id: str,\n    key_id: int | None = None,\n    group_id: int | None = None,\n    start_time: float | None = None,\n) -> None:\n    \"\"\"\n    Persists a cached event attachments into the file store.\n\n    Emits one outcome, either ACCEPTED on success or INVALID(missing_chunks) if\n    retrieving the attachment data fails.\n\n    :param cache_key:  The cache key at which the attachment is stored for\n                       debugging purposes.\n    :param attachment: The ``CachedAttachment`` instance to store.\n    :param project:    The project model that this attachment belongs to.\n    :param event_id:   Identifier of the event that this attachment belongs to.\n                       The event does not have to be stored yet.\n    :param key_id:     Optional identifier of the DSN that was used to ingest\n                       the attachment.\n    :param group_id:   Optional group identifier for the event. May be empty if\n                       the event has not been stored yet, or if it is not\n                       grouped.\n    :param start_time: UNIX Timestamp (float) when the attachment was ingested.\n                       If missing, the current time is used.\n    \"\"\"\n    if start_time is not None:\n        timestamp = to_datetime(start_time)\n    else:\n        timestamp = datetime.now(timezone.utc)\n\n    try:\n        attachment.data\n    except MissingAttachmentChunks:\n        track_outcome(\n            org_id=project.organization_id,\n            project_id=project.id,\n            key_id=key_id,\n            outcome=Outcome.INVALID,\n            reason=\"missing_chunks\",\n            timestamp=timestamp,\n            event_id=event_id,\n            category=DataCategory.ATTACHMENT,\n        )\n\n        logger.exception(\"Missing chunks for cache_key=%s\", cache_key)\n        return\n    from sentry import ratelimits as ratelimiter\n\n    is_limited, _, _ = ratelimiter.backend.is_limited_with_value(\n        key=\"event_attachment.save_per_sec\",\n        limit=options.get(\"sentry.save-event-attachments.project-per-sec-limit\"),\n        project=project,\n        window=1,\n    )\n    rate_limit_tag = \"per_sec\"\n    if not is_limited:\n        is_limited, _, _ = ratelimiter.backend.is_limited_with_value(\n            key=\"event_attachment.save_5_min\",\n            limit=options.get(\"sentry.save-event-attachments.project-per-5-minute-limit\"),\n            project=project,\n            window=5 * 60,\n        )\n        rate_limit_tag = \"per_five_min\"\n    if is_limited:\n        metrics.incr(\n            \"event_manager.attachments.rate_limited\", tags={\"rate_limit_type\": rate_limit_tag}\n        )\n        track_outcome(\n            org_id=project.organization_id,\n            project_id=project.id,\n            key_id=key_id,\n            outcome=Outcome.RATE_LIMITED,\n            reason=\"rate_limited\",\n            timestamp=timestamp,\n            event_id=event_id,\n            category=DataCategory.ATTACHMENT,\n            quantity=attachment.size or 1,\n        )\n        return\n\n    file = EventAttachment.putfile(project.id, attachment)\n\n    EventAttachment.objects.create(\n        # lookup:\n        project_id=project.id,\n        group_id=group_id,\n        event_id=event_id,\n        # metadata:\n        type=attachment.type,\n        name=attachment.name,\n        content_type=file.content_type,\n        size=file.size,\n        sha1=file.sha1,\n        # storage:\n        blob_path=file.blob_path,\n    )\n\n    track_outcome(\n        org_id=project.organization_id,\n        project_id=project.id,\n        key_id=key_id,\n        outcome=Outcome.ACCEPTED,\n        reason=None,\n        timestamp=timestamp,\n        event_id=event_id,\n        category=DataCategory.ATTACHMENT,\n        quantity=attachment.size or 1,\n    )\n\n\ndef save_attachments(cache_key: str | None, attachments: list[Attachment], job: Job) -> None:\n    \"\"\"\n    Persists cached event attachments into the file store.\n\n    Emits one outcome per attachment, either ACCEPTED on success or\n    INVALID(missing_chunks) if retrieving the attachment fails.\n    :param cache_key:  The cache key at which the attachment is stored for\n                       debugging purposes.\n    :param attachments: A filtered list of attachments to save.\n    :param job:         The job context container.\n    \"\"\"\n\n    event = job[\"event\"]\n\n    for attachment in attachments:\n        save_attachment(\n            cache_key,\n            attachment,\n            event.project,\n            event.event_id,\n            key_id=job[\"key_id\"],\n            group_id=event.group_id,\n            start_time=job[\"start_time\"],\n        )\n\n\n@sentry_sdk.tracing.trace\ndef _materialize_event_metrics(jobs: Sequence[Job]) -> None:\n    for job in jobs:\n        # Ensure the _metrics key exists. This is usually created during\n        # and prefilled with ingestion sizes.\n        event_metrics = job[\"event\"].data.get(\"_metrics\") or {}\n        job[\"event\"].data[\"_metrics\"] = event_metrics\n\n        # Capture the actual size that goes into node store.\n        event_metrics[\"bytes.stored.event\"] = len(\n            orjson.dumps(dict(job[\"event\"].data.items())).decode()\n        )\n\n        for metric_name in (\"flag.processing.error\", \"flag.processing.fatal\"):\n            if event_metrics.get(metric_name):\n                metrics.incr(f\"event_manager.save.event_metrics.{metric_name}\")\n\n        job[\"event_metrics\"] = event_metrics\n\n\n@sentry_sdk.tracing.trace\ndef _calculate_span_grouping(jobs: Sequence[Job], projects: ProjectsMapping) -> None:\n    for job in jobs:\n        # Make sure this snippet doesn't crash ingestion\n        # as the feature is under development.\n        try:\n            event = job[\"event\"]\n            groupings = event.get_span_groupings()\n            groupings.write_to_event(event.data)\n\n            metrics.distribution(\"save_event.transaction.span_count\", len(groupings.results))\n            unique_default_hashes = set(groupings.results.values())\n            metrics.incr(\n                \"save_event.transaction.span_group_count.default\",\n                amount=len(unique_default_hashes),\n                tags={\n                    \"platform\": job[\"platform\"] or \"unknown\",\n                    \"sdk\": normalized_sdk_tag_from_event(event.data),\n                },\n            )\n        except Exception:\n            sentry_sdk.capture_exception()\n\n\n@sentry_sdk.tracing.trace\ndef _detect_performance_problems(jobs: Sequence[Job], projects: ProjectsMapping) -> None:\n    for job in jobs:\n        if job[\"data\"].get(\"_performance_issues_spans\"):\n            job[\"performance_problems\"] = []\n        else:\n            job[\"performance_problems\"] = detect_performance_problems(\n                job[\"data\"], projects[job[\"project_id\"]]\n            )\n\n\nINSIGHT_MODULE_TO_PROJECT_FLAG_NAME: dict[InsightModules, str] = {\n    InsightModules.HTTP: \"has_insights_http\",\n    InsightModules.DB: \"has_insights_db\",\n    InsightModules.ASSETS: \"has_insights_assets\",\n    InsightModules.APP_START: \"has_insights_app_start\",\n    InsightModules.SCREEN_LOAD: \"has_insights_screen_load\",\n    InsightModules.VITAL: \"has_insights_vitals\",\n    InsightModules.CACHE: \"has_insights_caches\",\n    InsightModules.QUEUE: \"has_insights_queues\",\n    InsightModules.LLM_MONITORING: \"has_insights_llm_monitoring\",\n    InsightModules.AGENTS: \"has_insights_agent_monitoring\",\n}\n\n\n@sentry_sdk.tracing.trace\ndef _record_transaction_info(\n    jobs: Sequence[Job], projects: ProjectsMapping, skip_send_first_transaction: bool\n) -> None:\n    for job in jobs:\n        try:\n            event = job[\"event\"]\n\n            project = event.project\n            with sentry_sdk.start_span(op=\"event_manager.record_transaction_name_for_clustering\"):\n                record_transaction_name_for_clustering(project, event.data)\n\n            record_event_processed(project, event)\n\n            if not skip_send_first_transaction:\n                set_project_flag_and_signal(\n                    project,\n                    \"has_transactions\",\n                    first_transaction_received,\n                    event=event,\n                )\n\n            spans = job[\"data\"][\"spans\"]\n            for module, is_module in INSIGHT_MODULE_FILTERS.items():\n                if is_module(spans):\n                    set_project_flag_and_signal(\n                        project,\n                        INSIGHT_MODULE_TO_PROJECT_FLAG_NAME[module],\n                        first_insight_span_received,\n                        module=module,\n                    )\n\n            if job[\"release\"]:\n                environment = job[\"data\"].get(\"environment\") or None  # coorce \"\" to None\n                record_latest_release(project, job[\"release\"], environment)\n                record_release_received(project, job[\"release\"].version)\n        except Exception:\n            sentry_sdk.capture_exception()\n\n\nclass PerformanceJob(TypedDict, total=False):\n    performance_problems: Sequence[PerformanceProblem]\n    event: Event\n    groups: list[GroupInfo]\n    culprit: str\n    received_timestamp: float\n    event_metadata: Mapping[str, Any]\n    platform: str\n    level: str\n    logger_name: str\n    release: Release\n\n\ndef save_grouphash_and_group(\n    project: Project,\n    event: Event,\n    new_grouphash: str,\n    **group_kwargs: Any,\n) -> tuple[Group, bool, GroupHash]:\n    group = None\n    with transaction.atomic(router.db_for_write(GroupHash)):\n        group_hash, created = GroupHash.objects.get_or_create(project=project, hash=new_grouphash)\n        if created:\n            group = _create_group(project, event, **group_kwargs)\n            group_hash.update(group=group)\n\n    if group is None:\n        # If we failed to create the group it means another worker beat us to\n        # it. Since a GroupHash can only be created in a transaction with the\n        # Group, we can guarantee that the Group will exist at this point and\n        # fetch it via GroupHash\n        group = Group.objects.get(grouphash__project=project, grouphash__hash=new_grouphash)\n    return group, created, group_hash\n\n\n@sentry_sdk.tracing.trace\ndef _send_occurrence_to_platform(jobs: Sequence[Job], projects: ProjectsMapping) -> None:\n    for job in jobs:\n        event = job[\"event\"]\n        project = event.project\n        event_id = event.event_id\n\n        performance_problems = job[\"performance_problems\"]\n        for problem in performance_problems:\n            occurrence = IssueOccurrence(\n                id=uuid.uuid4().hex,\n                resource_id=None,\n                project_id=project.id,\n                event_id=event_id,\n                fingerprint=[problem.fingerprint],\n                type=problem.type,\n                issue_title=problem.title,\n                subtitle=problem.desc,\n                culprit=event.transaction,\n                evidence_data=problem.evidence_data,\n                evidence_display=problem.evidence_display,\n                detection_time=event.datetime,\n                level=job[\"level\"],\n            )\n\n            produce_occurrence_to_kafka(payload_type=PayloadType.OCCURRENCE, occurrence=occurrence)\n\n\n@sentry_sdk.tracing.trace\ndef save_transaction_events(\n    jobs: Sequence[Job],\n    projects: ProjectsMapping,\n    skip_send_first_transaction: bool = False,\n) -> Sequence[Job]:\n    from .ingest.types import ConsumerType\n\n    organization_ids = {project.organization_id for project in projects.values()}\n    organizations = {o.id: o for o in Organization.objects.get_many_from_cache(organization_ids)}\n\n    with metrics.timer(\"save_transaction_events.set_organization_cached_field_values\"):\n        for project in projects.values():\n            try:\n                project.set_cached_field_value(\n                    \"organization\", organizations[project.organization_id]\n                )\n            except KeyError:\n                continue\n    set_span_attribute(\"jobs\", len(jobs))\n    set_span_attribute(\"projects\", len(projects))\n\n    # NOTE: Keep this list synchronized with sentry/spans/consumers/process_segments/message.py\n\n    _get_or_create_release_many(jobs, projects)\n    _get_event_user_many(jobs, projects)\n    _derive_plugin_tags_many(jobs, projects)\n    _derive_interface_tags_many(jobs)\n    _calculate_span_grouping(jobs, projects)\n    _materialize_metadata_many(jobs)\n    _get_or_create_environment_many(jobs, projects)\n    _get_or_create_release_associated_models(jobs, projects)\n    _tsdb_record_all_metrics(jobs)\n    _materialize_event_metrics(jobs)\n    _nodestore_save_many(jobs=jobs, app_feature=\"transactions\")\n    _eventstream_insert_many(jobs)\n\n    for job in jobs:\n        track_sampled_event(\n            job[\"event\"].event_id,\n            ConsumerType.Transactions,\n            TransactionStageStatus.SNUBA_TOPIC_PUT,\n        )\n\n    _track_outcome_accepted_many(jobs)\n    _detect_performance_problems(jobs, projects)\n    _send_occurrence_to_platform(jobs, projects)\n    _record_transaction_info(jobs, projects, skip_send_first_transaction)\n\n    return jobs\n\n\n@sentry_sdk.tracing.trace\ndef save_generic_events(jobs: Sequence[Job], projects: ProjectsMapping) -> Sequence[Job]:\n    organization_ids = {project.organization_id for project in projects.values()}\n    organizations = {o.id: o for o in Organization.objects.get_many_from_cache(organization_ids)}\n\n    for project in projects.values():\n        try:\n            project.set_cached_field_value(\"organization\", organizations[project.organization_id])\n        except KeyError:\n            continue\n\n    _get_or_create_release_many(jobs, projects)\n    _get_event_user_many(jobs, projects)\n    _derive_plugin_tags_many(jobs, projects)\n    _derive_interface_tags_many(jobs)\n    _materialize_metadata_many(jobs)\n    _get_or_create_environment_many(jobs, projects)\n    _materialize_event_metrics(jobs)\n    _nodestore_save_many(jobs=jobs, app_feature=\"issue_platform\")\n\n    return jobs\n",
    "file_path": "/Users/wangyue/Code/CodeReviewData/ReviewDataset/sentry-greptile/src/sentry/event_manager.py",
    "line_count": 2720,
    "encoding": "utf-8",
    "error": null
}

Message 15 [Assistant]:
--------------------------------------------------------------------------------
Content:
文件很大，让我搜索 `get_data` 方法：
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "def get_data",
      "context_lines": 20
    },
    "id": "call_00_huEEQzh9itaEusc9vDJ5XE8d",
    "type": "tool_call"
  }
]

Message 16 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_huEEQzh9itaEusc9vDJ5XE8d
Result:
{
    "matches": [
        {
            "file": "tests/sentry/sentry_apps/api/endpoints/test_sentry_apps.py",
            "line_number": 151,
            "matched_line": "    def get_data(self, **kwargs: Any) -> Mapping[str, Any]:",
            "context_start_line": 131,
            "context_end_line": 171,
            "context": [
                "131:             \"metadata\": {},",
                "132:         }",
                "133: ",
                "134:         if mask_secret:",
                "135:             data[\"clientSecret\"] = MASKED_VALUE",
                "136: ",
                "137:         if has_features:",
                "138:             data[\"featureData\"] = [",
                "139:                 {",
                "140:                     \"featureId\": 0,",
                "141:                     \"featureGate\": \"integrations-api\",",
                "142:                     \"description\": (",
                "143:                         f\"{sentry_app.name} can **utilize the Sentry API** to pull data or\"",
                "144:                         + \" update resources in Sentry (with permissions granted, of course).\"",
                "145:                     ),",
                "146:                 }",
                "147:             ]",
                "148: ",
                "149:         assert data in orjson.loads(response.content)",
                "150: ",
                "151:     def get_data(self, **kwargs: Any) -> Mapping[str, Any]:",
                "152:         return {",
                "153:             \"author\": \"Sentry\",",
                "154:             \"events\": (\"issue\",),",
                "155:             \"isAlertable\": False,",
                "156:             \"isInternal\": False,",
                "157:             \"name\": \"MyApp\",",
                "158:             \"organization\": self.organization.slug,",
                "159:             \"redirectUrl\": \"\",",
                "160:             \"schema\": None,",
                "161:             \"scopes\": (",
                "162:                 \"project:read\",",
                "163:                 \"event:read\",",
                "164:                 \"org:read\",",
                "165:                 \"org:write\",",
                "166:                 \"org:admin\",",
                "167:                 \"org:integrations\",",
                "168:             ),",
                "169:             \"verifyInstall\": True,",
                "170:             \"webhookUrl\": \"https://example.com\",",
                "171:             **kwargs,"
            ]
        },
        {
            "file": "tests/sentry/api/endpoints/test_organization_member_index.py",
            "line_number": 607,
            "matched_line": "        def get_data(email: str, other_team_invite: bool = False, use_team_roles: bool = True):",
            "context_start_line": 587,
            "context_end_line": 627,
            "context": [
                "587:         for invite_role in invite_roles:",
                "588:             data = {",
                "589:                 \"email\": f\"{invite_role}_2@localhost\",",
                "590:                 \"role\": invite_role,",
                "591:                 \"teamRoles\": [",
                "592:                     {\"teamSlug\": self.team.slug, \"role\": \"contributor\"},",
                "593:                 ],",
                "594:             }",
                "595:             if any(invite_role == allowed_role.id for allowed_role in allowed_roles):",
                "596:                 self.get_success_response(self.organization.slug, **data, status_code=201)",
                "597:             else:",
                "598:                 self.get_error_response(self.organization.slug, **data, status_code=400)",
                "599: ",
                "600:     def invite_to_other_team_helper(self, role):",
                "601:         user = self.create_user(\"inviter@localhost\")",
                "602:         self.create_member(user=user, organization=self.organization, role=role, teams=[self.team])",
                "603:         self.login_as(user=user)",
                "604: ",
                "605:         other_team = self.create_team(organization=self.organization, name=\"Moo Deng's Team\")",
                "606: ",
                "607:         def get_data(email: str, other_team_invite: bool = False, use_team_roles: bool = True):",
                "608:             team_slug = other_team.slug if other_team_invite else self.team.slug",
                "609:             data: dict[str, str | list] = {",
                "610:                 \"email\": f\"{email}@localhost\",",
                "611:                 \"role\": \"member\",",
                "612:             }",
                "613: ",
                "614:             if use_team_roles:",
                "615:                 data[\"teamRoles\"] = [{\"teamSlug\": team_slug, \"role\": \"contributor\"}]",
                "616:             else:",
                "617:                 data[\"teams\"] = [team_slug]",
                "618: ",
                "619:             return data",
                "620: ",
                "621:         # members can never invite members if disable_member_invite = True",
                "622:         self.organization.flags.allow_joinleave = True",
                "623:         self.organization.flags.disable_member_invite = True",
                "624:         self.organization.save()",
                "625:         response = self.get_error_response(",
                "626:             self.organization.slug, **get_data(\"foo1\"), status_code=403",
                "627:         )"
            ]
        },
        {
            "file": "tests/sentry/api/endpoints/test_organization_member_invite_index.py",
            "line_number": 110,
            "matched_line": "        def get_data(email: str, other_team_invite: bool = False):",
            "context_start_line": 90,
            "context_end_line": 130,
            "context": [
                "90:         self.organization.save()",
                "91: ",
                "92:         for invite_role in invite_roles:",
                "93:             data = {",
                "94:                 \"email\": f\"{invite_role}_2@localhost\",",
                "95:                 \"orgRole\": invite_role,",
                "96:                 \"teams\": [self.team.slug],",
                "97:             }",
                "98:             if any(invite_role == allowed_role.id for allowed_role in allowed_roles):",
                "99:                 self.get_success_response(self.organization.slug, **data, status_code=201)",
                "100:             else:",
                "101:                 self.get_error_response(self.organization.slug, **data, status_code=400)",
                "102: ",
                "103:     def invite_to_other_team_helper(self, role):",
                "104:         user = self.create_user(\"inviter@localhost\")",
                "105:         self.create_member(user=user, organization=self.organization, role=role, teams=[self.team])",
                "106:         self.login_as(user=user)",
                "107: ",
                "108:         other_team = self.create_team(organization=self.organization, name=\"Moo Deng's Team\")",
                "109: ",
                "110:         def get_data(email: str, other_team_invite: bool = False):",
                "111:             team_slug = other_team.slug if other_team_invite else self.team.slug",
                "112:             data: dict[str, str | list] = {",
                "113:                 \"email\": f\"{email}@localhost\",",
                "114:                 \"orgRole\": \"member\",",
                "115:                 \"teams\": [team_slug],",
                "116:             }",
                "117:             return data",
                "118: ",
                "119:         # members can never invite members if disable_member_invite = True",
                "120:         # an invite request will be created instead of an invite",
                "121:         self.organization.flags.allow_joinleave = True",
                "122:         self.organization.flags.disable_member_invite = True",
                "123:         self.organization.save()",
                "124:         response = self.get_success_response(",
                "125:             self.organization.slug, **get_data(\"foo1\"), status_code=201",
                "126:         )",
                "127:         omi = OrganizationMemberInvite.objects.get(id=response.data[\"id\"])",
                "128:         assert omi.invite_status == InviteStatus.REQUESTED_TO_BE_INVITED.value",
                "129: ",
                "130:         self.organization.flags.allow_joinleave = False"
            ]
        },
        {
            "file": "src/sentry/datascrubbing.py",
            "line_number": 49,
            "matched_line": "def get_datascrubbing_settings(project):",
            "context_start_line": 29,
            "context_end_line": 69,
            "context": [
                "29:     # Order of merging is important here. We want to apply organization rules",
                "30:     # before project rules. For example:",
                "31:     #",
                "32:     # * Organization rule: remove substrings \"mypassword\"",
                "33:     # * Project rule: remove substrings \"my\"",
                "34:     #",
                "35:     # If we were to apply project rules before organization rules, \"password\"",
                "36:     # would leak. We effectively disabled an organization rule using a project rule.",
                "37:     #",
                "38:     # Of course organization rules can also break project rules the same way,",
                "39:     # but we communicate in the UI that organization options take precedence",
                "40:     # here.",
                "41:     return _merge_pii_configs(",
                "42:         [",
                "43:             (\"organization:\", _decode(project.organization.get_option(\"sentry:relay_pii_config\"))),",
                "44:             (\"project:\", _decode(project.get_option(\"sentry:relay_pii_config\"))),",
                "45:         ]",
                "46:     )",
                "47: ",
                "48: ",
                "49: def get_datascrubbing_settings(project):",
                "50:     org = project.organization",
                "51:     rv = {}",
                "52: ",
                "53:     exclude_fields_key = \"sentry:safe_fields\"",
                "54:     rv[\"excludeFields\"] = org.get_option(exclude_fields_key, []) + project.get_option(",
                "55:         exclude_fields_key, []",
                "56:     )",
                "57: ",
                "58:     if org.get_option(\"sentry:require_scrub_data\", False) or project.get_option(",
                "59:         \"sentry:scrub_data\", True",
                "60:     ):",
                "61:         rv[\"scrubData\"] = True",
                "62: ",
                "63:     if org.get_option(\"sentry:require_scrub_ip_address\", False) or project.get_option(",
                "64:         \"sentry:scrub_ip_address\", False",
                "65:     ):",
                "66:         rv[\"scrubIpAddresses\"] = True",
                "67: ",
                "68:     sensitive_fields_key = \"sentry:sensitive_fields\"",
                "69:     rv[\"sensitiveFields\"] = org.get_option(sensitive_fields_key, []) + project.get_option("
            ]
        },
        {
            "file": "src/sentry/event_manager.py",
            "line_number": 412,
            "matched_line": "    def get_data(self) -> MutableMapping[str, Any]:",
            "context_start_line": 392,
            "context_end_line": 432,
            "context": [
                "392:             key_id=str(self._key.id) if self._key else None,",
                "393:             grouping_config=self._grouping_config,",
                "394:             protocol_version=str(self.version) if self.version is not None else None,",
                "395:             is_renormalize=self._is_renormalize,",
                "396:             remove_other=self._remove_other,",
                "397:             normalize_user_agent=True,",
                "398:             sent_at=self.sent_at.isoformat() if self.sent_at is not None else None,",
                "399:             json_dumps=orjson.dumps,",
                "400:             **DEFAULT_STORE_NORMALIZER_ARGS,",
                "401:         )",
                "402: ",
                "403:         pre_normalize_type = self._data.get(\"type\")",
                "404:         self._data = rust_normalizer.normalize_event(dict(self._data), json_loads=orjson.loads)",
                "405: ",
                "406:         # XXX: This is a hack to make generic events work (for now?). I'm not sure whether we should",
                "407:         # include this in the rust normalizer, since we don't want people sending us these via the",
                "408:         # sdk.",
                "409:         if pre_normalize_type in (\"generic\", \"feedback\"):",
                "410:             self._data[\"type\"] = pre_normalize_type",
                "411: ",
                "412:     def get_data(self) -> MutableMapping[str, Any]:",
                "413:         return self._data",
                "414: ",
                "415:     @sentry_sdk.tracing.trace",
                "416:     def save(",
                "417:         self,",
                "418:         project_id: int | None,",
                "419:         raw: bool = False,",
                "420:         assume_normalized: bool = False,",
                "421:         start_time: float | None = None,",
                "422:         cache_key: str | None = None,",
                "423:         skip_send_first_transaction: bool = False,",
                "424:         has_attachments: bool = False,",
                "425:     ) -> Event:",
                "426:         \"\"\"",
                "427:         After normalizing and processing an event, save adjacent models such as",
                "428:         releases and environments to postgres and write the event into",
                "429:         eventstream. From there it will be picked up by Snuba and",
                "430:         post-processing.",
                "431: ",
                "432:         We re-insert events with duplicate IDs into Snuba, which is responsible"
            ]
        },
        {
            "file": "src/sentry/tasks/embeddings_grouping/utils.py",
            "line_number": 285,
            "matched_line": "def get_data_from_snuba(project, groups_to_backfill_with_no_embedding, worker_number=None):",
            "context_start_line": 265,
            "context_end_line": 305,
            "context": [
                "265:     if len(groups_to_backfill_batch) != len(groups_to_backfill_with_no_embedding):",
                "266:         logger.info(",
                "267:             \"backfill_seer_grouping_records.groups_already_had_embedding\",",
                "268:             extra={",
                "269:                 \"project_id\": project.id,",
                "270:                 \"total_batch_groups\": len(groups_to_backfill_batch),",
                "271:                 \"groups_with_embedding\": (",
                "272:                     len(groups_to_backfill_batch) - len(groups_to_backfill_with_no_embedding)",
                "273:                 ),",
                "274:                 \"worker_number\": worker_number,",
                "275:                 \"project_index_in_cohort\": project_index_in_cohort,",
                "276:             },",
                "277:         )",
                "278:     return (",
                "279:         groups_to_backfill_with_no_embedding,",
                "280:         batch_end_group_id,",
                "281:     )",
                "282: ",
                "283: ",
                "284: @sentry_sdk.tracing.trace",
                "285: def get_data_from_snuba(project, groups_to_backfill_with_no_embedding, worker_number=None):",
                "286:     # TODO(jangjodi): Only query per group if it has over 1 million events, or batch queries with new where condition",
                "287:     events_entity = Entity(\"events\", alias=\"events\")",
                "288: ",
                "289:     snuba_results = []",
                "290:     for group_ids_chunk in chunked(",
                "291:         groups_to_backfill_with_no_embedding,",
                "292:         options.get(\"similarity.backfill_snuba_concurrent_requests\"),",
                "293:     ):",
                "294:         snuba_requests = []",
                "295:         for group_id in group_ids_chunk:",
                "296:             group = Group.objects.get(id=group_id)",
                "297:             query = Query(",
                "298:                 match=events_entity,",
                "299:                 select=[",
                "300:                     Column(\"group_id\"),",
                "301:                     Column(\"event_id\"),",
                "302:                 ],",
                "303:                 where=[",
                "304:                     Condition(Column(\"project_id\"), Op.EQ, project.id),",
                "305:                     Condition(Column(\"group_id\"), Op.EQ, group_id),"
            ]
        },
        {
            "file": "src/sentry/tsdb/snuba.py",
            "line_number": 250,
            "matched_line": "    def get_data(",
            "context_start_line": 230,
            "context_end_line": 270,
            "context": [
                "230:                 return [",
                "231:                     \"toUnixTimestamp\",",
                "232:                     [[\"toDateTime\", [[\"toDate\", \"timestamp\"]]]],",
                "233:                     time_column_alias,",
                "234:                 ]",
                "235:             else:",
                "236:                 return None",
                "237: ",
                "238:         # if we don't have an explicit function mapped to this rollup, we have to calculate it on the fly",
                "239:         # multiply(intDiv(toUInt32(toUnixTimestamp(timestamp)), granularity)))",
                "240:         synthetic_rollup = [",
                "241:             \"multiply\",",
                "242:             [[\"intDiv\", [[\"toUInt32\", [[\"toUnixTimestamp\", \"timestamp\"]]], rollup]], rollup],",
                "243:             time_column_alias,",
                "244:         ]",
                "245: ",
                "246:         known_rollups = rollup_agg(rollup, time_column_alias)",
                "247: ",
                "248:         return known_rollups if known_rollups else synthetic_rollup",
                "249: ",
                "250:     def get_data(",
                "251:         self,",
                "252:         model,",
                "253:         keys,",
                "254:         start,",
                "255:         end,",
                "256:         rollup=None,",
                "257:         environment_ids=None,",
                "258:         aggregation=\"count()\",",
                "259:         group_on_model=True,",
                "260:         group_on_time=False,",
                "261:         conditions=None,",
                "262:         use_cache=False,",
                "263:         jitter_value=None,",
                "264:         tenant_ids: dict[str, str | int] | None = None,",
                "265:         referrer_suffix: str | None = None,",
                "266:     ):",
                "267:         if model in self.non_outcomes_snql_query_settings:",
                "268:             # no way around having to explicitly map legacy condition format to SnQL since this function",
                "269:             # is used everywhere that expects `conditions` to be legacy format",
                "270:             parsed_conditions = []"
            ]
        },
        {
            "file": "src/sentry/data_secrecy/service/service.py",
            "line_number": 26,
            "matched_line": "    def get_data_secrecy_waiver(self, *, organization_id: int) -> RpcDataSecrecyWaiver | None:",
            "context_start_line": 6,
            "context_end_line": 30,
            "context": [
                "6: import abc",
                "7: ",
                "8: from sentry.data_secrecy.service.model import RpcDataSecrecyWaiver",
                "9: from sentry.hybridcloud.rpc.resolvers import ByOrganizationId",
                "10: from sentry.hybridcloud.rpc.service import RpcService, regional_rpc_method",
                "11: from sentry.silo.base import SiloMode",
                "12: ",
                "13: ",
                "14: class DataSecrecyService(RpcService):",
                "15:     key = \"data_secrecy\"",
                "16:     local_mode = SiloMode.REGION",
                "17: ",
                "18:     @classmethod",
                "19:     def get_local_implementation(cls) -> RpcService:",
                "20:         from sentry.data_secrecy.service.impl import DatabaseBackedDataSecrecyService",
                "21: ",
                "22:         return DatabaseBackedDataSecrecyService()",
                "23: ",
                "24:     @regional_rpc_method(resolve=ByOrganizationId())",
                "25:     @abc.abstractmethod",
                "26:     def get_data_secrecy_waiver(self, *, organization_id: int) -> RpcDataSecrecyWaiver | None:",
                "27:         pass",
                "28: ",
                "29: ",
                "30: data_secrecy_service = DataSecrecyService.create_delegation()"
            ]
        },
        {
            "file": "src/sentry/data_secrecy/service/impl.py",
            "line_number": 8,
            "matched_line": "    def get_data_secrecy_waiver(self, *, organization_id: int) -> RpcDataSecrecyWaiver | None:",
            "context_start_line": 1,
            "context_end_line": 16,
            "context": [
                "1: from sentry.data_secrecy.models.datasecrecywaiver import DataSecrecyWaiver",
                "2: from sentry.data_secrecy.service.model import RpcDataSecrecyWaiver",
                "3: from sentry.data_secrecy.service.serial import serialize_data_secrecy_waiver",
                "4: from sentry.data_secrecy.service.service import DataSecrecyService",
                "5: ",
                "6: ",
                "7: class DatabaseBackedDataSecrecyService(DataSecrecyService):",
                "8:     def get_data_secrecy_waiver(self, *, organization_id: int) -> RpcDataSecrecyWaiver | None:",
                "9:         try:",
                "10:             data_secrecy_waiver = DataSecrecyWaiver.objects.filter(",
                "11:                 organization_id=organization_id",
                "12:             ).get()",
                "13:         except DataSecrecyWaiver.DoesNotExist:",
                "14:             return None",
                "15: ",
                "16:         return serialize_data_secrecy_waiver(data_secrecy_waiver=data_secrecy_waiver)"
            ]
        },
        {
            "file": "src/sentry/lang/native/error.py",
            "line_number": 68,
            "matched_line": "    def get_data(self):",
            "context_start_line": 48,
            "context_end_line": 88,
            "context": [
                "48:             self.image_arch: str | None = obj.arch",
                "49:         else:",
                "50:             self.image_uuid = None",
                "51:             self.image_arch = None",
                "52: ",
                "53:     @property",
                "54:     def is_user_fixable(self):",
                "55:         \"\"\"These are errors that a user can fix themselves.\"\"\"",
                "56:         return self.type in USER_FIXABLE_ERRORS",
                "57: ",
                "58:     @property",
                "59:     def is_fatal(self):",
                "60:         \"\"\"If this is true then a processing issues has to be reported.\"\"\"",
                "61:         return self.type in FATAL_ERRORS",
                "62: ",
                "63:     @property",
                "64:     def is_sdk_failure(self):",
                "65:         \"\"\"An error that most likely happened because of a bad SDK.\"\"\"",
                "66:         return self.type == EventError.NATIVE_UNKNOWN_IMAGE",
                "67: ",
                "68:     def get_data(self):",
                "69:         \"\"\"Returns the event data.\"\"\"",
                "70:         rv = {\"message\": self.message, \"type\": self.type}",
                "71:         if self.image_path is not None:",
                "72:             rv[\"image_path\"] = self.image_path",
                "73:         if self.image_uuid is not None:",
                "74:             rv[\"image_uuid\"] = self.image_uuid",
                "75:         if self.image_arch is not None:",
                "76:             rv[\"image_arch\"] = self.image_arch",
                "77:         return rv",
                "78: ",
                "79:     def __str__(self):",
                "80:         rv = []",
                "81:         if self.type is not None:",
                "82:             rv.append(\"%s: \" % self.type)",
                "83:         rv.append(self.message or \"no information available\")",
                "84:         if self.image_uuid is not None:",
                "85:             rv.append(\" image-uuid=%s\" % self.image_uuid)",
                "86:         if self.image_name is not None:",
                "87:             rv.append(\" image-name=%s\" % self.image_name)",
                "88:         return \"\".join(rv)"
            ]
        }
    ],
    "total": 10
}

Message 17 [Assistant]:
--------------------------------------------------------------------------------
Content:
让我搜索 `EventManager` 类中的 `get_data` 方法：
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "class EventManager.*get_data",
      "is_regex": true,
      "context_lines": 30
    },
    "id": "call_00_41uyNPAHfrUKIYLy1v1Oak8X",
    "type": "tool_call"
  }
]

Message 18 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_41uyNPAHfrUKIYLy1v1Oak8X
Result:
{
    "matches": [],
    "message": "No matches found for pattern: class EventManager.*get_data"
}

Message 19 [Assistant]:
--------------------------------------------------------------------------------
Content:
让我直接搜索 `EventManager` 类中的 `get_data` 方法定义：
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "def get_data.*self",
      "context_lines": 10,
      "max_results": 20
    },
    "id": "call_00_AyJkDhBunrgDn886f4f6rsI7",
    "type": "tool_call"
  }
]

Message 20 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_AyJkDhBunrgDn886f4f6rsI7
Result:
{
    "matches": [],
    "message": "No matches found for pattern: def get_data.*self"
}

Message 21 [Assistant]:
--------------------------------------------------------------------------------
Content:
让我查看 `EventManager` 类的完整定义，看看 `get_data` 方法在哪里：
Tool Calls:
[
  {
    "name": "run_grep",
    "args": {
      "pattern": "EventManager.*:",
      "context_lines": 50,
      "max_results": 5
    },
    "id": "call_00_UE0oW8QC5Hj2kInoPGC0VOkw",
    "type": "tool_call"
  }
]

Message 22 [Tool]:
--------------------------------------------------------------------------------
Tool: run_grep
Tool Call ID: call_00_UE0oW8QC5Hj2kInoPGC0VOkw
Result:
{
    "matches": [],
    "message": "No matches found for pattern: EventManager.*:"
}

Message 23 [Assistant]:
--------------------------------------------------------------------------------
Content:
```json
{
    "risk_type": "lifecycle",
    "file_path": "src/sentry/testutils/factories.py",
    "line_number": [1049, 1049],
    "description": "在 `store_event` 方法中调用 `_set_sample_rate_from_error_sampling(normalized_data)` 会直接修改传入的 `normalized_data` 字典。由于 `normalized_data` 来自 `manager.get_data()`，而 `EventManager` 可能内部重用或缓存数据，这种副作用可能影响其他依赖相同数据的地方。测试工厂的修改可能无意中影响其他测试用例。",
    "confidence": 0.8,
    "severity": "warning",
    "suggestion": "考虑在调用 `_set_sample_rate_from_error_sampling` 之前对 `normalized_data` 进行深拷贝，以避免副作用。例如：`normalized_data_copy = copy.deepcopy(normalized_data)` 然后传递副本。"
}
```


